# MATH50003 Numerical Analysis: Problem Sheet 4

This problem sheet explores least squares, the QR decomposition including for
tridiagonal matrices,
and the PLU decompositions.

Questions marked with a ‚ãÜ are meant to be completed without using a computer.
Problems are denoted A/B/C to indicate their difficulty.


```julia
using LinearAlgebra, Plots, Test
```



## 1. Least squares and QR decompositions

**Problem 1.1 (B)** Find and plot the best least squares fit of ${1 \over 5x^2 + 1}$ by degree $n$
polynomials for $n = 0,\ldots,10$ at 1000 evenly spaced points between $0$ and $1$.

**SOLUTION**
```julia
x = range(0, 1; length=1000)
pl = plot()
f = 1 ./ (5x.^2 .+ 1)
for n = 0:10
    # Creates 1000 x n+1 matrix with entries x[k]^(j-1)
    A = x .^ (0:n)'
    c = A \ f
    plot!(x, A*c)
end
pl
```
```julia
# Here is the same approach as above but this time explicitly using QR as discussed in the lecture notes:
x = range(0, 1; length=1000)
pl = plot()
f = 1 ./ (5x.^2 .+ 1)
for n = 0:10
    # Creates 1000 x n+1 matrix with entries x[k]^(j-1)
    A = x .^ (0:n)'
    (Q, R) = qr(A)
    c = R \ Q[:, 1:n+1]' * f
    plot!(x, A * c)
end
pl
```
**END**

**Problem 1.2‚ãÜ (B)** Show that every matrix has a QR decomposition such that the diagonal of $R$ is non-negative.
Make sure to include the case of more columns than rows.

**SOLUTION**

Beginning with the square case, a square matrix $A=QR$ with square orthogonal $Q$ and square upper triangular $R$ can always be rewritten in the form $A=QD^{-1}DR$ where $D$ is a diagonal matrix with $sign(R[j,j])$ on the diagonal. As a result, $DR$ is an upper triangular matrix with positive diagonal. It remains to check that $QD^{-1}=-QD$ is still orthogonal - this is easy to check since $$-QD(-QD)^T = QDDQ^T = QQ^T = I.$$
Note we have made use of the fact that the inverse of a diagonal matrix is diagonal, that any diagonal matrix satisfies $D^{T} = D$ and that $DD=I$ since $\text{sign}(R[j,j])^2 = 1$.

The same argument works for the non-square cases as long as we take care to consider the appropriate dimensions and pad with the identity matrix. Note that $Q$ is always a square matrix in the $QR$ decomposition. Assume the $R$ factor has more columns $m$ than rows $n$. Then a square $n \times n$ diagonal matrix with its $n$ diagonal entries being $\text{sign}(R[j,j]), j=1..n$ works with the same argument as above.

Finally, assume that $R$ has less colums $m$ than rows $n$. In this case the square $n \times n$ diagonal matrix with its first $m$ diagonal entries being $\text{sign}(R[j,j]), j=1..m$ and any remaining diagonal entries being $1$ works with the same argument as above.

Since the matrix $D$ is always square diagonal and orthogonal by construction, everything else for both of these cases is exactly as in the square case.

Here are some practical examples in Julia:

```julia
E = [-1 0 0 0; 0 1 0 0; 0 0 1 0; 0 0 0 1]
R = [-1 3; 0 2; 0 0; 0 0]
```
```julia
E*R
```
```julia
E = [-1 0 0 0; 0 1 0 0; 0 0 -1 0; 0 0 0 1]
R = [-1 3 7 8 2 1; 0 2 9 2 1 9; 0 0 -7 2 1 9; 0 0 0 7 5 2]
```
```julia
E*R
```
**END**

**Problem 1.3‚ãÜ (B)** Show that the QR decomposition of a square invertible matrix is unique,
provided that the diagonal of $R$ is positive.


**SOLUTION**

Assume there is a second decomposition also with positive diagonal
$$
A = QR = \tilde{Q} \tilde{R}
$$
Then we know
$$
Q^‚ä§ \tilde{Q} = R \tilde{R}^{-1}
$$
Note $Q^‚ä§ \tilde{Q}$ is orthogonal, and $R \tilde{R}^{-1}$ has positive eigenvalues (the diagonal), hence all $m$ eigenvalues of
$Q^‚ä§ \tilde{Q}$ are 1. This means that $Q^‚ä§ \tilde{Q} = I$ and hence $\tilde{Q} = Q$, which then implies $\tilde{R} = R$.‚àé
**END**

## 2. Gram‚ÄìSchmidt

**Problem 2.1‚ãÜ (B)** The modified Gram‚ÄìSchmidt algorithm is a slight variation of Gram‚ÄìSchmidt where
instead of computing
$$
ùêØ_j := ùêö_j - \sum_{k=1}^{j-1} \underbrace{ùê™_k^\top ùêö_j}_{r_{kj}} ùê™_k
$$
we compute it step-by-step:
$$
\begin{align*}
ùêØ_j^1 &:= ùêö_j \\
ùêØ_j^{k+1} &:= ùêØ_j^k - ùê™_k^\top ùêØ_j^k ùê™_k
\end{align*}
$$
Show that $ùêØ_j^j = ùêØ_j$.

**SOLUTION**

Recall that the column vectors of $Q$ are orthonormal i.e. $(ùê™_i,ùê™_j) = \delta_{ij}$.
Next observe that substituting from $ùêØ^1_j$ for $ùêö_j$, for each $ùêØ^i_j$ we get $i$ terms (note that the second term in the definition of $ùêØ_j^{k+1}$ contributes only 1 term by orthonormality of $Q$, retrieving the contribution from $ùêØ^{i-1}_j$ plus one.)
$$
ùêØ^i_j := ùêö_j - \sum_{k=1}^{i-2}{q_k^\top ùêö_j} q_k - {q_{i-1}^\top ùêö_j} q_{i-1}
$$

**END**

**Problem 2.2 (B)** Complete the following
function implementing the modified Gram‚ÄìSchmidt algorithm:
```julia
function modifiedgramschmidt(A)
    m,n = size(A)
    m ‚â• n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    for j = 1:n
        # TODO: Implement the Modified Gram‚ÄìSchmidt algorthm
    end
    Q,R
end

A = randn(300,300)
Q,R = modifiedgramschmidt(A)
@test A ‚âà Q*R
@test Q'Q ‚âà I
```

**SOLUTION**
```julia
function modifiedgramschmidt(A)
    m,n = size(A)
    m ‚â• n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    # looping through the columns
    for j = 1:n
        v = A[:,j]
        # now go through each row
        for k = 1:j-1
            # take inner product with q_k and v
            R[k, j] = Q[:, k]' * v
            v = v - R[k, j] * Q[:, k]
        end
        R[j,j] = norm(v)
        Q[:,j] = v/R[j,j]
    end
    Q,R
end

A = randn(300,300)
Q,R = modifiedgramschmidt(A)
@test A ‚âà Q*R
@test Q'Q ‚âà I
```

**END**

**Problem 2.3 (B)** Compare the orthogonality of `Q` between `gramschmidt` and `modifiedgramschmidt`
when applied to a `300 √ó 300` random matrix.

**SOLUTION**

For reference, here is the gramschmidt algorithm from the lecture notes:

```julia
function gramschmidt(A)
    m,n = size(A)
    m ‚â• n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    for j = 1:n
        for k = 1:j-1
            R[k,j] = Q[:,k]'*A[:,j]
        end
        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j]
        R[j,j] = norm(v)
        Q[:,j] = v/R[j,j]
    end
    Q,R
end

# Now compare MGS with GS:

A = randn(300,300)
Q,R = gramschmidt(A)
Qm,Rm = modifiedgramschmidt(A)

maximum(abs.(Q*Q')-I)
```
Modified Gram‚ÄìSchmidt is more accurate for constructing an orthogonal matrix:
```julia
maximum(abs.(Qm*Qm')-I)
```
**END**

## 3. Householder reflections

**Problem 3.1 (B)**
Complete the definition of `Reflections` which supports a sequence of reflections,
that is,
$$
Q = Q_{ùêØ_1} \cdots Q_{ùêØ_n}
$$
where the vectors are stored as a matrix `V` whose $j$-th column is $ùêØ_j$, and
$$
Q_{ùêØ_j} = I - 2 ùêØ_j ùêØ_j^\top.
$$

```julia
struct Reflections{T} <: AbstractMatrix{T}
    V::Matrix{T}
end

import Base: *, size, getindex

size(Q::Reflections) = (size(Q.V,1), size(Q.V,1))


function *(Q::Reflections, x::AbstractVector)
    T = eltype(Q)
    r = Vector{T}(x) # convert x to a vector of type T
    # TODO: Apply Q in O(mn) operations by applying
    # the reflection corresponding to each column of Q.V to r
    
    ## SOLUTION
    for j = size(Q.V, 2):-1:1
        v = Q.V[:, j]
        r = r - 2 * v * dot(v, r)
    end
    ## END

    r
end

function getindex(Q::Reflections, k::Int, j::Int)
    # TODO: Return Q[k,j] in O(mn) operations (hint: use *)

    ## SOLUTION
    T = eltype(Q.V)
    ej = zeros(T, size(Q, 1))
    ej[j] = one(T)
    return (Q*ej)[k]
    ## END
end

Y = randn(5,3)
V = Y * Diagonal([1/norm(Y[:,j]) for j=1:3])
Q = Reflections(V)
@test Q ‚âà (I - 2V[:,1]*V[:,1]')*(I - 2V[:,2]*V[:,2]')*(I - 2V[:,3]*V[:,3]')
@test Q'Q ‚âà I
```


**Problem 3.2 (B)** Complete the following function that implements
 Householder QR using only $O(mn^2)$ operations.
```julia
function householderqr(A)
    m,n = size(A)
    if m < n
        error("Only support more rows than columns")
    end
    # R begins as A, modify it in place
    R = copy(A)
    Q = Reflections(Matrix(1.0I, m, n))
    for j = 1:n
        # TODO: populate Q and R using O(m*(n-j)) operations
        ## SOLUTION
        y = copy(R[j:end, j])
        y[1] += ((y[1] ‚â• 0) ? 1 : -1) * norm(y)
        w = y / norm(y)
        Q.V[j:end, j] = w
        R[j:end, :] = R[j:end, :] - 2 * w * (w' * R[j:end, :])
        ## END
    end
    Q,R
end

A = randn(4,6)
Q,R = householderqr(A)
@test Q*R ‚âà A
@test Q'Q ‚âà I
```

**SOLUTION**

Here we sketch some additional notes comparing performance.

```julia
# Here's a performance comparison with what we saw in the lecture notes:
function householderreflection(x)
    y = copy(x)
    # we cannot use sign(x[1]) in case x[1] == 0
    y[1] += (x[1] ‚â• 0 ? 1 : -1)*norm(x)
    w = y/norm(y)
    I - 2*w*w'
end
function lecturehouseholderqr(A)
    m,n = size(A)
    R = copy(A)
    Q = Matrix(1.0I, m, m)
    for j = 1:n
        Q‚±º = householderreflection(R[j:end,j])
        R[j:end,:] = Q‚±º*R[j:end,:]
        Q[:,j:end] = Q[:,j:end]*Q‚±º
    end
    Q,R
end

using BenchmarkTools

for j = 1:5
    A = randn(j*100,j*100)
    @btime lecturehouseholderqr(A);
end
```
```julia
for j = 1:5
    A = randn(j*100,j*100)
    @btime householderqr(A);
end
```

**END**


## 4. Banded QR with Given's rotations

**Problem 4.1‚ãÜ (A)**  Describe an algorithm for computing the QR decomposition
of a tridiagonal matrix using rotations instead of reflections to upper-triangularise
column-by-column.

**SOLUTION**

Let $A$ be a tridiagonal matrix:

$$
A = \begin{bmatrix}
a_{11} & a_{12} & 0 & \cdots  &0\\
a_{21} & a_{22} & a_{23} & \ddots  &\vdots\\
0      & a_{32} & a_{33} & \ddots  &0\\
       & 0      & \ddots & \ddots  &a_{n-1,n}\\
       &        & 0      & a_{n,n-1}  & a_{nn}
\end{bmatrix} =
\begin{bmatrix}
\mathbf{a}_1 & \mathbf{a}_2 &\cdots & \mathbf{a}_n
\end{bmatrix},
$$
where each $\mathbf{a}_j \in \mathbb{R}^n$ and $[\mathbf{a}_j]_k = 0$ for $|j-k| > 1$.

Recall that,
$$
\frac{1}{\sqrt{a^2 + b^2}} \begin{bmatrix}
a & b \\
-b & a
\end{bmatrix} \begin{bmatrix}
a \\ b
\end{bmatrix} = \begin{bmatrix}
\sqrt{a^2 + b^2} \\ 0
\end{bmatrix},
$$
and that,
$$
\frac{1}{\sqrt{a^2 + b^2}} \begin{bmatrix}
a & b \\
-b & a
\end{bmatrix} = \begin{bmatrix}
\cos \theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix},
$$
is a rotation matrix where $\theta = -\arctan(b/a)$. With this in mind, consider multiplying $A$ from the left by,
$$
Q_1 = \begin{bmatrix}
\frac{a_{11}}{r_{11}} & \frac{a_{21}}{r_{11}} \\
-\frac{a_{21}}{r_{11}} & \frac{a_{11}}{r_{11}} \\
&&1\\
&&&\ddots\\
&&&&1
\end{bmatrix},
$$
where $r_{11} = \sqrt{a_{11}^2 + a_{21}^2}$. This rotates dimensions 1 and 2 through angle $\theta=-\arctan(a_{21}/a_{11})$. We have,
$$
Q_1 \mathbf{a}_1 = \begin{bmatrix}
r_{11} \\0 \\ \vdots \\ 0
\end{bmatrix},\hspace{10mm}
Q_1 \mathbf{a}_2 = \begin{bmatrix}
r_{12} := \frac{1}{r_{11}}(a_{11}a_{12} + a_{21}a_{22}) \\
t_1    := \frac{1}{r_{11}}(a_{11}a_{22} - a_{21}a_{12}) \\
a_{32} \\
0 \\
\vdots \\ 0
\end{bmatrix},\hspace{10mm}
Q_1 \mathbf{a}_3 = \begin{bmatrix}
r_{13} := \frac{1}{r_{11}}a_{21}a_{23} \\
s_1    := \frac{1}{r_{11}}a_{11}a_{23} \\
a_{33} \\
a_{43} \\
0 \\
\vdots \\ 0
\end{bmatrix},\hspace{10mm}
Q_1 \mathbf{a}_k = \mathbf{a}_k \textrm{ for } k > 3.
$$
Then we take,
$$
Q_2 = \begin{bmatrix}
1 \\
& \frac{t_1}{r_{22}} & \frac{a_{32}}{r_{22}} \\
& -\frac{a_{32}}{r_{22}} & \frac{t_1}{r_{22}} \\
&&&1 \\
&&&&\ddots\\
&&&&&1
\end{bmatrix},
$$
where $r_{22} = \sqrt{t_1^2 + a_{32}^2}$, a matrix which rotates dimensions 2 and 3 through angle $\theta_2 = -\arctan(a_{32}/t_1)$. Then,
$$
Q_2 Q_1 \mathbf{a}_{1} = Q_1\mathbf{a}_1 = \begin{bmatrix}
r_{11} \\0 \\ \vdots \\ 0
\end{bmatrix},\hspace{10mm}
Q_2 Q_1 \mathbf{a}_{2} = Q_2\begin{bmatrix}
r_{12} \\
t_1    \\
a_{32} \\
0 \\
\vdots \\ 0
\end{bmatrix} = \begin{bmatrix}
r_{12} \\
r_{22} \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix},\hspace{10mm}
Q_2Q_1\mathbf{a}_3 = Q_2 \begin{bmatrix}
r_{13} \\
s_1    \\
a_{33} \\
a_{43} \\
0 \\
\vdots \\ 0
\end{bmatrix}
=\begin{bmatrix}
r_{13} \\
r_{23} := \frac{1}{r_{22}}(t_1s_1 + a_{32}a_{33})  \\
t_2 := \frac{1}{r_{22}}(t_1a_{33} - a_{32}s_1) \\
a_{43} \\
0 \\
\vdots \\ 0
\end{bmatrix}
$$

$$
Q_2Q_1\mathbf{a}_4 = Q_2\begin{bmatrix}
0 \\
0 \\
a_{34} \\
a_{44} \\
a_{54} \\
0 \\
\vdots \\
0
\end{bmatrix} = \begin{bmatrix}
0 \\
r_{24} := \frac{1}{r_{22}}a_{32}a_{34} \\
s_2 := \frac{1}{r_{22}} t_1 a_{34} \\
a_{44} \\
a_{54} \\
0 \\
\vdots \\
0
\end{bmatrix}, \hspace{10mm}
Q_2Q_1 \mathbf{a}_k = \mathbf{a}_k \textrm{ for } k > 4.
$$

Now, for $j=3\rightarrow n-1$ we take,
$$
Q_j := \begin{bmatrix}
\mathbf{I}_{j-1} \\
& \frac{t_{j-1}}{r_{jj}} & \frac{a_{j+1, j}}{r_{jj}} \\
& \frac{a_{j+1, j}}{r_{jj}} & \frac{t_{j-1}}{r_{jj}} \\
&&& \mathbf{I}_{n-j-1}
\end{bmatrix},
$$
where $r_{jj} := \sqrt{t_{j-1}^2 + a_{j+1, j}^2}$.

This gives,
$$
Q_j \dots Q_1 \mathbf{a}_k =  \begin{bmatrix}
\mathbf{0}_{k-3} \\
r_{k-2,k} \\
r_{k-1, k} \\
r_{k,k} \\
\mathbf{0}_{n-k}
\end{bmatrix},
$$
for $k \leq j$,
and,
$$
Q_j \dots Q_1 \mathbf{a}_{j+1} = \begin{bmatrix}
\mathbf{0} \\
r_{j-1,j+1} \\
r_{j,j+1} := \frac{1}{r_{jj}}(t_{j-1}s_{j-1} + a_{j+1,j}a_{j+1,j+1}) \\
t_j := \frac{1}{r_{jj}}(t_{j-1}a_{j+1,j+1} - s_{j-1}a_{j+1,j})\\
a_{j+2,j+1} \\
0 \\
\vdots \\ 0
\end{bmatrix},\hspace{10mm}
Q_j \dots Q_1 \mathbf{a}_{j+2} = \begin{bmatrix}
\mathbf{0} \\
r_{j,j+2} := \frac{1}{r_{jj}}a_{j+1,j}a_{j+1,j+2} \\
s_j := \frac{1}{r_{jj}}t_{j-1} a_{j+1,j+2} \\
a_{j+2,j+2} \\
0 \\
\vdots \\
0
\end{bmatrix}.
$$
Finally we define, $r_{nn} = \frac{1}{r_{n-1,n-1}}(t_{n-2}a_{n,n} - a_{n, n-1}s_{n-2})$, to obtain,

$$
Q_{n-1}\dots Q_1 A = \begin{bmatrix}
r_{11} & r_{12} & r_{13} & 0      & \dots & 0\\
0      & r_{22} & r_{23} & r_{24} &       & 0\\
       & \ddots      & \ddots & \ddots & \ddots  & \vdots\\
       &             &    0    &  r_{n-2,n-2} & r_{n-2, n-1}     & r_{n-2,n}\\
       &             &        &    0    &     r_{n-1,n-1}    & r_{n-1,n}\\
       &             &        &        &     0    & r_{n,n}\\
\end{bmatrix} =: R
$$

so that $A = QR$, for $Q = Q_1^{-1}\dots Q_{n-1}^{-1}$, where each matrix $Q_j$ rotates the coordinates $(j,j+1)$ through the angle $\theta_j = -\arctan(a_{j+1, j}/t_{j-1})$, and thus each matrix $Q_j^{-1}$ rotates the coordinates $(j, j+1)$ through the angle $\arctan(a_{j+1, j}/t_{j-1})$. This will be important for Problem 4.3.

**END**

**Problem 4.2 (B)** Implement `Rotations` which represents an orthogonal matrix `Q` that is a product
of rotations of angle `Œ∏[k]`, each acting on the entries `k:k+1`. That is, it returns $Q = Q_1‚ãØQ_k$ such that
$$
Q_k[k:k+1,k:k+1] = 
\begin{bmatrix}
cos(Œ∏[k]) -sin(Œ∏[k])
sin(Œ∏[k]) cos(Œ∏[k])
\end{bmatrix}
$$

```julia
struct Rotations{T} <: AbstractMatrix{T}
    Œ∏::Vector{T}
end

import Base: *, size, getindex

size(Q::Rotations) = (length(Q.Œ∏)+1, length(Q.Œ∏)+1)


function *(Q::Rotations, x::AbstractVector)
    T = eltype(Q)
    y = convert(Vector{T}, x)
    # TODO: Apply Q in O(n) operations, modifying y in-place

    ## SOLUTION
    Œ∏ = Q.Œ∏
    #Does Q1....Qn x
    for k = length(Œ∏):-1:1
        #below has 4 ops to make the matrix and 12 to do the matrix-vector multiplication,
        #total operations will be 48n = O(n)
        c, s = cos(Œ∏[k]), sin(Œ∏[k])
        y[k:(k+1)] = [c -s; s c] * y[k:(k+1)]
    end
    ## END

    y
end

function getindex(Q::Rotations, k::Int, j::Int)
    # TODO: Return Q[k,j] in O(n) operations (hint: use *)

    ## SOLUTION
    #recall that A_kj = e_k'*A*e_j for any matrix A
    #so if we use * above, this will take O(n) operations
    n = size(Q)[1]
    ej = zeros(eltype(Q), n)
    ej[j] = 1
    #note, must be careful to ensure that ej is a VECTOR
    #not a MATRIX, otherwise * above will not be used
    Qj = Q * ej
    Qj[k]
    ## END
end

Œ∏1 = randn(5)
Q = Rotations(Œ∏1)
@test Q'Q ‚âà I
@test Rotations([œÄ/2, -œÄ/2]) ‚âà [0 0 -1; 1 0 0; 0 -1 0]
```

**Problem 4.3 (A)** Combine `Rotations` and `UpperTridiagonal` from last problem sheet
to implement a banded QR decomposition, `bandedqr`, that only takes $O(n)$ operations. Hint: use `atan(y,x)`
to determine the angle.
```julia
# First we include UpperTridiagonal from last problem sheet.
# bandedqr is below.
import Base: *, size, getindex, setindex!
struct UpperTridiagonal{T} <: AbstractMatrix{T}
    d::Vector{T}   # diagonal entries
    du::Vector{T}  # super-diagonal enries
    du2::Vector{T} # second-super-diagonal entries
end

size(U::UpperTridiagonal) = (length(U.d),length(U.d))

function getindex(U::UpperTridiagonal, k::Int, j::Int)
    d,du,du2 = U.d,U.du,U.du2
    # TODO: return U[k,j]
    if j - k == 0
        d[j]
    elseif j - k == 1
        du[k]
    elseif j - k == 2
        du2[k]
    else
        0
    end
end

function setindex!(U::UpperTridiagonal, v, k::Int, j::Int)
    d,du,du2 = U.d,U.du,U.du2
    if j > k+2
        error("Cannot modify off-band")
    end

    # TODO: modify d,du,du2 so that U[k,j] == v
    if j - k == 0
        d[k] = v
    elseif j - k == 1
        du[k] = v
    elseif j - k == 2
        du2[k] = v
    else
        error("Cannot modify off-band")
    end
    U = UpperTridiagonal(d,du,du2)

    U # by convention we return the matrix
end

function bandedqr(A::Tridiagonal)
    n = size(A, 1)
    Q = Rotations(zeros(n - 1)) # Assume Float64
    R = UpperTridiagonal(zeros(n), zeros(n - 1), zeros(n - 2))
    R[1, 1:2] = A[1, 1:2]
        
    for j = 1:n-1
        # angle of rotation
        Q.Œ∏[j] = atan(A[j+1, j], R[j, j])
        Œ∏ = -Q.Œ∏[j] # rotate in oppoiste direction 

        c, s = cos(Œ∏), sin(Œ∏)
        # [c -s; s c] represents the rotation that introduces a zero.

        ## TODO: modify rows k and k+1 of R to represent
        # applying the rotation. Note you will need to use row k+1 of A.
        
        ## SOLUTION
        # This is [c -s; s c] to j-th column, but ignore second row
        # which is zero
        R[j, j] = c * R[j, j] - s * A[j+1, j]
        # This is [c -s; s c] to (j+1)-th column
        R[j:j+1, j+1] = [c -s; s c] * [R[j, j+1]; A[j+1, j+1]]

        if j < n - 1
            # This is [c -s; s c] to (j+2)-th column, where R is still zero
            R[j:j+1, j+2] = [-s; c] * A[j+1, j+2]
        end
        ## END
    end
    Q, R
end

A = Tridiagonal([1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4])
Q, R = bandedqr(A)
@test Q*R ‚âà A
```


**Problem 4.4‚ãÜ (B)** Could one redesign the above to only use IEEE operatations (addition, multiplication, square-roots,
avoiding calls `atan`, `cos`, and `sin`)?
Would it have been possible to implement this algorithm using reflections?
If so, what would be the structure of a matrix whose columns are the vectors of reflections?

**SOLUTION**

Yes, one could avoid using ```atan```, ```cos```, and ```sin```. At each stage of the algorithm, we only use ```atan``` to compute ```Œ∏[i]``` which we then use to compute $Q_i^{-1}$ and thus $Q$ at the end. Instead, one could simply compute $Q_i^{-1}$ directly from $t_{i-1}$ and $a_{i+1, i}$ and save it at each stage. An efficient implementation of $Q_1^{-1}\dots Q_n^{-1}$ would then compute $Q$ at the end.
One could implement this using Householder Reflections as well. Interpreting the 'vectors of reflections' to mean the vectors which define each Householder reflection (padded with 0s at the front so that they are the same dimension and fit into a matrix), the matrix with these columns would be a lower bidiagonal matrix with bandwidths $(l, u) = (1, 0)$.

**END**

## 5. PLU decomposition

**Problem 5.1‚ãÜ (C)** Compute the PLU decompositions for the following matrices:
$$
\begin{bmatrix}
0 & 2 & 1 \\
2 & 6 & 1 \\
1 & 1 & 4
\end{bmatrix},
\begin{bmatrix}
1 & 2 & -1 & 0 \\
2 & 4 & -2 & 1 \\
-3 & -5 & 6 & 1 \\
-1 & 2 & 8 & -2
\end{bmatrix}
$$

**SOLUTION**
**Part 1**
Compute the PLU decomposition of,
$$
\begin{bmatrix}
0 & 2 & 1 \\
2 & 6 & 1 \\
1 & 1 & 4
\end{bmatrix}
$$
We begin by permuting the first and second row as |2| > |0|, hence,
$$P_1 =  \begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}, \hspace{10mm}
P_1A =  \begin{bmatrix}
2 & 6 & 1 \\
0 & 2 & 1 \\
1 & 1 & 4
\end{bmatrix}
$$
We then choose,
$$
L_1 = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
-\frac{1}{2} & 0 & 1
\end{bmatrix}, \hspace{10mm}
L_1P_1A =  \begin{bmatrix}
2 & 6 & 1 \\
0 & 2 & 1 \\
0 & -2 & \frac{7}{2}
\end{bmatrix}
$$
There is no need to permute at this stage, so $P_2 = I_3$, the $3-$dimensional identity. Then we can choose,
$$
L_2 = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}, \hspace{10mm}
L_2 P_2 L_1 P_1 A =  \begin{bmatrix}
2 & 6 & 1 \\
0 & 2 & 1 \\
0 & 0 & 9/2
\end{bmatrix} =: U
$$
Since $P_2 = I_3$, this reduces to $L_2L_1P_1A = U \Rightarrow A = P_1^{-1}L_1^{-1}L_2^{-1}U$. Since $P_1$ simply permutes two rows, it is its own inverse, and $L_1^{-1}L_2^{-1}$ is simply,
$$
L := L_1^{-1}L_2^{-1} =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
\frac{1}{2} & -1 & 1
\end{bmatrix}
$$
Hence, we have $A = PLU$, where,
$$
P = P_1^{-1}= \begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}, \hspace{10mm}
L = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
\frac{1}{2} & -1 & 1
\end{bmatrix}, \hspace{10mm}
U=\begin{bmatrix}
2 & 6 & 1 \\
0 & 2 & 1 \\
0 & 0 & 9/2
\end{bmatrix}
$$
We can quickly check:
```julia
P = [0 1 0; 1 0 0; 0 0 1]
L = [1 0 0; 0 1 0; 1/2 -1 1]
U = [2 6 1; 0 2 1; 0 0 9/2]

P*L*U
```

**Part 2**

Find the $PLU$ decomposition of,
$$
A = \begin{bmatrix}
1 & 2 & -1 & 0 \\
2 & 4 & -2 & 1 \\
-3 & -5 & 6 & 1 \\
-1 & 2 & 8 & -2
\end{bmatrix}
$$
We see that we must start with the permutation,
$$
P_1 :=\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}, \hspace{10mm}
P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
2 & 4 & -2 & 1 \\
1 & 2 & -1 & 0 \\
-1 & 2 & 8 & -2
\end{bmatrix}
$$
We then choose $L_1$,
$$
L_1 := \begin{bmatrix}
1 & 0 & 0 & 0 \\
\frac{2}{3} & 1 & 0 & 0 \\
\frac{1}{3} & 0 & 1 & 0 \\
-\frac{1}{3} & 0 & 0 & 1
\end{bmatrix}, \hspace{10mm}
L_1P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{2}{3} & 2 & \frac{5}{3} \\
0 & \frac{1}{3} & 1 & \frac{1}{3} \\
0 & \frac{11}{3} & 6 & -\frac{7}{3}
\end{bmatrix}
$$
We then choose $P_2$,
$$
P_2 :=\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix}, \hspace{10mm}
P_2L_1P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{11}{3} & 6 & -\frac{7}{3} \\
0 & \frac{1}{3} & 1 & \frac{1}{3} \\
0 & \frac{2}{3} & 2 & \frac{5}{3}
\end{bmatrix}
$$
We then choose $L_2$,
$$
L_2 := \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & -\frac{1}{11} & 1 & 0 \\
0 & -\frac{2}{11} & 0 & 1
\end{bmatrix}, \hspace{10mm}
L_2P_2L_1P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{11}{3} & 6 & -\frac{7}{3} \\
0 & 0 & \frac{5}{11} & \frac{6}{11} \\
0 & 0 & \frac{10}{11} & \frac{23}{11}
\end{bmatrix}
$$
We then choose $P_3$ to swap the third and fourth rows,
$$
P_3 :=\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}, \hspace{10mm}
P_3L_2P_2L_1P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{11}{3} & 6 & -\frac{7}{3} \\
0 & 0 & \frac{10}{11} & \frac{23}{11} \\
0 & 0 & \frac{5}{11} & \frac{6}{11}
\end{bmatrix}
$$
We complete the triangularisation by choosing $L_3$,
$$
L_3 := \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & -\frac{1}{2} & 1
\end{bmatrix}, \hspace{10mm}
L_3P_3L_2P_2L_1P_1A = \begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{11}{3} & 6 & -\frac{7}{3} \\
0 & 0 & \frac{10}{11} & \frac{23}{11} \\
0 & 0 & 0 & -\frac{1}{2}
\end{bmatrix}
$$
We now consider,
$$L_3P_3L_2P_2L_1P_1 = L_3\tilde{L_2}\tilde{L_1}P_3P_2P_1,$$
where $\tilde{L_1}$ and $\tilde{L_1}$ satisfy,
$$
\begin{align*}
P_3P_2L_1 &= \tilde{L_1}P_3P_2 \\
P_3L_2 &= \tilde{L_2}P_3
\end{align*}
$$
These can be computed via,
$$
\begin{align*}
\tilde{L_1} &= P_3P_2L_1P_2^{-1}P_3^{-1} \\
\tilde{L_2} &= P_3L_2P_3^{-1}
\end{align*}
$$
to obtain,
$$
\begin{align*}
\tilde{L_1} &= \begin{bmatrix}
1 & 0 & 0 & 0 \\
-\frac{1}{3} & 1 & 0 & 0 \\
\frac{2}{3} & 0 & 1 & 0 \\
\frac{1}{3} & 0 & 0 & 1 
\end{bmatrix} \\
\tilde{L_2} &= \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & -\frac{2}{11} & 1 & 0 \\
0 & -\frac{1}{11} & 0 & 1 
\end{bmatrix}
\end{align*}
$$
This gives us,
$$
L^{-1} = L_3 \tilde{L_2} \tilde{L_1} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
-\frac{1}{3} & 1 & 0 & 0 \\
\frac{2}{3} & -\frac{2}{11} & 1 & 0 \\
\frac{1}{3} & -\frac{1}{11} & -\frac{1}{2} & 1
\end{bmatrix},
$$
from which it is clear that,
$$
L =  \begin{bmatrix}
1 & 0 & 0 & 0 \\
\frac{1}{3} & 1 & 0 & 0 \\
-\frac{2}{3} & \frac{2}{11} & 1 & 0 \\
-\frac{1}{3} & \frac{1}{11} & \frac{1}{2} & 1
\end{bmatrix}
$$

Finally, we have that,
$$
P = P_1^{-1} P_2^{-1} P_3^{-1} = \begin{bmatrix}
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix},
$$
so that $A = PLU,$ with,
$$
P = P_1^{-1} P_2^{-1} P_3^{-1} = \begin{bmatrix}
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix}, \hspace{10mm}
L =  \begin{bmatrix}
1 & 0 & 0 & 0 \\
\frac{1}{3} & 1 & 0 & 0 \\
-\frac{2}{3} & \frac{2}{11} & 1 & 0 \\
-\frac{1}{3} & \frac{1}{11} & \frac{1}{2} & 1
\end{bmatrix}, \hspace{10mm}
U=\begin{bmatrix}
-3 & -5 & 6 & 1 \\
0 & \frac{11}{3} & 6 & -\frac{7}{3} \\
0 & 0 & \frac{10}{11} & \frac{23}{11} \\
0 & 0 & 0 & -\frac{1}{2}
\end{bmatrix}
$$
To check:
```julia
P=[0 0 0 1;0 0 1 0;1 0 0 0;0 1 0 0]
L=[1 0 0 0;1/3 1 0 0; -2/3 2/11 1 0; -1/3 1/11 1/2 1]
U=[-3 -5 6 1;0 11/3 6 -7/3;0 0 10/11 23/11;0 0 0 -1/2]
P*L*U
```

**END**