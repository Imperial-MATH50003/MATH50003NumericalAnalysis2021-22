# MATH50003 Numerical Analysis: Problem Sheet 4

This problem sheet explores least squares, the QR decomposition including for
tridiagonal matrices,
and the PLU decompositions.

Questions marked with a â‹† are meant to be completed without using a computer.

```julia
using LinearAlgebra, Plots
```



## 1. Least squares and QR decompositions

**Problem 1.1** Find and plot the best least squares fit of ${1 \over 5x^2 + 1}$ by degree $n$
polynomials for $n = 0,\ldots,10$ at 1000 evenly spaced points between $0$ and $1$.

**SOLUTION**
```julia
x = range(0, 1; length=1000)
pl = plot()
f = 1 ./ (5x.^2 .+ 1)
for n = 0:10
    A = x .^ (0:n)'
    c = A \ f
    plot!(x, A*c)
end
pl
```

**Problem 1.2â‹†** Show that every matrix has a QR decomposition such that the diagonal of $R$ is non-negative.
Make sure to inlude the case of more columns than rows.


**Problem 1.3â‹†** Show that the QR decomposition of a square invertible matrix is unique, 
provided that the diagonal of $R$ is positive.


**SOLUTION**

Assume there is a second decomposition also with positive diagonal
$$
A = QR = QÌƒ RÌƒ
$$
Then we know 
$$
Q^âŠ¤ QÌƒ = R RÌƒ^{-1}
$$
Note $Q^âŠ¤ QÌƒ$ is orthogonal, and $R RÌƒ^{-1}$ has positive eigenvalues (the diagonal), hence all $m$ eigenvalues of
$Q^âŠ¤ QÌƒ$ are 1. This means that $Q^âŠ¤ QÌƒ = I$ and hence $QÌƒ = Q$, which then implies $RÌƒ = R$.

âˆ

## 2. Gramâ€“Schmidt

**Problem 2.1â‹†** The modified Gramâ€“Schmidt algorithm is a slight variation of Gramâ€“Schmidt where
instead of computing
$$
ğ¯_j := ğš_j - \sum_{k=1}^{j-1} \underbrace{ğª_k^\top ğš_j}_{r_{kj}} ğª_k
$$
we compute it step-by-step:
$$
\begin{align*}
ğ¯_j^1 &:= ğš_j \\
ğ¯_j^{k+1} &:= ğ¯_j^k - ğª_k^\top ğ¯_j^k ğª_k
\end{align*}
$$
Show that $ğ¯_j^j = ğ¯_j$. 

**Problem 2.2** Complete the following
function implementing the modified Gramâ€“Schmidt algorithm:
```julia
function modifiedgramschmidt(A)
    m,n = size(A)
    m â‰¥ n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    for j = 1:n
        # TODO: Implement the Modified Gramâ€“Schmidt algorthm
    end
    Q,R
end
```

**Problem 2.3** Compare the orthogonality of `Q` between `gramschmidt` and `modifiedgramschmidt`
when applied to a `300 Ã— 300` random matrix.

## 3. Householder reflections


**Problem 3.1**
Complete the definition of `Reflections` which supports a sequence of reflections,
that is, 
$$
Q = Q_{ğ¯_1} \cdots Q_{ğ¯_n}
$$
where the vectors are stored as a matrix `V` whose $j$-th column is $ğ¯_j$, and
$$
Q_{ğ¯_j} = I - 2 ğ¯_j ğ¯_j^\top.
$$

```julia
struct Reflections{T} <: AbstractMatrix{T}
    V::Matrix{T}
end

import Base: *, size, getindex

size(Q::Reflections) = (size(Q.v,1), size(Q.v,1))


function *(Q::Reflections, x::AbstractVector)
    # TODO: Apply Q in O(mn) operations
end

function getindex(Q::Reflections, k::Int, j::Int)
    # TODO: Return Q[k,j] in O(mn) operations (hint: use *)
end
```

**Problem 3.2** Complete the following function that implements
 Householder QR using only $O(mn^2)$ operations:
```julia
function householderqr(A)
    m,n = size(A)
    R = copy(A)
    Q = Reflections(Matrix(1.0I, m, n))
    for j = 1:n
        # TODO: populate Q and R using O(m*(n-j)) operations
    end
    Q,R
end
```

## 4. Banded QR with Given's rotations

**Problem 4.1â‹†**  Describe an algorithm for computing the QR decomposition
of a tridiagonal matrix using rotations instead of reflections to upper-triangularise
column-by-column.

**SOLUTION**

Let $A$ be a tridiagonal matrix:

$$
A = \left[\begin{matrix} 
a_{11} & a_{12} & 0 & \cdots  &0\\
a_{21} & a_{22} & a_{23} & \ddots  &\vdots\\
0      & a_{32} & a_{33} & \ddots  &0\\
       & 0      & \ddots & \ddots  &a_{n-1,n}\\
       &        & 0      & a_{n,n-1}  & a_{nn} 
\end{matrix}\right] = 
\left[ \begin{matrix}
\mathbf{a}_1 & \mathbf{a}_2 &\cdots & \mathbf{a}_n
\end{matrix}\right],
$$
where each $\mathbf{a}_j \in \mathbb{R}^n$ and $[\mathbf{a}_j]_k = 0$ for $|j-k| > 1$.

Recall that,
$$
\frac{1}{\sqrt{a^2 + b^2}} \left[\begin{matrix}
a & b \\
-b & a
\end{matrix}\right] \left[\begin{matrix}
a \\ b
\end{matrix}\right] = \left[\begin{matrix}
\sqrt{a^2 + b^2} \\ 0
\end{matrix}\right],
$$
and that,
$$
\frac{1}{\sqrt{a^2 + b^2}} \left[\begin{matrix}
a & b \\
-b & a
\end{matrix}\right] = \left[\begin{matrix}
\cos \theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{matrix}\right],
$$
is a rotation matrix where $\theta = -\arctan(b/a)$. With this in mind, consider multiplying $A$ from the left by,
$$
Q_1 = \left[\begin{matrix}
\frac{a_{11}}{r_{11}} & \frac{a_{21}}{r_{11}} \\
-\frac{a_{21}}{r_{11}} & \frac{a_{11}}{r_{11}} \\
&&1\\
&&&\ddots\\
&&&&1
\end{matrix}\right],
$$
where $r_{11} = \sqrt{a_{11}^2 + a_{21}^2}$. This rotates dimensions 1 and 2 through angle $\theta=-\arctan(a_{21}/a_{11})$. We have,
$$
Q_1 \mathbf{a}_1 = \left[\begin{matrix}
r_{11} \\0 \\ \vdots \\ 0
\end{matrix}\right],\hspace{10mm}
Q_1 \mathbf{a}_2 = \left[\begin{matrix}
r_{12} := \frac{1}{r_{11}}(a_{11}a_{12} + a_{21}a_{22}) \\
t_1    := \frac{1}{r_{11}}(a_{11}a_{22} - a_{21}a_{12}) \\
a_{32} \\
0 \\
\vdots \\ 0
\end{matrix}\right],\hspace{10mm}
Q_1 \mathbf{a}_3 = \left[\begin{matrix}
r_{13} := \frac{1}{r_{11}}a_{21}a_{23} \\
s_1    := \frac{1}{r_{11}}a_{11}a_{23} \\
a_{33} \\
a_{43} \\
0 \\
\vdots \\ 0
\end{matrix}\right],\hspace{10mm}
Q_1 \mathbf{a}_k = \mathbf{a}_k \textrm{ for } k > 3.
$$
Then we take,
$$
Q_2 = \left[ \begin{matrix}
1 \\
& \frac{t_1}{r_{22}} & \frac{a_{32}}{r_{22}} \\
& -\frac{a_{32}}{r_{22}} & \frac{t_1}{r_{22}} \\
&&&1 \\
&&&&\ddots\\
&&&&&1
\end{matrix}\right],
$$
where $r_{22} = \sqrt{t_1^2 + a_{32}^2}$, a matrix which rotates dimensions 2 and 3 through angle $\theta_2 = -\arctan(a_{32}/t_1)$. Then,
$$
Q_2 Q_1 \mathbf{a}_{1} = Q_1\mathbf{a}_1 = \left[\begin{matrix}
r_{11} \\0 \\ \vdots \\ 0
\end{matrix}\right],\hspace{10mm}
Q_2 Q_1 \mathbf{a}_{2} = Q_2\left[\begin{matrix}
r_{12} \\
t_1    \\
a_{32} \\
0 \\
\vdots \\ 0
\end{matrix}\right] = \left[\begin{matrix}
r_{12} \\
r_{22} \\
0 \\
0 \\
\vdots \\
0
\end{matrix}\right],\hspace{10mm}
Q_2Q_1\mathbf{a}_3 = Q_2 \left[\begin{matrix}
r_{13} \\
s_1    \\
a_{33} \\
a_{43} \\
0 \\
\vdots \\ 0
\end{matrix}\right]
=\left[\begin{matrix}
r_{13} \\
r_{23} := \frac{1}{r_{22}}(t_1s_1 + a_{32}a_{33})  \\
t_2 := \frac{1}{r_{22}}(t_1a_{33} - a_{32}s_1) \\
a_{43} \\
0 \\
\vdots \\ 0
\end{matrix}\right]
$$

$$
Q_2Q_1\mathbf{a}_4 = Q_2\left[\begin{matrix}
0 \\
0 \\
a_{34} \\
a_{44} \\
a_{54} \\
0 \\
\vdots \\
0
\end{matrix}\right] = \left[\begin{matrix}
0 \\
r_{24} := \frac{1}{r_{22}}a_{32}a_{34} \\
s_2 := \frac{1}{r_{22}} t_1 a_{34} \\
a_{44} \\
a_{54} \\
0 \\
\vdots \\
0
\end{matrix}\right], \hspace{10mm}
Q_2Q_1 \mathbf{a}_k = \mathbf{a}_k \textrm{ for } k > 4.
$$

Now, for $j=3\rightarrow n-1$ we take,
$$
Q_j := \left[\begin{matrix}
\mathbf{I}_{j-1} \\
& \frac{t_{j-1}}{r_{jj}} & \frac{a_{j+1, j}}{r_{jj}} \\
& \frac{a_{j+1, j}}{r_{jj}} & \frac{t_{j-1}}{r_{jj}} \\
&&& \mathbf{I}_{n-j-1}
\end{matrix}\right],
$$
where $r_{jj} := \sqrt{t_{j-1}^2 + a_{j+1, j}^2}$.

This gives,
$$
Q_j \dots Q_1 \mathbf{a}_k =  \left[\begin{matrix}
\mathbf{0}_{k-3} \\
r_{k-2,k} \\
r_{k-1, k} \\
r_{k,k} \\
\mathbf{0}_{n-k}
\end{matrix}\right],
$$
for $k \leq j$, 
and,
$$
Q_j \dots Q_1 \mathbf{a}_{j+1} = \left[\begin{matrix}
\mathbf{0} \\
r_{j-1,j+1} \\
r_{j,j+1} := \frac{1}{r_{jj}}(t_{j-1}s_{j-1} + a_{j+1,j}a_{j+1,j+1}) \\
t_j := \frac{1}{r_{jj}}(t_{j-1}a_{j+1,j+1} - s_{j-1}a_{j+1,j})\\
a_{j+2,j+1} \\
0 \\
\vdots \\ 0
\end{matrix}\right],\hspace{10mm}
Q_j \dots Q_1 \mathbf{a}_{j+2} = \left[\begin{matrix}
\mathbf{0} \\
r_{j,j+2} := \frac{1}{r_{jj}}a_{j+1,j}a_{j+1,j+2} \\
s_j := \frac{1}{r_{jj}}t_{j-1} a_{j+1,j+2} \\
a_{j+2,j+2} \\
0 \\
\vdots \\ 
0
\end{matrix}\right].
$$
Finally we define, $r_{nn} = \frac{1}{r_{n-1,n-1}}(t_{n-2}a_{n,n} - a_{n, n-1}s_{n-2})$, to obtain,

$$
Q_{n-1}\dots Q_1 A = \left[\begin{matrix}
r_{11} & r_{12} & r_{13} & 0      & \dots & 0\\
0      & r_{22} & r_{23} & r_{24} &       & 0\\
       & \ddots      & \ddots & \ddots & \ddots  & \vdots\\
       &             &    0    &  r_{n-2,n-2} & r_{n-2, n-1}     & r_{n-2,n}\\
       &             &        &    0    &     r_{n-1,n-1}    & r_{n-1,n}\\
       &             &        &        &     0    & r_{n,n}\\
\end{matrix}\right] =: R
$$

so that $A = QR$, for $Q = Q_1^{-1}\dots Q_{n-1}^{-1}$, where each matrix $Q_j$ rotates the coordinates $(j,j+1)$ through the angle $\theta_j = -\arctan(a_{j+1, j}/t_{j-1})$, and thus each matrix $Q_j^{-1}$ rotates the coordinates $(j, j+1)$ through the angle $\arctan(a_{j+1, j}/t_{j-1})$. This will be important for Problem 4.3.


**Problem 4.2** Implement `Rotations` which represents an orthogonal matrix `Q` that is a product
 of rotations of angle `Î¸[k]`, each acting on the entries `k:k+1`.
```julia
struct Rotations{T} <: AbstractMatrix{T}
    Î¸::Vector{T}
end

import Base: *, size, getindex

size(Q::Rotations) = (length(Q.Î¸)+1, length(Q.Î¸)+1)


function *(Q::Rotations, x::AbstractVector)
    # TODO: Apply Q in O(n) operations
end

function getindex(Q::Rotations, k::Int, j::Int)
    # TODO: Return Q[k,j] in O(n) operations (hint: use *)
end
```

**SOLUTION**

First we make sure that the ```UpperTridiagonal``` class is defined, this will be where we store our values $r_{i,j}$:

```julia
struct UpperTridiagonal{T} <: AbstractMatrix{T}
    d::Vector{T}   # diagonal entries
    du::Vector{T}  # super-diagonal enries
    du2::Vector{T} # second-super-diagonal entries
end

size(U::UpperTridiagonal) = (length(U.d),length(U.d))

function getindex(U::UpperTridiagonal, k::Int, j::Int)
    d,du,du2 = U.d,U.du,U.du2
    # TODO: return U[k,j]
    if j - k == 0
        d[j]
    elseif j - k == 1
        du[k]
    elseif j - k == 2
        du2[k]
    else
        0
    end
end

function setindex!(U::UpperTridiagonal, v, k::Int, j::Int)
    d,du,du2 = U.d,U.du,U.du2
    if j > k+2
        error("Cannot modify off-band")
    end

    # TODO: modify d,du,du2 so that U[k,j] == v
    if j - k == 0
        d[k] = v
    elseif j - k == 1
        du[k] = v
    elseif j - k == 2
        du2[k] = v
    else
        error("Cannot modify off-band")
    end
    U = UpperTridiagonal(d,du,du2)

    U # by convention we return the matrix
end
```
Now we imlement the algorithm described above. Note that at each stage it is sufficient to compute and store the following six quantities:
\begin{align}
\theta_j &= \arctan(a_{j+1,j}/t_{j-1})\\
r_{jj} &= \sqrt{t_{j-1}^2 + a_{j+1,j}^2}, \\
r_{j,j+1} &= \frac{1}{r_{jj}}(t_{j-1}s_{j-1} + a_{j+1,j}a_{j+1,j+1}) \\
r_{j,j+2} &= \frac{1}{r_{jj}}a_{j+1,j}a_{j+1,j+2} \\
t_{j} &= \frac{1}{r_{jj}}(t_{j-1}a_{j+1,j+1}) - s_{j-1}a_{j+1, j} \\
s_j &= \frac{1}{r_{jj}}t_{j-1}a_{j+1,j+2}
\end{align}
Note that we store $\arctan(a_{j+1,j}/t_{j-1})$ as we are interested in $Q = Q_1^{-1}\dots Q_n^{-1}$, where each $Q_j^{-1}$ rotates the coordinates ```j:j+1``` through angle $\arctan(a_{j+1,j}/t_{j-1})$. We don't have to store any values for $Q$ as our implementation of ```Rotations``` above will construct $Q$ from the list of angles that we store.
```julia
using LinearAlgebra
function bandedqr(A::Tridiagonal)
    n = size(A,1)
    Q = Rotations(zeros(n-1)) # Assume Float64
    R = UpperTridiagonal(zeros(n), zeros(n-1), zeros(n-2))
    t = zeros(n-1)
    s = zeros(n-1)
    
    #we will do the first iteration outside of the loop as t and s have special values here
    #find the first angle, note we take the negative as we want the inverse of Q where QA = R
    Q.Î¸[1] = atan(A[2,1], A[1,1])
    
    #update values for R
    #R_11
    v = sqrt(A[1,1]^2 + A[2,1]^2)
    setindex!(R, v, 1, 1)
    #R_12
    v = 1/R[1,1] * (A[1,1]*A[1,2] + A[2,1]*A[2,2])
    setindex!(R, v, 1, 2)
    #R_13
    v = A[2,1]*A[2,3]/R[1,1]
    setindex!(R, v, 1, 3)
    
    #create initial storage values
    t[1] = (-A[2,1]*A[1,2] + A[1,1]*A[2,2])/R[1,1]
    s[1] = A[1,1]*A[2,3]/R[1,1]
    
    if n==2
       Q, R 
    end
    
    for j = 2:(n-1)
        Q.Î¸[j] = atan(A[j+1,j],t[j-1])
        
        v = sqrt(A[j+1,j]^2 + t[j-1]^2)
        setindex!(R, v, j, j)
        
        v=1/R[j,j]*(t[j-1]*s[j-1] + A[j+1,j]*A[j+1,j+1])
        setindex!(R, v, j, j+1)
        
        #only compute this if j+2 <= n
        #equivalently.        j < n-1
        if j < n-1
            v=1/R[j,j]*A[j+1,j]*A[j+1, j+2]
            setindex!(R, v, j, j+2)
        end
        
        t[j] = 1/R[j,j]*(-A[j+1,j]*s[j-1] + t[j-1]*A[j+1, j+1])
        
        #only compute this if j < n-1
        if j < n-1
            s[j] = 1/R[j,j]*t[j-1]*A[j+1, j+2]
        end
    end
    v = (-A[n, n-1]*s[n-2] + t[n-2]*A[n,n])/R[n-1,n-1]
    setindex!(R, v, n, n)
    #now we just need to invert Q
    
    Q,R
end
```
We can check an example:
```julia
A = Tridiagonal([1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4])
```
```julia
Q, R = bandedqr(A)
```
```julia
Q*R - A
```

**Problem 4.3** Combine `Rotations` and `UpperTridiagonal` from last problem sheet
to implement a banded QR decomposition that only takes $O(n)$ operations:
```julia
function bandedqr(A::Tridiagonal)
    n = size(A,1)
    Q = Rotations(zeros(n-1)) # Assume Float64
    R = UpperTridiagonal(zeros(n), zeros(n-1), zeros(n-2))
    for j = 1:n-1
        # TODO: populate Q and R
    end
    Q,R
end
```

**Problem 4.4â‹†** Could one redesign the above to only use IEEE operatations (addition, multiplication, square-roots, 
avoiding calls `atan`, `cos`, and `sin`)?
Would it have been possible to implement this algorithm using reflections?
If so, what would be the structure of a matrix whose columns are the vectors of reflections?

## 5. PLU decomposition

**Problem 5.1â‹†** Compute the PLU decompositions for the following matrices:
$$
\begin{bmatrix}
0 & 2 & 1 \\
2 & 6 & 1 \\
1 & 1 & 4
\end{bmatrix},
\begin{bmatrix}
1 & 2 & -1 & 0 \\
2 & 4 & -2 & 1 \\
-3 & -5 & 6 & 1 \\
-1 & 2 & 8 & -2
\end{bmatrix}
$$
