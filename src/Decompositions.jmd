# Decompositions and least squares


In this lecture, we look at several factorizations of a matrix. For a square or rectangular matrix $A âˆˆ â„^{m Ã— n}$ with $m â‰¥ n$, we consider:
1. The _QR decomposition_
$$
A = \underbrace{Q}_{m Ã— m} \underbrace{R}_{m Ã— n} = \begin{bmatrix} ğª_1 | \cdots | ğª_m \end{bmatrix} \begin{bmatrix} Ã— & \cdots & Ã— \\ & \ddots & \vdots \\ && Ã— \\ &&0 \\ &&\vdots \\ && 0 \end{bmatrix} 
$$
where $Q$ is orthogonal ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) and $R$ is _right triangular_.

2. The _reduced QR decomposition_
$$
A = \underbrace{QÌ‚}_{m Ã— n} \underbrace{RÌ‚}_{m Ã— m} = \begin{bmatrix} ğª_1 | \cdots | ğª_n \end{bmatrix} \begin{bmatrix} Ã— & \cdots & Ã— \\ & \ddots & \vdots \\ && Ã—  \end{bmatrix} 
$$
where $Q$ has orthogonal columns ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) 

For a square matrix we consider the _PLU decomposition_:
$$
A = P^âŠ¤ LU
$$
where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular.

Finally, for a square, _symmetric positive definite_ ($ğ±^âŠ¤ A ğ± > 0$ for all $ğ± âˆˆ â„^n$) 
matrix we consider the _Cholesky decomposition_:
$$
A = L L^âŠ¤
$$

The importance of these decomposition for square matrices is that their component pieces are easy to invert on a computer:
$$
\begin{align*}
A = P^âŠ¤ LU &\Rightarrow\qquad A^{-1}ğ› = U^{-1} L^{-1} P ğ› \\
A = QR &\Rightarrow\qquad A^{-1}ğ› = R^{-1} Q^\top ğ› \\
A = L L^âŠ¤ &\Rightarrow\qquad A^{-1}ğ› = L^{-âŠ¤} L^{-1} ğ›
\end{align*}
$$
and we saw last lecture that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$.
For rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find
$ğ±$ that minimizes the 2-norm
$$
\| A ğ± - ğ› \|.
$$

In this lecture we discuss the followng:

1. QR,  Reduced QR, and least squares: We discuss the QR decomposition and its usage in solving least squares problems.
We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt, and the Full QR decomposition using 
Householder reflections.
2. PLU decomposition: we discuss how the LU decomposition can be computed using Gaussian elimination, and the computation of
the PLU decomposition via Gaussian elimination with pivoting.
3. Cholesky decomposition: we introduce symmetric positive definite matrices and show that their LU decomposition can be re-interpreted
as a Cholesky decomposition.

```julia
using LinearAlgebra, Plots
```



## 1. QR, Reduced QR, and least squares

Here we consider rectangular matrices with more rows than columns.
A QR decomposition decomposes a matrix into an orthogonal matrix $Q$ times a right triangular matrix $R$. 
Note the QR decomposition contains within it the reduced QR decomposition:
$$
A = QR = \begin{bmatrix} QÌ‚ | ğª_{n+1} | â‹¯ | ğª_m \end{bmatrix} \begin{bmatrix} RÌ‚ \\  ğŸ_{m-n Ã— n} \end{bmatrix} = QÌ‚ RÌ‚.
$$

### Relationship with least squares

We can use it to solve a least squares problem using the norm-preserving property (see PS3) of orthogonal matrices:
$$
\| A ğ± - ğ› \| = \| Q R ğ± - ğ› \| = \| R ğ± - Q^âŠ¤ ğ› \| = \left \| 
\begin{bmatrix} RÌ‚ \\ ğŸ_{m-n Ã— n} \end{bmatrix} ğ± - \begin{bmatrix} QÌ‚^âŠ¤ \\ ğª_{n+1}^âŠ¤ \\ \vdots \\ ğª_m^âŠ¤ \end{bmatrix}     ğ› \right \|
$$
Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| RÌ‚ ğ± - QÌ‚^âŠ¤ ğ› \|
$$
This norm is minimisable if it is attained. Provided the column rank of $A$ is full, $RÌ‚$ will be invertible (Exercise: why is this?).
Thus we have the solution
$$
ğ± = RÌ‚^{-1} QÌ‚^âŠ¤ ğ›
$$

**Example (quadratic fit)** Suppose we want to find noisy data by a quadratic
$$
p(x) = a + bx + cx^2
$$
That is, we want to choose $a,b,c$ at data samples $x_1, \ldots, x_m$ so that the following is true:
$$
a + b x_k + c x_k^2 â‰ˆ f_k
$$
where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm
$$
\left\| \begin{bmatrix} 1 & x_1 & x_1^2 \\ \vdots & \vdots & \vdots \\ 1 & x_m & x_m^2 \end{bmatrix}
\begin{bmatrix} a \\ b \\ c \end{bmatrix} - \begin{bmatrix} f_1 \\ \vdots \\ f_m \end{bmatrix} \right \|
$$
We can solve this using the QR decomposition:
```julia
m,n = 100,3

x = range(0,1; length=m) # 100 points
f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic

A = [ones(n) x x.^2] # 100 x 3 matrix
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns

a,b,c = RÌ‚ \ QÌ‚'f
```
We can visualise the fit:
```julia
p = x -> a + b*x + c*x^2

scatter(x, f; label="samples", legend=:bottomright)
plot!(x, p.(x); label="quadratic")
```
Note that `\` with a rectangular system does the least squares by default:
```julia
A \ f
```





### Gramâ€“Schmidt and reduced QR


How do we compute the QR decomposition? We begin with a method
you may have seen before in another guise. Write
$$
A = \begin{bmatrix} ğš_1 | \dots | ğš_n \end{bmatrix}
$$
where $ğš_k \in  â„^m$.
Note that the column span of the first $j$ columns of $A$
will be the same as the first $j$ columns of $QÌ‚$:
$$
\span{ğš_1,\ldots,ğš_j} = \span{ğª_1,\ldots,ğª_j}
$$
In other words: the columns of $QÌ‚$ are an orthogonal basis
of the column span of $A$.
To see this: note that since `RÌ‚` is triangular we have:
```julia
A[:,1:j] = QÌ‚[:,1:j]*RÌ‚[1:j,1:j]
```
for all $j$, and the multiplication is taking linear combinations of
`QÌ‚[:,1:j]`. 
 
It is possible to find an orthogonal basis using the _Gramâ€“Schmidt algorithm_,
We construct it via induction:
assume that
$$
\span{ğš_1,\ldots,ğš_{j-1}} = \span{ğª_1,\ldots,ğª_{j-1}}
$$
where $ğª_1,\ldots,ğª_{j-1}$ are orthogonal:
$$
ğª_k^\top ğª_â„“ = Î´_{kâ„“} = \begin{cases} 1 & k == â„“ \\
                                            0 & \hbox{otherwise} \end{cases}.
$$
for $k,â„“ < 1$.
Define
$$
ğ¯_j := ğš_j - \sum_{k=1}^{j-1} \underbrace{ğª_k^\top ğš_j}_{v_{kj}} ğª_k
$$
so that for $k < j$
$$
ğª_k^\top ğ¯_j = ğª_k^\top ğš_j - \sum_{k=1}^{j-1} \underbrace{ğª_k^\top ğš_j}_{v_{kj}} ğª_k^\top ğª_k = 0.
$$
Then we define
$$
ğª_j := {ğ¯_j \over \|ğ¯_j\|}.
$$
which sastisfies the same properties as the assumption.

We now reinterpret this construction as a reduced QR decomposition.
Define
$$
r_{jj} := {1 \over \|ğ¯_j\|}, \qquad r_{kj} := {v_{kj} \over \|ğ¯_j\|}
$$
Then rearrange the definition we have
$$
ğš_j = \begin{bmatrix} ğª_1|\cdots|ğª_j \end{bmatrix} \begin{bmatrix} r_{1j} \\ \vdots \\ r_{jj} \end{bmatrix}
$$
Thus
$$
\begin{bmatrix} ğš_1|\cdots|ğš_j \end{bmatrix} r_{11} & \cdots & r_{1j} \\ & \ddots & \vdots \\ && r_{jj} \end{bmatrix}
$$
That is, we are computing the reduced QR decomposition column-by-column. Running this algorithm to $j = n$ completes the decomposition.

Unfortunately, the Gramâ€“Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point
accumulate in a disasterous way. This will be explored in the problem sheet.


### Householder reflections and QR

As an alternative, we will consider using Householder reflections to introduce zeros.
Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices
to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ 
(using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column,
that is, for
$$
Q_1 := Q_{ğš_1}^{Â±,\rm H},
$$
consider
$$
Q_1 A = \begin{bmatrix} r_{11} & r_{12} & \cdots & r_{1n} \\ 
& ğš_2^1 & \cdots & ğš_n^1   \end{bmatrix}
$$
where 
$$
r_{1j} :=  (Q_1 ğš_j)[1] \qquad \hbox{and} \qquad ğš_j^1 := (Q_1 ğš_j)[2:m],
$$
noting $r_{11} = Â±\|ğš_1\|$ and all entries of $ğš_1^1$ are zero (thus not included).
That is, we have made the first column triangular.

But now consider
$$
Q_2 := \begin{bmatrix} 1  \\ & Q_{ğš_2^1}^{Â±,\rm H} \end{bmatrix}
$$
so that
$$
Q_2 Q_1A  \begin{bmatrix} r_{11} & r_{12} & \cdots & r_{1n} \\ 
    & r_{22} & r_{23} \cdots & r_{2n} \\
&& ğš_3^2 & \cdots & ğš_n^2   \end{bmatrix}
$$
where 
$$
r_{2j} :=  (Q_2 ğš_j^1)[1] \qquad \hbox{and} \qquad ğš_j^2 := (Q_2 ğš_j^1)[2:m-1]
$$
Thus the first two columns are triangular. 

The inductive construction is thus clear. If we define $ğš_j^0 := ğš_j$ we
have the construction
$$
\begin{align*}
Q_j &:= \begin{bmatrix} I_{j-1 \times j-1}  \\ & Q_{ğš_j^j}^{Â±,\rm H} \end{bmatrix} \\
ğš_j^k &:= (Q_k ğš_j^{k-1})[2:m-k+1] \\
r_{kj} &:= (Q_k ğš_j^{k-1})[1]
\end{align*}
$$
Then
$$
Q_n \cdots Q_1 A = \underbrace{\begin{bmatrix} 
r_{11} & \cdots & r_{1n} \\ & \ddots & \vdots\\
                                        && r_{nn} \\&& 0 \\ && \vdots \\ && 0 \end{bmatrix}}_R
$$
i.e.
$$
A = \underbrace{Q_n \cdots Q_1}_Q R.
$$

The implementation is cleaner. We do a naive implementation here:
```julia
function householderreflection(x)
    y = copy(x)
    y[1] += sign(x[1])norm(x)
    w = y/norm(y)
    I - 2*w*w'
end

m,n = 7,5
A = randn(m, n)
R = copy(A)
Q = Matrix(1.0I, m, m)
for j = 1:n
    Qâ±¼ = householderreflection(R[j:end,j])
    R[j:end,:] = Qâ±¼*R[j:end,:]
    Q[:,j:end] = Q[:,j:end]*Qâ±¼
end
Q*R â‰ˆ A
```
Note because we are forming a full matrix representation of each Householder
reflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet
will consider a better implementation that takes $O(n^3)$ operations.


**Example** We will now do an example by hand. Consider the $4 \times 3$ matrix
$$
A = \begin{bmatrix} 
1 & 2 & 2 \\
1 & 1 & 2 \\
1 & 1 & 1 \\
1 & 1 & 1.
$$
For the first column we have
$$
Q_1 = I - {1 \over 6} \begin{bmatrix}3 \\ 1\\ 1\\ 1 \end{bmatrix} \begin{bmatrix}3 & 1& 1& 1 \end{bmatrix} = {1 \over 6} \begin{bmatrix} -3 & -3 & -3 & -3\\ -3 & 5 &-1 & -1 \\ -3 & -1 & 5 & -1 &-3 &-1 & -1 5 \end{bmatrix}
$$
and
$$
Q_1 A = {1 \over 6} \begin{bmatrix} -12 & -15 &  -18 \\  & -3 & 2 \\ &-3 &-4\\ -3 & -4 \end{bmatrix}
$$
Then 
$$
Q_2 = \begin{bmatrix} 1 \\ & I - 2
\end{bmatrix}
$$


## 2. PLU Decomposition

Just as Gramâ€“Schmidt can be reinterpreted as a reduced QR decomposition,
Gaussian elimination with pivoting can be interpreted as a PLU decomposition.


### LU Decomposition

Before discussing pivoting, consider standard Gaussian elimation where one row-reduces
to introduce zeros column-by-column. We will mimick the computation of the QR decomposition
to view this as a _triangular triangularisation_.

Consider the matrix
$$
L_ğ¯ = \begin{bmatrix} 1 \\ -{v_2 \over v_1} & 1 \\ \vdots &&\ddots \\
                -{v_n \over v_1}  &&& 1
                \end{bmatrix} = I -\left[{ğ¯ \over v_1}  - ğ_1 \right] ğ_1^\top
$$
so that
$$
L_ğ¯ ğ¯ = ğ¯ - (ğ¯ - v_1 ğ_1) = v_1 ğ_1.
$$
Note that this is immediately invertible:
$$
L_ğ¯^{-1} = \begin{bmatrix} 1 \\ {v_2 \over v_1} & 1 \\ \vdots &&\ddots \\
                {v_n \over v_1}  &&& 1
                \end{bmatrix} = I + \left[{ğ¯ \over v_1}  - ğ_1 \right] ğ_1^\top
$$


For $L_1 := L_{ğš_1}$ we have 
$$
L_1 A =  \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 
& ğš_2^1 & \cdots & ğš_n^1   \end{bmatrix}
$$
where $ğš_j^1 := (L_1 ğš_j)[2:m]$ and $u_{1j} = a_{1j}$. But now consider
$$
L_2 := \begin{bmatrix} 1  \\ & L_{ğš_2^1} \end{bmatrix}.
$$
Then
$$
L_2 L_1 A = \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 
    & u_{22} & u_{23} \cdots & u_{2n} \\
&& ğš_3^2 & \cdots & ğš_n^2   \end{bmatrix}
$$
where 
$$
r_{2j} :=  (ğš_j^1)[1] \qquad \hbox{and} \qquad ğš_j^2 := (L_2 ğš_j^1)[2:m-1]
$$
Thus the first two columns are triangular. 

The inductive construction is again clear. If we define $ğš_j^0 := ğš_j$ we
have the construction
$$
\begin{align*}
L_j &:= \begin{bmatrix} I_{j-1 \times j-1}  \\ & L_{ğš_j^j} \end{bmatrix} \\
ğš_j^k &:= (L_k ğš_j^{k-1})[2:m-k+1]
 \\
u_{kj} &:= (L_k ğš_j^{k-1})[1]
\end{align*}
$$
Then
$$
L_n \cdots L_1 A = \underbrace{\begin{bmatrix} 
u_{11} & \cdots & u_{1n} \\ & \ddots & \vdots\\
                                        && u_{nn}\end{bmatrix}}_U
$$
i.e.
$$
A = \underbrace{L_n^{-1} \cdots L_1^{-1}}_L R.
$$



### PLU Decomposition


## 3. Cholesky Decomposition

Cholesky Decomposition is a form of Gaussian elimination (without pivoting)
that exploits symmetry in the problem. It is only relevant for _symmetric positive definite_
matrices.

**Definition (positive definite)** A square matrix $A \in \bbR^{n \times n}$ is _positive definite_ if
for all $ğ± \in \bbR^n$ we have
$$
ğ±^\top A ğ± > 0
$$

