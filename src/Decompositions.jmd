# Decompositions and least squares


In this lecture, we look at several factorizations of a matrix. For a square or rectangular matrix $A âˆˆ â„^{m Ã— n}$ with $m â‰¥ n$, we consider:
1. The _QR decomposition_
$$
A = \underbrace{Q}_{m Ã— m} \underbrace{R}_{m Ã— n} = \begin{bmatrix} ðª_1 | \cdots ðª_m \end{pmatrix} \begin{bmatrix} Ã— & \cdots & Ã— \\ & \ddots & \vdots \\ && Ã— \\ &&0 \\ &&\vdots \\ && 0 \end{bmatrix} 
$$
where $Q$ is orthogonal and $R$ is _right triangular_.
2. The _reduced QR decomposition_
$$
A = \underbrace{QÌ‚}_{m Ã— n} \underbrace{RÌ‚}_{m Ã— m} = \begin{bmatrix} ðª_1 | \cdots ðª_n \end{pmatrix} \begin{bmatrix} Ã— & \cdots & Ã— \\ & \ddots & \vdots \\ && Ã—  \end{bmatrix} 
$$
where $Q$ has orthogonal columns ($Q^âŠ¤Q = I$) 

For a square matrix we consider the _PLU decomposition_:
$$
A = P^âŠ¤ LU
$$
where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular.

Finally, for a square, _symmetric positive definite_ ($ð±^âŠ¤ A ð± > 0$ for all $ð± âˆˆ â„^n$) 
matrix we consider the _Cholesky decomposition_:
$$
A = L L^âŠ¤
$$

The importance of these decomposition for square matrices is that their component pieces are easy to invert on a computer:
$$
\begin{align*}
A = P^âŠ¤ LU &\Rightarrow A^{-1}ð› = U^{-1} L^{-1} P ð› \\
A = QR &\Rightarrow A^{-1}ð› = R^{-1} Q^\top ð› \\
A = L L^âŠ¤ &\Rightarrow A^{-1}ð› = L^{-âŠ¤} L^{-1} ð› \\
\end{align*}
and we saw last lecture that triangular and orthogonal matrices are easy to invert when applied to a vector $ð›$.
For rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find
$ð±$ that minimizes the 2-norm
$$
\| A ð± - ð› \|.
$$

In this lecture we discuss the followng:

1. QR,  Reduced QR, and least squares: We discuss the QR decomposition and its usage in solving least squares problems.
We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt, and the Full QR decomposition using 
Householder reflections.
2. PLU decomposition: we discuss how the LU decomposition can be computed using Gaussian elimination, and the computation of
the PLU decomposition via Gaussian elimination with pivoting.
3. Cholesky decomposition: we introduce symmetric positive definite matrices and show that their LU decomposition can be re-interpreted
as a Cholesky decomposition.




## 1. QR, Reduced QR, and least squares

A QR decomposition decomposes a matrix into an orthogonal matrix $Q$ times a right triangular matrix $R$. 
Note the QR decomposition contains within it the reduced QR decomposition:
$$
A = QR = \begin{bmatrix} QÌ‚ | ðª_{n+1} | â‹¯ | ðª_m \end{pmatrix} \begin{bmatrix} RÌ‚ \\  ðŸŽ_{m-n Ã— n} \end{pmatrix} = QÌ‚ RÌ‚.
$$

### Relationship with least squares

We can use it to solve a least squares problem using the norm-preserving property (see PS3) of orthogonal matrices:
$$
\| A ð± - ð› \| = \| Q R ð± - ð› \| = \| R ð± - Q^âŠ¤ ð› \| = \left \| 
\begin{bmatrix} RÌ‚ \\ ðŸŽ_{m-n Ã— n} \end{bmatrix} ð± - \begin{bmatrix} QÌ‚^âŠ¤ \\ ðª_{n+1}^âŠ¤ \\ \vdots \\ ðª_m^âŠ¤ \end{pmatrix}     ð› \right \|
$$
Now note that the rows $k > n$ are independent of $ð±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| RÌ‚ ð± - QÌ‚^âŠ¤ ð› \|
$$
This norm is minimisable if it is attained. Provided the column rank of $A$ is full, $RÌ‚$ will be invertible (Exercise: why is this?).
Thus we have the solution
$$
ð± = RÌ‚^{-1} QÌ‚^âŠ¤ ð›
$$

Let's look at an example using `\` to do a least squares solve. Suppose we want to find noisy data by a quadratic
$$
p(x) = a + bx + cx^2
$$
That is, we want to choose $a,b,c$ at data samples $x_1, \ldots, x_m$ so that the following is true:
$$
a + b x_k + c x_k^2 â‰ˆ f_k
$$
where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise
$$
\left\| \begin{bmatrix} 1 & x_1 & x_1^2 \\ \vdots & \vdots & \vdots \\ 1 & x_m & x_m^2 \end{bmatrix}
\begin{bmatrix} a \\ b \\ c \end{bmatrix} - \begin{bmatrix} f_1 \\ \vdots \\ f_m \end{bmatrix}
$$
We can solve this using the QR decomposition:
```julia
x = range(0,1; length=100) # 1000 points
f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic

A = [ones(length(x)) x x.^2]
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:3] # Q represents full orthogonal matrix so we take first 3 columns

a,b,c = R \ QÌ‚'f
```
We can visualise the fit:
```julia
p = x -> a + b*x + c*x^2

scatter(x, f; label="samples", legend=:bottomright)
plot!(x, p.(x); label="quadratic")
```
Note that `\` with a rectangular system does least squares by default:
```julia
A \ f
```





### Gramâ€“Schmidt and reduced QR

We will now reinterpret 

### Householder reflections and QR





## 2. PLU Decomposition


## 3. 
