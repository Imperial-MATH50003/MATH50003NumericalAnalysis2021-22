# Decompositions and least squares


We now look at several decompositions (or factorisations) 
of a matrix into products of structured matrices, and their use in solving least squares problems and linear systems.
For a square or rectangular matrix $A âˆˆ â„^{m Ã— n}$ with more rows than columns ($m â‰¥ n$), we consider:
1. The _QR decomposition_
$$
A = Q R = \underbrace{\begin{bmatrix} ğª_1 | \cdots | ğª_m \end{bmatrix}}_{m Ã— m} \underbrace{\begin{bmatrix} Ã— & \cdots & Ã— \\ & â‹± & â‹® \\ && Ã— \\ &&0 \\ &&â‹® \\ && 0 \end{bmatrix}}_{m Ã— n}
$$
where $Q$ is orthogonal ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) and $R$ is _right triangular_, which means it 
is only nonzero on or to the right of the diagonal.

2. The _reduced QR decomposition_
$$
A = QÌ‚ RÌ‚ = \underbrace{\begin{bmatrix} ğª_1 | \cdots | ğª_n \end{bmatrix}}_{m Ã— n} \underbrace{\begin{bmatrix} Ã— & \cdots & Ã— \\ & â‹± & â‹® \\ && Ã—  \end{bmatrix}}_{n Ã— n}
$$
where $Q$ has orthogonal columns ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) and $RÌ‚$ is upper triangular.

For a square matrix we consider the _PLU decomposition_:
$$
A = P^âŠ¤ LU
$$
where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular.

Finally, for a square, _symmetric positive definite_ ($ğ±^âŠ¤ A ğ± > 0$ for all $ğ± âˆˆ â„^n$, $ğ± \neq 0$) 
matrix we consider the _Cholesky decomposition_:
$$
A = L L^âŠ¤
$$

The importance of these decomposition for square matrices is that their component pieces are easy to invert on a computer:
$$
\begin{align*}
A = P^âŠ¤ LU &\Rightarrow\qquad A^{-1}ğ› = U^{-1} L^{-1} P ğ› \\
A = QR &\Rightarrow\qquad A^{-1}ğ› = R^{-1} Q^\top ğ› \\
A = L L^âŠ¤ &\Rightarrow\qquad A^{-1}ğ› = L^{-âŠ¤} L^{-1} ğ›
\end{align*}
$$
and we saw last lecture that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$,
e.g., using forward/back-substitution.
For rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find
$ğ±$ that minimizes the 2-norm
$$
\| A ğ± - ğ› \|.
$$

In this lecture we discuss the followng:

1. QR and least squares: We discuss the QR decomposition and its usage in solving least squares problems.
2. Reduced QR and Gramâ€“Schmidt: We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt.
3. Householder reflections and QR: We discuss comuting the  QR decomposition using Householder reflections.
2. PLU decomposition: we discuss how the LU decomposition can be computed using Gaussian elimination, and the computation of
the PLU decomposition via Gaussian elimination with pivoting.
3. Cholesky decomposition: we introduce symmetric positive definite matrices and show that their LU decomposition can be re-interpreted
as a Cholesky decomposition.
4. Timings: we see the relative trade-off in speed between the different decompositions.

```julia
using LinearAlgebra, Plots, BenchmarkTools
```



## 1. QR and least squares

Here we consider rectangular matrices with more rows than columns.
A QR decomposition decomposes a matrix into an orthogonal matrix $Q$ times a right triangular matrix $R$. 
Note the QR decomposition contains within it the reduced QR decomposition:
$$
A = QR = \begin{bmatrix} QÌ‚ | ğª_{n+1} | â‹¯ | ğª_m \end{bmatrix} \begin{bmatrix} RÌ‚ \\  ğŸ_{m-n Ã— n} \end{bmatrix} = QÌ‚ RÌ‚.
$$


We can use it to solve a least squares problem using the norm-preserving property (see PS3) of orthogonal matrices:
$$
\| A ğ± - ğ› \| = \| Q R ğ± - ğ› \| = \| R ğ± - Q^âŠ¤ ğ› \| = \left \| 
\begin{bmatrix} RÌ‚ \\ ğŸ_{m-n Ã— n} \end{bmatrix} ğ± - \begin{bmatrix} QÌ‚^âŠ¤ \\ ğª_{n+1}^âŠ¤ \\ â‹® \\ ğª_m^âŠ¤ \end{bmatrix}     ğ› \right \|
$$
Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| RÌ‚ ğ± - QÌ‚^âŠ¤ ğ› \|
$$
This norm is minimisable if it is attained. Provided the column rank of $A$ is full, $RÌ‚$ will be invertible (Exercise: why is this?).
Thus we have the solution
$$
ğ± = RÌ‚^{-1} QÌ‚^âŠ¤ ğ›
$$

**Example (quadratic fit)** Suppose we want to fit noisy data by a quadratic
$$
p(x) = pâ‚€ + pâ‚ x + pâ‚‚ x^2
$$
That is, we want to choose $pâ‚€,pâ‚,pâ‚‚$ at data samples $x_1, \ldots, x_m$ so that the following is true:
$$
pâ‚€ + pâ‚ x_k + pâ‚‚ x_k^2 â‰ˆ f_k
$$
where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm
$$
\left\| \begin{bmatrix} 1 & x_1 & x_1^2 \\ â‹® & â‹® & â‹® \\ 1 & x_m & x_m^2 \end{bmatrix}
\begin{bmatrix} pâ‚€ \\ pâ‚ \\ pâ‚‚ \end{bmatrix} - \begin{bmatrix} f_1 \\ â‹® \\ f_m \end{bmatrix} \right \|
$$
We can solve this using the QR decomposition:
```julia
m,n = 100,3

x = range(0,1; length=m) # 100 points
f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic

A = x .^ (0:2)'  # 100 x 3 matrix, equivalent to [ones(m) x x.^2]
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns

pâ‚€,pâ‚,pâ‚‚ = RÌ‚ \ QÌ‚'f
```
We can visualise the fit:
```julia
p = x -> pâ‚€ + pâ‚*x + pâ‚‚*x^2

scatter(x, f; label="samples", legend=:bottomright)
plot!(x, p.(x); label="quadratic")
```
Note that `\` with a rectangular system does least squares by default:
```julia
A \ f
```





## 2. Reduced QR and Gramâ€“Schmidt


How do we compute the QR decomposition? We begin with a method
you may have seen before in another guise. Write
$$
A = \begin{bmatrix} ğš_1 | \dots | ğš_n \end{bmatrix}
$$
where $ğš_k \in  â„^m$ and assume they are linearly independent ($A$ has full column rank).
Note that the column span of the first $j$ columns of $A$
will be the same as the first $j$ columns of $QÌ‚$, as
$RÌ‚$ must be non-singular:
$$
\hbox{span}(ğš_1,\ldots,ğš_j) = \hbox{span}(ğª_1,\ldots,ğª_j)
$$
In other words: the columns of $QÌ‚$ are an orthogonal basis
of the column span of $A$.
To see this note that since `RÌ‚` is triangular we have
$$
\begin{bmatrix} ğš_1 | \dots | ğš_j \end{bmatrix} = \begin{bmatrix} ğª_1 | \dots | ğª_j \end{bmatrix}  RÌ‚[1:j,1:j]
$$
for all $j$. That is, if $ğ¯ âˆˆ \hbox{span}(ğš_1,\ldots,ğš_j)$ we have for $ğœ âˆˆ â„^j$
$$
ğ¯ = \begin{bmatrix} ğš_1 | \dots | ğš_j \end{bmatrix} ğœ = \begin{bmatrix} ğª_1 | \dots | ğª_j \end{bmatrix}  RÌ‚[1:j,1:j] ğœ âˆˆ \hbox{span}(ğª_1,\ldots,ğª_j)
$$
 while if $ğ° âˆˆ \hbox{span}(ğª_1,\ldots,ğª_j)$ we have for $ğ âˆˆ â„^j$
$$
ğ° = \begin{bmatrix} ğª_1 | \dots | ğª_j \end{bmatrix} ğ  =  \begin{bmatrix} ğš_1 | \dots | ğš_j \end{bmatrix} RÌ‚[1:j,1:j]^{-1} ğ âˆˆ  \hbox{span}(ğš_1,\ldots,ğš_j).
$$

 
It is possible to find an orthogonal basis using the _Gramâ€“Schmidt algorithm_,
We construct it via induction:
assume that
$$
\hbox{span}(ğš_1,\ldots,ğš_{j-1}) = \hbox{span}(ğª_1,\ldots,ğª_{j-1})
$$
where $ğª_1,\ldots,ğª_{j-1}$ are orthogonal:
$$
ğª_k^\top ğª_â„“ = Î´_{kâ„“} = \begin{cases} 1 & k = â„“ \\
                                            0 & \hbox{otherwise} \end{cases}.
$$
for $k,â„“ < j$.
Define
$$
ğ¯_j := ğš_j - \sum_{k=1}^{j-1} \underbrace{ğª_k^\top ğš_j}_{r_{kj}} ğª_k
$$
so that for $k < j$
$$
ğª_k^\top ğ¯_j = ğª_k^\top ğš_j - \sum_{k=1}^{j-1} \underbrace{ğª_k^\top ğš_j}_{r_{kj}} ğª_k^\top ğª_k = 0.
$$
Then we define
$$
ğª_j := {ğ¯_j \over \|ğ¯_j\|}.
$$
which sastisfies $ğª_k^\top ğª_j =Î´_{kj}$ for $k \leq j$.

We now reinterpret this construction as a reduced QR decomposition.
Define
$
r_{jj} := {\|ğ¯_j\|}
$
Then rearrange the definition we have
$$
ğš_j = \begin{bmatrix} ğª_1|\cdots|ğª_j \end{bmatrix} \begin{bmatrix} r_{1j} \\ â‹® \\ r_{jj} \end{bmatrix}
$$
Thus
$$
\begin{bmatrix} ğš_1|\cdots|ğš_j \end{bmatrix} 
\begin{bmatrix} r_{11} & \cdots & r_{1j} \\ & â‹± & â‹® \\ && r_{jj} \end{bmatrix}
$$
That is, we are computing the reduced QR decomposition column-by-column. 
Running this algorithm to $j = n$ completes the decomposition.

### Gramâ€“Schmidt in action

We are going to compute the reduced QR of a random matrix
```julia
m,n = 5,4
A = randn(m,n)
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n]
```
The first column of `QÌ‚` is indeed a normalised first column of `A`:
```julia
R = zeros(n,n)
Q = zeros(m,n)
R[1,1] = norm(A[:,1])
Q[:,1] = A[:,1]/R[1,1]
```
We now determine the next entries as
```julia
R[1,2] = Q[:,1]'A[:,2]
v = A[:,2] - Q[:,1]*R[1,2]
R[2,2] = norm(v)
Q[:,2] = v/R[2,2]
```
And the third column is then:
```julia
R[1,3] = Q[:,1]'A[:,3]
R[2,3] = Q[:,2]'A[:,3]
v = A[:,3] - Q[:,1:2]*R[1:2,3]
R[3,3] = norm(v)
Q[:,3] = v/R[3,3]
```
(Note the signs may not necessarily match.)

We can clean this up as a simple algorithm:
```julia
function gramschmidt(A)
    m,n = size(A)
    m â‰¥ n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    for j = 1:n
        for k = 1:j-1
            R[k,j] = Q[:,k]'*A[:,j]
        end
        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j]
        R[j,j] = norm(v)
        Q[:,j] = v/R[j,j]
    end
    Q,R
end

Q,R = gramschmidt(A)
norm(A - Q*R)
```


### Complexity and stability

We see within the `for j = 1:n` loop that we have $O(mj)$ operations. Thus the 
total complexity is $O(m n^2)$ operations.


Unfortunately, the Gramâ€“Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point
accumulate in a way that we lose orthogonality:
```julia
A = randn(300,300)
Q,R = gramschmidt(A)
norm(Q'Q-I)
```

## 3. Householder reflections and QR

As an alternative, we will consider using Householder reflections to introduce zeros below
the diagonal.
Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices
to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ 
(using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column,
that is, for
$$
Q_1 := Q_{ğš_1}^{\rm H},
$$
consider
$$
Q_1 A = \begin{bmatrix} \times & \times & \cdots & \times \\
& \times & \cdots & \times \\
                    & â‹® & â‹± & â‹® \\
                    & \times & \cdots & \times \end{bmatrix} = 
\begin{bmatrix} r_{11} & r_{12} & \cdots & r_{1n} \\ 
& ğš_2^1 & \cdots & ğš_n^1   \end{bmatrix}
$$
where 
$$
r_{1j} :=  (Q_1 ğš_j)[1] \qquad \hbox{and} \qquad ğš_j^1 := (Q_1 ğš_j)[2:m],
$$
noting $r_{11} = -\hbox{sign}(a_{11})\|ğš_1\|$ and all entries of $ğš_1^1$ are zero (thus not included).
That is, we have made the first column triangular.

But now consider
$$
Q_2 := \begin{bmatrix} 1  \\ & Q_{ğš_2^1}^{\rm H} \end{bmatrix} = Q_{\begin{bmatrix} 0 \\ ğš_2^1 \end{bmatrix}}^H
$$
so that
$$
Q_2 Q_1A = \begin{bmatrix} \times & \times & \times & \cdots & \times \\
& \times & \times  & \cdots & \times \\
                    && â‹® & â‹± & â‹® \\
                    && \times & \cdots & \times \end{bmatrix}  =  \begin{bmatrix} r_{11} & r_{12} & r_{13} & \cdots & r_{1n} \\ 
    & r_{22} & r_{23} & \cdots & r_{2n} \\
&& ğš_3^2 & \cdots & ğš_n^2   \end{bmatrix}
$$
where 
$$
r_{2j} :=  (Q_2 ğš_j^1)[1] \qquad \hbox{and} \qquad ğš_j^2 := (Q_2 ğš_j^1)[2:m-1]
$$
Thus the first two columns are triangular. 

The inductive construction is thus clear. If we define $ğš_j^0 := ğš_j$ we
have the construction
$$
\begin{align*}
Q_j &:= \begin{bmatrix} I_{j-1 \times j-1}  \\ & Q_{ğš_j^j}^{Â±,\rm H} \end{bmatrix} \\
ğš_j^k &:= (Q_k ğš_j^{k-1})[2:m-k+1] \\
r_{kj} &:= (Q_k ğš_j^{k-1})[1]
\end{align*}
$$
Then
$$
Q_n \cdots Q_1 A = \underbrace{\begin{bmatrix} 
r_{11} & \cdots & r_{1n} \\ & â‹± & â‹®\\
                                        && r_{nn} \\&& 0 \\ && â‹® \\ && 0 \end{bmatrix}}_R
$$
i.e.
$$
A = \underbrace{Q_1 \cdots Q_n}_Q R.
$$

The implementation is cleaner. We do a naive implementation here:
```julia
function householderreflection(x)
    y = copy(x)
    # we cannot use sign(x[1]) in case x[1] == 0
    y[1] += (x[1] â‰¥ 0 ? 1 : -1)*norm(x) 
    w = y/norm(y)
    I - 2*w*w'
end
function householderqr(A)
    m,n = size(A)
    R = copy(A)
    Q = Matrix(1.0I, m, m)
    for j = 1:n
        Qâ±¼ = householderreflection(R[j:end,j])
        R[j:end,:] = Qâ±¼*R[j:end,:]
        Q[:,j:end] = Q[:,j:end]*Qâ±¼
    end
    Q,R
end

m,n = 7,5
A = randn(m, n)
Q,R = householderqr(A)
Q*R â‰ˆ A
```
Note because we are forming a full matrix representation of each Householder
reflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet
will consider a better implementation that takes $O(n^3)$ operations.


**Example** We will now do an example by hand. Consider the $4 \times 3$ matrix
$$
A = \begin{bmatrix} 
1 & 2 & -1 \\ 
0 & 15 & 18 \\
-2 & -4 & -4 \\
-2 & -4 & -10
\end{bmatrix}
$$
For the first column we have
$$
Q_1 = I - {1 \over 12} \begin{bmatrix} 4 \\ 0 \\ -2 \\ -2 \end{bmatrix} \begin{bmatrix} 4 & 0 & -2 & -2 \end{bmatrix} =
{1 \over 3} \begin{bmatrix}
-1 & 0 & 2 & 2 \\
0 & 3 & 0 & 0 \\
2 & 0 & 2 & -1 \\
2 & 0 & -1 &2
\end{bmatrix}
$$
so that
$$
Q_1 A = \begin{bmatrix} -3 & -6 & -9 \\
 & 15 & 18 \\
  & 0 & 0 \\
& 0 & -6
\end{bmatrix}
$$
In this example the next column is already upper-triangular,
but because of our choice of reflection we will end up swapping the sign, that is
$$
Q_2 = \begin{bmatrix} 1 \\ & -1 \\ && 1 \\ &&& 1 \end{bmatrix}
$$
so that
$$
Q_2 Q_1 A = \begin{bmatrix} -3 & -6 & -9 \\
 & -15 & -18 \\
  & 0 & 0 \\
& 0 & -6
\end{bmatrix}
$$
The final reflection is
$$
Q_3 = \begin{bmatrix} I_{2 \times 2} \\ &  I - \begin{bmatrix} 1 \\ -1 \end{bmatrix} \begin{bmatrix} 1 & -1 \end{bmatrix} 
\end{bmatrix} = \begin{bmatrix} Ã¥1 \\ & 1 \\ & & 0 & 1 \\ & & 1 & 0 \end{bmatrix}
$$
giving us
$$
Q_3 Q_2 Q_1 A = \underbrace{\begin{bmatrix} -3 & -6 & -9 \\
 & -15 & -18 \\
  &  & -6 \\
&  & 0
\end{bmatrix}}_R
$$
That is,
$$
A = Q_1 Q_2 Q_3 R = \underbrace{{1 \over 3} \begin{bmatrix}
-1 & 0 & 2 & 2 \\
0 & 3 & 0 & 0 \\
2 & 0 & -1 & 2 \\
2 & 0 & 2 &-1
\end{bmatrix}}_Q \underbrace{\begin{bmatrix} -3 & -6 & -9 \\
 & -15 & -18 \\
  &  & -6 \\
&  & 0
\end{bmatrix}}_R = \underbrace{{1 \over 3} \begin{bmatrix}
-1 & 0 & 2  \\
0 & 3 & 0  \\
2 & 0 & -1  \\
2 & 0 & 2 
\end{bmatrix}}_QÌ‚  \underbrace{\begin{bmatrix} -3 & -6 & -9 \\
 & -15 & -18 \\
  &  & -6 
\end{bmatrix}}_RÌ‚
$$



## 2. PLU Decomposition

Just as Gramâ€“Schmidt can be reinterpreted as a reduced QR decomposition,
Gaussian elimination with pivoting can be interpreted as a PLU decomposition.


### Special "one-column" lower triangular matrices


Consider the following set of $n Ã— n$ lower triangular
matrices which equals identity apart from one-column:
$$
{\cal L}_j := \left\{I + \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ : ğ¥_j âˆˆ â„^{n-j} \right\}
$$
where  $ğŸ_j$ denotes the zero vector of length $j$. 
That is, if $L_j âˆˆ {\cal L}_j$ then it is equal to the identity matrix apart from in the $j$ th column:
$$
L_j = \begin{bmatrix}
        1 \\ & {â‹±} \\ && 1 \\
                    && â„“_{j+1,j} & 1 \\
                    && â‹® && \dots \\
                    && â„“_{n,j} & & & 1 \end{bmatrix} = 
$$

These satisify the following special properties:

**Proposition (one-column lower triangular inverse)**
If $L_j \in {\cal L}_j$ then
$$
L_j^{-1}  = I - \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ = \begin{bmatrix}
        1 \\ & â‹± \\ && 1 \\
                    &&-â„“_{j+1,j} & 1 \\
                    &&â‹® && \dots \\
                    &&-â„“_{n,j} & & & 1 \end{bmatrix} âˆˆ {\cal L}_j.
$$


**Proposition (one-column lower triangular multiplication)**
If $L_j \in {\cal L}_j$ and $L_k \in {\cal L}_k$ for $k â‰¥ j$ then
$$
L_j L_k =  I + \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ +  \begin{bmatrix} ğŸ_k \\ ğ¥_k \end{bmatrix} ğ_k^âŠ¤
$$


**Lemma (one-column lower triangular with pivoting)**
If $Ïƒ$ is a permutation that leaves the first $j$
rows fixed (that is, $Ïƒ_â„“ = â„“$ for $â„“ â‰¤Â j$) and $L_j âˆˆ {\cal L}_j$ then
$$
P_Ïƒ L_j=  \tilde L_j P_Ïƒ
$$
where $\tilde L_j âˆˆ {\cal L}_j$.

**Proof**
Write
$$
P_Ïƒ = \begin{bmatrix} I_j \\ & P_Ï„ \end{bmatrix}
$$
where $Ï„$ is the permutation with Cauchy notation
$$
\begin{pmatrix}
1 & \cdots & n-j \\
Ïƒ_{j+1}-j & â‹¯ & Ïƒ_n-j
\end{pmatrix}
$$
Then we have
$$
P_Ïƒ L_j = P_Ïƒ + \begin{bmatrix} ğŸ_j \\ P_Ï„ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ =
\underbrace{(I +  \begin{bmatrix} ğŸ_j \\ P_Ï„ ğ¥_j \end{bmatrix} ğ_j^âŠ¤)}_{\tilde L_j} P_Ïƒ
$$
noting that $ğ_j^âŠ¤ P_Ïƒ = ğ_j^âŠ¤$ (as $Ïƒ_j = j$). 
âˆ


### LU Decomposition

Before discussing pivoting, consider standard Gaussian elimation where one row-reduces
to introduce zeros column-by-column. We will mimick the computation of the QR decomposition
to view this as a _triangular triangularisation_.


Consider the matrix
$$
L_1 = \begin{bmatrix} 1 \\ -{a_{21} \over a_{11}} & 1 \\ â‹® &&â‹± \\
                -{a_{n1} \over a_{11}}  &&& 1
                \end{bmatrix} = I - \begin{bmatrix} 0 \\ {ğš_1[2:n] \over ğš_1[1]} \end{bmatrix}  ğ_1^\top.
$$
We then have
$$
L_1 A =  \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 
& ğš_2^1 & \cdots & ğš_n^1   \end{bmatrix}
$$
where $ğš_j^1 := (L_1 ğš_j)[2:n]$ and $u_{1j} = a_{1j}$. But now consider
$$
L_2 := I - \begin{bmatrix} 0 \\ {ğš_2^1[2:n-1] \over ğš_2^1[1]} \end{bmatrix}  ğ_1^\top.
$$
Then
$$
L_2 L_1 A = \begin{bmatrix} u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\ 
    & u_{22} & u_{23} & \cdots & u_{2n} \\
&& ğš_3^2 & \cdots & ğš_n^2   \end{bmatrix}
$$
where 
$$
u_{2j} :=  (ğš_j^1)[1] \qquad \hbox{and} \qquad ğš_j^2 := (L_2 ğš_j^1)[2:n-1]
$$
Thus the first two columns are triangular. 

The inductive construction is again clear. If we define $ğš_j^0 := ğš_j$ we
have the construction
$$
\begin{align*}
L_j &:= I - \begin{bmatrix} ğŸ_j \\ {ğš_{j+1}^j[2:n-j] \over ğš_{j+1}^j[1]} \end{bmatrix} ğ_j^âŠ¤ \\
ğš_j^k &:= (L_k ğš_j^{k-1})[2:n-k+1]
 \\
u_{kj} &:= (L_k ğš_j^{k-1})[1]
\end{align*}
$$
Then
$$
L_{n-1} \cdots L_1 A = \underbrace{\begin{bmatrix} 
u_{11} & \cdots & u_{1n} \\ & â‹± & â‹®\\
                                        && u_{nn}\end{bmatrix}}_U
$$
i.e.
$$
A = \underbrace{L_{1}^{-1} \cdots L_{n-1}^{-1}}_L U.
$$

Writing
$$
L_j = I + \begin{bmatrix} ğŸ_j \\ \ell_{j+1,j} \\ â‹® \\ \ell_{n,j} \end{bmatrix} ğ_j^\top
$$
and using the properties of inversion and multiplication we therefore deduce
$$
L = \begin{bmatrix} 1 \\ -\ell_{21} & 1 \\
-\ell_{31} & -\ell_{32} & 1 \\
 â‹® & â‹® & â‹± & â‹± \\
    -\ell_{n1} & -\ell_{n2} & \cdots & -\ell_{n,n-1} & 1
    \end{bmatrix}
$$




**Example (computer)**
We will do a numerical example (by-hand is equivalent to Gaussian elimination).
The first lower triangular matrix is:
```julia
n = 4
A = randn(n,n)
Lâ‚ = I -[0; A[2:end,1]/A[1,1]] * [1 zeros(1,n-1)]
```
Which indeed introduces zeros in the first column:
```julia
Aâ‚ = Lâ‚*A
```
Now we make the next lower triangular operator:
```julia
Lâ‚‚ = I - [0; 0; Aâ‚[3:end,2]/Aâ‚[2,2]] * [0 1 zeros(1,n-2)]
```
So that
```julia
Aâ‚‚ = Lâ‚‚*Lâ‚*A
```
The last one is:
```julia
Lâ‚ƒ = I - [0; 0; 0; Aâ‚‚[4:end,3]/Aâ‚‚[3,3]] * [0 0 1 zeros(1,n-3)]
```
Giving us
```julia
U = Lâ‚ƒ*Lâ‚‚*Lâ‚*A
```
and
```julia
L = inv(Lâ‚)*inv(Lâ‚‚)*inv(Lâ‚ƒ)
```
Note how the entries of `L` are indeed identical to the negative
lower entries of `Lâ‚`, `Lâ‚‚` and `Lâ‚ƒ`.

**Example (by-hand)**

Consider the matrix
$$
A = \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
We have
$$
L_2 L_1 A = L_2 \begin{bmatrix}1 \\ 
                        -2 & 1 \\ -1 &  & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
    = \begin{bmatrix}1 \\ & 1\\ & -{3 \over 2} & 1 
    \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\
                         & 2 & 6 \\
                         & 3 & 8 \end{bmatrix}
    = \underbrace{\begin{bmatrix} 1 & 1 & 1 \\
                         & 2 & 6 \\
                         &  & -1 \end{bmatrix}}_U
$$
We then deduce $L$ by taking the negative of the lower 
entries of $L_1,L_2$:
$$
L = \begin{bmatrix} 1 \\ 2 & 1 \\ 1 &{3 \over 2} & 1
\end{bmatrix}.
$$


### PLU Decomposition

We learned in first year linear algebra that if a diagonal entry is zero
when doing Gaussian elimnation one has to _row pivot_. For stability, 
in implementation one _always_ pivots: swap the largest in magnitude entry for the entry on the diagonal.
In terms of a decomposition, this leads to 
$$
L_{n-1} P_{n-1} \cdots P_2 L_1 P_1 A = U
$$
where $P_j$ is a permutation that leaves rows 1 through $j-1$ fixed,
and swaps row $j$ with a row $k \geq j$ whose entry is maximal in absolute value.

Thus we can deduce that 
$$
L_{n-1} P_{n-1} \cdots P_2 L_1 P_1 = \underbrace{L_{n-1} \tilde L_{n-2} \cdots  \tilde L_1}_{L^{-1}}  \underbrace{P_{n-1} \cdots P_2 P_1}_P.
$$
where the tilde denotes the combined actions of swapping the permutation and lower-triangular matrices, that is,
$$
P_{n-1}\cdots P_{j+1} L_j = \tilde L_j P_{n-1}\cdots P_{j+1}.
$$
where $\tilde L_j \in {\cal L}_j$.
The entries of $L$ are then again the negative of the entries below the diagonal of $L_{n-1}, \tilde L_{n-2}, \ldots,\tilde L_1$.


Writing
$$
\tilde L_j = I + \begin{bmatrix} ğŸ_j \\ \tilde \ell_{j+1,j} \\ â‹® \\ \tilde \ell_{n,j} \end{bmatrix} ğ_j^\top
$$
and using the properties of inversion and multiplication we therefore deduce
$$
L = \begin{bmatrix} 
1 \\ 
-\tilde \ell_{21} & 1 \\
-\tilde \ell_{31} & -\tilde \ell_{32} & 1 \\
 â‹® & â‹® & â‹± & â‹± \\
 -\tilde \ell_{n-1,1} & -\tilde \ell_{n-1,2} & \cdots &  - \tilde \ell_{n-1,n-2} & 1 \\
    -\tilde \ell_{n1} & -\tilde \ell_{n2} & \cdots &  - \tilde \ell_{n,n-2} &  -\ell_{n,n-1} & 1
\end{bmatrix}
$$



**Example**

Again we consider the matrix
$$
A = \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
Even though $a_{11} = 1 \neq 0$, we still pivot: placing 
the maximum entry on the diagonal to mitigate numerical errors.
That is, we first pivot and upper triangularise the first column:
$$
 L_1 P_1 A =  L_1\begin{bmatrix} 0 & 1 \\ 1 & 0 \\ && 1 \end{bmatrix}
\begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix} = 
                     \begin{bmatrix}1 \\ -{1 \over 2} & 1 \\ -{1 \over 2} && 1 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                1 & 1 & 1 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
We now pivot and upper triangularise the second column:
$$
  L_2 P_2 L_1 P_1 A = 
                    L_2 \begin{bmatrix}
                    1 \\ &0 & 1 \\ &1 & 0 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                0 & -1 & -3 \\
                    0 & 2 & 5
                    \end{bmatrix}
                    = \begin{bmatrix} 1\\ & 1 \\ & {1 \over 2} & 1 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                0 & 2 & 5 \\
                0 & -1 & -3 
                    \end{bmatrix} = 
                    \underbrace{\begin{bmatrix} 2 & 4 & 8 \\
                0 & 2 & 5 \\
                0 & 0 & -{1 \over 2}
                    \end{bmatrix}}_U
$$
We now move $P_2$ to the right:
$$
L_2 P_2 L_1 P_1 = \underbrace{\begin{bmatrix} 1\\ -{1 \over 2} & 1 \\  -{1 \over 2}  & {1 \over 2} & 1 \end{bmatrix}}_{L_2 \tilde L_1} \underbrace{\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}}_P
$$
That is
$$
L = \begin{bmatrix} 1\\ {1 \over 2} & 1 \\  {1 \over 2}  & -{1 \over 2} & 1 \end{bmatrix}
$$

We see how this example is done on a computer:
```julia
A = [1 1 1;
     2 4 8;
     1 4 9]
L,U,Ïƒ = lu(A) # Ïƒ is a vector encoding the permutation
```
The permutation is
```julia
Ïƒ
```
Thus to invert a system we can do:
```julia
b = randn(3)
U\(L\b[Ïƒ]) == A\b
```
Note the entries match exactly because this precisely what `\` is using.

## 3. Cholesky Decomposition

Cholesky Decomposition is a form of Gaussian elimination (without pivoting)
that exploits symmetry in the problem, resulting in a substantial speedup. 
It is only relevant for _symmetric positive definite_
matrices.

**Definition (positive definite)** A square matrix $A \in â„^{n \times n}$ is _positive definite_ if
for all $ğ± \in â„^n, x \neq 0$ we have
$$
ğ±^\top A ğ± > 0
$$

First we establish some basic properties of positive definite matrices:

**Proposition (conj. pos. def.)** If  $A \in â„^{n \times n}$ is positive definite and 
$V \in â„^{n \times n}$ is non-singular then
$$
V^\top A V
$$
is positive definite.

**Proposition (diag positivity)** If $A \in â„^{n \times n}$ is positive definite
then its diagonal entries are positive: $a_{kk} > 0$.


**Theorem (subslice pos. def.)** If $A \in â„^{n \times n}$ is positive definite
and $ğ¤ \in \{1,\ldots,n\}^n$ where any integer appears only once then $A[ğ¤,ğ¤]$ is also
positive definite.



We leave the proofs to the problem sheets. Here is the key result:


**Theorem (Cholesky and sym. pos. def.)** A matrix $A$ is symmetric positive definite if and only if it has a Cholesky decomposition
$$
A = L L^âŠ¤
$$
where the diagonals of $L$ are positive.

**Proof** If $A$ has a Cholesky decomposition it is symmetric ($A^âŠ¤ = (L L^âŠ¤)^âŠ¤ = A$) and for $ğ± â‰  0$ we have
$$
ğ±^âŠ¤ A ğ± = (Lğ±)^âŠ¤ L ğ± = \|Lğ±\|^2 > 0
$$
where we use the fact that $L$ is non-singular.

For the other direction we will prove it by induction, with the $1 Ã— 1$ case being trivial. 
Write
$$
A = \begin{bmatrix} Î± & ğ¯^\top \\
                    ğ¯   & K
                    \end{bmatrix} = \underbrace{\begin{bmatrix} \sqrt{Î±} \\ 
                                    {ğ¯ \over \sqrt{Î±}} & I \end{bmatrix}}_{L_1}
                                    \underbrace{\begin{bmatrix} 1  \\ & K - {ğ¯ ğ¯^\top \over Î±} \end{bmatrix}}_{A_1}
                                    \underbrace{\begin{bmatrix} \sqrt{Î±} & {ğ¯^\top \over \sqrt{Î±}} \\
                                     & I \end{bmatrix}}_{L_1^\top}.
$$
Note that $K - {ğ¯ ğ¯^\top \over Î±}$ is a subslice of $A_1 = L_1^{-1} A L_1^{-âŠ¤}$, hence by the previous propositions is
itself symmetric positive definite. Thus we can write 
$$
K - {ğ¯ ğ¯^\top \over Î±} = \tilde L \tilde L^âŠ¤
$$
and hence $A = L L^âŠ¤$ for
$$
L= L_1 \begin{bmatrix}1 \\ & \tilde L \end{bmatrix}.
$$
satisfies $A = L L^âŠ¤$.
âˆ


Note hidden in this proof is a simple algorithm form computing the Cholesky decomposition.
We define
$$
\begin{align*}
A_1 &:= A \\
ğ¯_k &:= A_k[2:n-k+1,1] \\
Î±_k &:= A_k[1,1] \\
A_{k+1} &:= A_k[2:n-k+1,2:n-k+1] - {ğ¯_k ğ¯_k^âŠ¤ \over Î±_k}.
\end{align*}
$$
Then
$$
L = \begin{bmatrix} \sqrt{Î±_1} \\
    {ğ¯_1[1] \over \sqrt{Î±_1}}  &  \sqrt{Î±_2} \\
    {ğ¯_1[2] \over \sqrt{Î±_1}}  & {ğ¯_2[1] \over \sqrt{Î±_2}} &  \sqrt{Î±_2} \\
    â‹® & â‹® & â‹± & â‹± \\
    {ğ¯_1[n-1] \over \sqrt{Î±_1}} &{ğ¯_2[n-2] \over \sqrt{Î±_2}} & â‹¯ & {ğ¯_{n-1}[1] \over \sqrt{Î±_{n-1}}} & \sqrt{Î±_{n}}
    \end{bmatrix}
$$

This algorithm succeeds if and only if $A$ is symmetric positive definite.

**Example** Consider the matrix
$$
A_0 = A = \begin{bmatrix}
2 &1 &1 &1 \\
1 & 2 & 1 & 1 \\
1 & 1 & 2 & 1 \\
1 & 1 & 1 & 2
\end{bmatrix}
$$
Then
$$
A_1 = \begin{bmatrix}
2 &1 &1 \\
1 & 2 & 1 \\
1 & 1 & 2 
\end{bmatrix} - {1 \over 2} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} =
{1 \over 2} \begin{bmatrix}
3 & 1 & 1 \\
1 & 3 & 1 \\
1 & 1 & 3 
\end{bmatrix}
$$
Continuing, we have 
$$
A_2 = {1 \over 2} \left( \begin{bmatrix}
3 & 1 \\ 1 & 3
\end{bmatrix} - {1 \over 3} \begin{bmatrix} 1 \\ 1  \end{bmatrix} \begin{bmatrix} 1 & 1  \end{bmatrix}
\right)
= {1 \over 3} \begin{bmatrix} 4 & 1 \\ 1 & 4 \end{bmatrix}
$$
Finally
$$
A_3 = {5 \over 4}.
$$
Thus we get
$$
L= L_1 L_2 L_3 = \begin{bmatrix} \sqrt{2} \\ {1 \over \sqrt{2}} & {\sqrt{3} \over 2} \\ 
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {2 \over \sqrt{3}} \\
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {1 \over \sqrt{12}} & {\sqrt{5} \over 2}
\end{bmatrix}
$$


# 4. Timings

The different decompositions have trade-offs between speed and stability.
First we compare the speed of the different decompositions on a symmetric positive
definite matrix, from fastest to slowest:

```julia
n = 100
A = Symmetric(rand(n,n)) + 100I # shift by 10 ensures positivity
@btime cholesky(A);
@btime lu(A);
@btime qr(A);
```
On my machine, `cholesky` is ~1.5x faster than `lu`,  
which is ~2x faster than QR. 



In terms of stability, QR computed with Householder reflections
(and Cholesky for positive definite matrices) are stable, 
whereas LU is usually unstable (unless the matrix
is diagonally dominant). PLU is a very complicated story: in theory it is unstable,
but the set of matrices for which it is unstable is extremely small, so small one does not
normally run into them.

Here is an example matrix that is in this set. 
```julia
function badmatrix(n)
    A = Matrix(1I, n, n)
    A[:,end] .= 1
    for j = 1:n-1
        A[j+1:end,j] .= -1
    end
    A
end
A = badmatrix(5)
```
Note that pivoting will not occur (we do not pivot as the entries below the diagonal are the same magnitude as the diagonal), thus the PLU Decomposition is equivalent to an LU decomposition:
```julia
L,U = lu(A)
```
But here we see an issue: the last column of `U` is growing exponentially fast! Thus when `n` is large
we get very large errors:
```julia
n = 100
b = randn(n)
A = badmatrix(n)
norm(A\b - qr(A)\b) # A \ b still uses lu
```
Note `qr` is completely fine:
```julia
norm(qr(A)\b - qr(big.(A)) \b) # roughly machine precision
```

Amazingly, PLU is fine if applied to a small perturbation of `A`:
```julia
Îµ = 0.000001
AÎµ = A .+ Îµ .* randn.()
norm(AÎµ \ b - qr(AÎµ) \ b) # Now it matches!
```



The big _open problem_ in numerical linear algebra is to prove that the set of matrices
for which PLU fails has extremely small measure.


