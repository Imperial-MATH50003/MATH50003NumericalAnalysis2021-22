# Norms, singular values, and conditioning


In this lecture we discuss matrix and vector norms. The matrix $2$-norm involves
_singular values_, which are a measure of how matrices "stretch" vectors, similar to
eigenvalues but more robust. We also introduce condition of problems, and show that
the singular values of a matrix give a notion of a _condition number_, which allows us
to bound errors in linear algebra operations.



## 1. Vector norms

Recall the definition of a (vector-)norm:

**Definition (vector-norm)** A norm $\| Ì‡\|$ on $â„^n$ is a function that satisfies the following, for $ğ±,ğ² âˆˆ â„^n$ and
$c âˆˆ â„$:
1. Triangle inequality: $\|ğ± + ğ² \| â‰¤ \|ğ±\| + \|ğ²\|$
2. Homogeneneity: $\| c ğ± \| = |c| \| ğ± \|$
3. Positive-definiteness: $\|ğ±\| = 0$ implies that $ğ± = 0$.


Consider the following example:

**Definition (p-norm)**
For $1 â‰¤ p < âˆ$ and $ğ± \in â„^n$, define the $p$-norm:
$$
\|ğ±\|_p := (\sum_{k=1}^n |x_k|^p)^{1/p}
$$
where $x_k$ is the $k$-th entry of $ğ±$. 
For $p = âˆ$ we define
$$
\|ğ±\|_âˆ := \max_k |x_k|
$$

**Theorem (p-norm)** $\| â‹… \|_p$ is a norm.

**Proof**

Homogeneity and positive-definiteness are straightforward: e.g.,
$$
\|c ğ±\|_p = (\sum_{k=1}^n |cx_k|^p)^{1/p} = (|c|^p \sum_{k=1}^n |x_k|^p)^{1/p} = |c| \| ğ± \|
$$
and if $\| ğ± \|_p = 0$ then all $|x_k|^p$ are have to be zero.

For $p = 1,âˆ$ the triangle inequality is also straightforward:
$$
\| ğ± + ğ² \|_âˆ = \max_k (|x_k + y_k|) â‰¤Â \max_k (|x_k| + |y_k|) â‰¤ \|ğ±\|_âˆ + \|ğ²\|_âˆ
$$
and
$$
\| ğ± + ğ² \|_1 = \sum_{k=1}^n |x_k + y_k| â‰¤Â  \sum_{k=1}^n (|x_k| + |y_k|) = \| ğ± \|_1 + \| ğ²\|_1
$$

For $p = 2$ it can be proved using the Cauchyâ€“Schwartz inequality:
$$
|ğ±^âŠ¤ ğ²| â‰¤ \| ğ± \|_2 \| ğ² \|_2
$$
That is, we have
$$
\| ğ± + ğ² \|^2 = \|ğ±\|^2 + 2 ğ±^âŠ¤ ğ² + \|ğ²\|^2 â‰¤Â \|ğ±\|^2 + 2\| ğ± \| \| ğ² \| + \|ğ²\|^2 = (\| ğ± \| +  \| ğ² \|)
$$

For general $1 < p  < âˆ$ the proof is more involved.

âˆ


 In Julia can use the inbuilt `norm` function to calculate norms:
 ```julia
 norm([1,-2,3]) == norm([1,-2,3],2) == sqrt(1^2+2^2+3^2);
 norm([1,-2,3],1) == sqrt(1 + 2 + 3)
 ```


## 2. Matrix norms
 Just like vectors, matrices have norms that measure their "length".  The simplest example is the FrÃ¶benius norm, 
 defined for an $m \times n$ real matrix $A$ as
$$
\|A\|_F := \sqrt{\sum_{k=1}^m \sum_{j=1}^n A_{kj}^2}
$$

While this is the simplest norm, it is not the most useful.  Instead, we will build a matrix norm from a 
vector norm:



**Definition (matrix-norm)** Suppose $A âˆˆ â„^{m Ã— n}$  and consider two norms $\| â‹… \|_X$ on $â„^n$  and 
$\| â‹… \|_Y$ on $â„^n$. Define the _(induced) matrix norm_ as:
$$
\|A \|_{X â†’ Y} := \sup_{ğ¯ : \|ğ¯\|_X=1} \|A ğ¯\|_Y
$$
Also define
$$
\|A\|_X \triangleq \|A\|_{X \rightarrow X}
$$

For  the induced 2, 1, and $âˆ$-norm we use
$$
\|A\|_2, \|A\|_1 \qquad \hbox{and} \qquad \|A\|_âˆ.
$$

Note an equivalent definition of the induced norm:
$$
\|A\|_{X â†’ Y} = \sup_{ğ± âˆˆ â„^n, ğ± â‰  0} {\|A ğ±\|_Y \over \| ğ±\|_X}
$$
This follows since we can scale $ğ±$ by its norm so that it has unit norm, that is,
${ğ±} \over \|ğ±\|_X$ has unit norm.

**Lemma (matrix norms are norms)** Induced matrix norms are norms, that is for $\| â‹… \| = \| â‹… \|_{X â†’ Y}$ we have:
1. Triangle inequality: $\| A + B \| â‰¤  \|A\| + \|B\|$
1. Homogeneneity: $\|c A \| = |c| \|A\|$
3. Positive-definiteness: $\|A\| =0 \Rightarrow A = 0$
In addition, they satisfy the following additional propertie:
1. $\|A ğ± \|_Y â‰¤ \|A\|_{X â†’ Y} \|ğ± \|_X$
2. Multiplicative inequality: $\| AB\|_{X â†’ Z} â‰¤ \|A \|_{Y â†’ Z} \|B\|_{X â†’  Y}$

**Proof**

First we show the _triangle inequality_:
$$
\|A + B \| â‰¤ \sup_{ğ¯ : \|ğ¯\|_X=1} (\|A ğ¯\|_Y + \|B ğ¯\|_Y) â‰¤ \| A \| + \|B \|.
$$
Homogeneity is also immediate. Positive-definiteness follows from the fact that if
$\|A\| = 0$ then $A ğ±  = 0$ for all $ğ± âˆˆ â„^n$.
The property $\|A ğ± \|_Y â‰¤ \|A\|_{X â†’ Y} \|ğ± \|_X$ follows from the definition. Finally, 
Finally, the multiplicative inequality follows from
$$
\|A B\| = \sup_{ğ¯ : \|ğ¯\|_X=1} \|A B ğ¯ |_Z â‰¤Â \sup_{ğ¯ : \|ğ¯\|_X=1} \|A\|_{Y â†’ Z} \| B ğ¯ | = \|A \|_{Y â†’ Z} \|B\|_{X â†’  Y}
$$



âˆ

We have some simple examples of induced norms:


An example that is not simple is $\|A \|_2$.


## 3. Singular value decomposition

To define the induced $2$-norm we need to consider the following:

**Definition (singular value decomposition)** For $A âˆˆ â„^{m Ã— n}$ with rank $r > 0$, 
the _reduced singular value decomposition (SVD)_ is
$$
A = U Î£ V^âŠ¤
$$
where $U âˆˆ â„^{m Ã— r}$ and $V âˆˆ  â„^{r Ã— n}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose
diagonal entries, which which we call _singular values_, are all non-negative and decreasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_{\min(m,n)} â‰¥ 0$.
The _full singular value decomposition (SVD)_ is
$$
A = U Î£ V^âŠ¤
$$
where $U âˆˆ â„^{m Ã— m}$ and $V âˆˆ  â„^{n Ã— n}$ are orthogonal matrices and $Î£ âˆˆ â„^{m Ã— n}$.

For symmetric matrices, the SVD is related to the eigenvalue decomposition.
Recall that a symmetric matrix has real eigenvalues and orthogonal eigenvectors:
$$
A = Q Î› Q^âŠ¤ = \undebrace{Q}_U \underbrace{|Î›|}_Î£ \underbrace{(\sign Î› Q)^âŠ¤}_{V^âŠ¤}
$$
For non-symmetric matrices we relate it to the eigenvalues of the _Gram matrix_ $A^âŠ¤A$ and $AA^âŠ¤$ via:
$$
\begin{align*}
A^âŠ¤ A = V Î£^2 V^âŠ¤ \\
A A^âŠ¤ = U Î£^2 U^âŠ¤
\end{align*}
$$
That is, $Ïƒ_k^2$ are non-zero eigenvalues of $A^âŠ¤ A$ and $A A^âŠ¤$. 
We now establish some properties of a Gram matrix:

**Proposition (Gram matrix kernel)** The kernel of $A$ is the also the kernel of $A^âŠ¤ A$. 

**Proof**
If $A^âŠ¤ A ğ± = 0$ then we have
$$
0 = ğ± A^âŠ¤ A ğ± = \| A ğ± \|^2
$$
which means $A ğ± = 0$ and $ğ± âˆˆ \hbox{ker}(A)$.
âˆ



This connection allows us to prove existence:

**Theorem** Every $A âˆˆ â„^{mÃ— n}$ has an SVD.

**Proof**

First note that $A^âŠ¤ A = Q Î› Q^âŠ¤$ has non-negative eigenvalues $Î»_k$ as,
for the corresponding (orthonormal) eigenvector $ğª_k$,
$$
Î»_k = Î»_k ğª_k^âŠ¤ ğª_k = ğª_k^âŠ¤ A^âŠ¤ A ğª_k = \| A ğª_k \| â‰¥ 0.
$$
Further, the kernel of $A^âŠ¤ A$ is the same as $A$.
Assume the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$
are an enumeration of the non-zero eigenvalues and
$$
V := \begin{bmatrix} ğª_1 | â‹¯ | ğª_r \end{bmatrix}
$$
the corresponding (orthonormal) eigenvectors, with
$$
K = \begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \end{bmatrix}
$$
the corresponding kernel. 
Define
$$
Î£ :=  \begin{bmatrix} \sqrt{Î»_1} \\ & â‹± \\ && \sqrt{Î»_r} \end{bmatrix}
$$
Now define
$$
U := AV Î£^{-1}
$$
which is orthogonal since $A^âŠ¤ A V = Î£^2 V$:
$$
U^âŠ¤ U = Î£^{-1} V^âŠ¤ A^âŠ¤ A V Î£^{-1} = I.
$$
Thus we have
$$
U Î£ V^âŠ¤ = A V V^âŠ¤
$$
Finally, note that 

âˆ