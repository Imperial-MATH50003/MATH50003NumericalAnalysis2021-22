# Norms, singular values, and conditioning


In this lecture we discuss matrix and vector norms. The matrix $2$-norm involves
_singular values_, which are a measure of how matrices "stretch" vectors, similar to
eigenvalues but more robust. We also introduce condition of problems, and show that
the singular values of a matrix give a notion of a _condition number_, which allows us
to bound errors introduced by floating point numbers in linear algebra operations.



## 1. Vector norms

Recall the definition of a (vector-)norm:

**Definition (vector-norm)** A norm $\|\cdot\|$ on $â„^n$ is a function that satisfies the following, for $ğ±,ğ² âˆˆ â„^n$ and
$c âˆˆ â„$:
1. Triangle inequality: $\|ğ± + ğ² \| â‰¤ \|ğ±\| + \|ğ²\|$
2. Homogeneneity: $\| c ğ± \| = |c| \| ğ± \|$
3. Positive-definiteness: $\|ğ±\| = 0$ implies that $ğ± = 0$.


Consider the following example:

**Definition (p-norm)**
For $1 â‰¤ p < âˆ$ and $ğ± \in â„^n$, define the $p$-norm:
$$
\|ğ±\|_p := \left(\sum_{k=1}^n |x_k|^p\right)^{1/p}
$$
where $x_k$ is the $k$-th entry of $ğ±$. 
For $p = âˆ$ we define
$$
\|ğ±\|_âˆ := \max_k |x_k|
$$

**Theorem (p-norm)** $\| â‹… \|_p$ is a norm for $1 â‰¤ p â‰¤ âˆ$.

**Proof**

Homogeneity and positive-definiteness are straightforward: e.g.,
$$
\|c ğ±\|_p = (\sum_{k=1}^n |cx_k|^p)^{1/p} = (|c|^p \sum_{k=1}^n |x_k|^p)^{1/p} = |c| \| ğ± \|
$$
and if $\| ğ± \|_p = 0$ then all $|x_k|^p$ are have to be zero.

For $p = 1,âˆ$ the triangle inequality is also straightforward:
$$
\| ğ± + ğ² \|_âˆ = \max_k (|x_k + y_k|) â‰¤Â \max_k (|x_k| + |y_k|) â‰¤ \|ğ±\|_âˆ + \|ğ²\|_âˆ
$$
and
$$
\| ğ± + ğ² \|_1 = \sum_{k=1}^n |x_k + y_k| â‰¤Â  \sum_{k=1}^n (|x_k| + |y_k|) = \| ğ± \|_1 + \| ğ²\|_1
$$

For $p = 2$ it can be proved using the Cauchyâ€“Schwartz inequality:
$$
|ğ±^âŠ¤ ğ²| â‰¤ \| ğ± \|_2 \| ğ² \|_2
$$
That is, we have
$$
\| ğ± + ğ² \|^2 = \|ğ±\|^2 + 2 ğ±^âŠ¤ ğ² + \|ğ²\|^2 â‰¤Â \|ğ±\|^2 + 2\| ğ± \| \| ğ² \| + \|ğ²\|^2 = (\| ğ± \| +  \| ğ² \|)
$$

For general $1 < p  < âˆ$, it suffices to assume $\| ğ± + ğ² \| = 1$.
consider $\| ğ± + ğ² \|^p$...

âˆ


 In Julia can use the inbuilt `norm` function to calculate norms:
 ```julia
 norm([1,-2,3]) == norm([1,-2,3],2) == sqrt(1^2+2^2+3^2);
 norm([1,-2,3],1) == sqrt(1 + 2 + 3)
 ```


## 2. Matrix norms
 Just like vectors, matrices have norms that measure their "length".  The simplest example is the FrÃ¶benius norm, 
 defined for an $m \times n$ real matrix $A$ as
$$
\|A\|_F := \sqrt{\sum_{k=1}^m \sum_{j=1}^n A_{kj}^2}
$$
While this is the simplest norm, it is not the most useful.  Instead, we will build a matrix norm from a 
vector norm:



**Definition (matrix-norm)** Suppose $A âˆˆ â„^{m Ã— n}$  and consider two norms $\| â‹… \|_X$ on $â„^n$  and 
$\| â‹… \|_Y$ on $â„^n$. Define the _(induced) matrix norm_ as:
$$
\|A \|_{X â†’ Y} := \sup_{ğ¯ : \|ğ¯\|_X=1} \|A ğ¯\|_Y
$$
Also define
$$
\|A\|_X \triangleq \|A\|_{X \rightarrow X}
$$

For  the induced 2, 1, and $âˆ$-norm we use
$$
\|A\|_2, \|A\|_1 \qquad \hbox{and} \qquad \|A\|_âˆ.
$$

Note an equivalent definition of the induced norm:
$$
\|A\|_{X â†’ Y} = \sup_{ğ± âˆˆ â„^n, ğ± â‰  0} {\|A ğ±\|_Y \over \| ğ±\|_X}
$$
This follows since we can scale $ğ±$ by its norm so that it has unit norm, that is,
${ğ±} \over \|ğ±\|_X$ has unit norm.

**Lemma (matrix norms are norms)** Induced matrix norms are norms, that is for $\| â‹… \| = \| â‹… \|_{X â†’ Y}$ we have:
1. Triangle inequality: $\| A + B \| â‰¤  \|A\| + \|B\|$
1. Homogeneneity: $\|c A \| = |c| \|A\|$
3. Positive-definiteness: $\|A\| =0 \Rightarrow A = 0$
In addition, they satisfy the following additional propertie:
1. $\|A ğ± \|_Y â‰¤ \|A\|_{X â†’ Y} \|ğ± \|_X$
2. Multiplicative inequality: $\| AB\|_{X â†’ Z} â‰¤ \|A \|_{Y â†’ Z} \|B\|_{X â†’  Y}$

**Proof**

First we show the _triangle inequality_:
$$
\|A + B \| â‰¤ \sup_{ğ¯ : \|ğ¯\|_X=1} (\|A ğ¯\|_Y + \|B ğ¯\|_Y) â‰¤ \| A \| + \|B \|.
$$
Homogeneity is also immediate. Positive-definiteness follows from the fact that if
$\|A\| = 0$ then $A ğ±  = 0$ for all $ğ± âˆˆ â„^n$.
The property $\|A ğ± \|_Y â‰¤ \|A\|_{X â†’ Y} \|ğ± \|_X$ follows from the definition. Finally, 
Finally, the multiplicative inequality follows from
$$
\|A B\| = \sup_{ğ¯ : \|ğ¯\|_X=1} \|A B ğ¯ |_Z â‰¤Â \sup_{ğ¯ : \|ğ¯\|_X=1} \|A\|_{Y â†’ Z} \| B ğ¯ | = \|A \|_{Y â†’ Z} \|B\|_{X â†’  Y}
$$



âˆ

We have some simple examples of induced norms:

**Example ($1$-norm)** We claim 
$$
\|A \|_1 = \max_j \|ğš_j\|_1
$$
that is, the maximum $1$-norm of the columns. To see this use the triangle inequality to
find for $\|ğ±\|_1 = 1$
$$
\| A ğ± \|_1 â‰¤Â âˆ‘_{j = 1}^n |x_j| \| ğš_j\|_1 â‰¤Â \max_j \| ğš_j\| âˆ‘_{j = 1}^n |x_j| = \max_j \| ğš_j\|_1.
$$
But the bound is also attained since if $j$ is the column that maximises the norms then
$$
\|A ğ_j \|_1 = \|ğš_j\|_1 =  \max_j \| ğš_j\|_1.
$$

In the problem sheet we see that
$$
\|A\|_âˆ = \max_k \|A[k,:]\|_1
$$
that is, the maximum $1$-norm of the rows.



An example that is not simple is $\|A \|_2$, but we do have two simple cases:

**Proposition (diagonal/orthogonal 2-norms)** If $Î›$ is diagonal with entries $Î»_k$ then
$\|Î›\|_2 = \max_k |Î»_k|.$. If $Q$ is orthogonal then $\|Q\| = 1$.


## 3. Singular value decomposition

To define the induced $2$-norm we need to consider the following:

**Definition (singular value decomposition)** For $A âˆˆ â„^{m Ã— n}$ with rank $r > 0$, 
the _reduced singular value decomposition (SVD)_ is
$$
A = U Î£ V^âŠ¤
$$
where $U âˆˆ â„^{m Ã— r}$ and $V âˆˆ  â„^{r Ã— n}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose
diagonal entries, which which we call _singular values_, are all non-negative and decreasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_{\min(m,n)} â‰¥ 0$.
The _full singular value decomposition (SVD)_ is
$$
A = U Î£ V^âŠ¤
$$
where $U âˆˆ â„^{m Ã— m}$ and $V âˆˆ  â„^{n Ã— n}$ are orthogonal matrices and $Î£ âˆˆ â„^{m Ã— n}$.

For symmetric matrices, the SVD is related to the eigenvalue decomposition.
Recall that a symmetric matrix has real eigenvalues and orthogonal eigenvectors:
$$
A = Q Î› Q^âŠ¤ = \underbrace{Q}_U \underbrace{|Î›|}_Î£ \underbrace{(\hbox{sign}(Î›) Q)^âŠ¤}_{V^âŠ¤}
$$
For non-symmetric matrices we relate it to the eigenvalues of the _Gram matrix_ $A^âŠ¤A$ and $AA^âŠ¤$ via:
$$
\begin{align*}
A^âŠ¤ A = V Î£^2 V^âŠ¤ \\
A A^âŠ¤ = U Î£^2 U^âŠ¤
\end{align*}
$$
That is, $Ïƒ_k^2$ are non-zero eigenvalues of $A^âŠ¤ A$ and $A A^âŠ¤$. 
We now establish some properties of a Gram matrix:

**Proposition (Gram matrix kernel)** The kernel of $A$ is the also the kernel of $A^âŠ¤ A$. 

**Proof**
If $A^âŠ¤ A ğ± = 0$ then we have
$$
0 = ğ± A^âŠ¤ A ğ± = \| A ğ± \|^2
$$
which means $A ğ± = 0$ and $ğ± âˆˆ \hbox{ker}(A)$.
âˆ



This connection allows us to prove existence:

**Theorem (SVD existence)** Every $A âˆˆ â„^{mÃ— n}$ has an SVD.

**Proof**

First note that $A^âŠ¤ A = Q Î› Q^âŠ¤$ has non-negative eigenvalues $Î»_k$ as,
for the corresponding (orthonormal) eigenvector $ğª_k$,
$$
Î»_k = Î»_k ğª_k^âŠ¤ ğª_k = ğª_k^âŠ¤ A^âŠ¤ A ğª_k = \| A ğª_k \| â‰¥ 0.
$$
Further, the kernel of $A^âŠ¤ A$ is the same as $A$.
Assume the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$
are an enumeration of the non-zero eigenvalues and
$$
V := \begin{bmatrix} ğª_1 | â‹¯ | ğª_r \end{bmatrix}
$$
the corresponding (orthonormal) eigenvectors, with
$$
K = \begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \end{bmatrix}
$$
the corresponding kernel. 
Define
$$
Î£ :=  \begin{bmatrix} \sqrt{Î»_1} \\ & â‹± \\ && \sqrt{Î»_r} \end{bmatrix}
$$
Now define
$$
U := AV Î£^{-1}
$$
which is orthogonal since $A^âŠ¤ A V = Î£^2 V$:
$$
U^âŠ¤ U = Î£^{-1} V^âŠ¤ A^âŠ¤ A V Î£^{-1} = I.
$$
Thus we have
$$
U Î£ V^âŠ¤ = A V V^âŠ¤ = A \underbrace{\begin{bmatrix} V | K \end{bmatrix}}_Q\underbrace{\begin{bmatrix} V^âŠ¤ \\ K^âŠ¤ \end{bmatrix}}_{Q^âŠ¤}
$$
where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.

âˆ

Singular values tell us the 2-norm:

**Corollary (singular values and norm)**
$$
\|A \|_2 = Ïƒ_1
$$
and if $A$ is invertible, then
$$
\|A^{-1} \|_2 = Ïƒ_r^{-1}
$$

**Proof**

First we establish the upper-bound:
$$
\|A \|_2 â‰¤Â  \|U \|_2 \| Î£ \|_2 \| V^âŠ¤\|_2 = \| Î£ \|_2  = Ïƒ_1
$$
This is attained using the first right singular vector:
$$
\|A ğ¯_1\|_2 = \|Î£ V^âŠ¤ ğ¯_1\|_2 = \|Î£  ğ_1\|_2 = Ïƒ_1
$$
The inverse result follows since the inverse has SVD
$$
A^{-1} = V Î£^{-1} U^âŠ¤ = V (W Î£^{-1} W) U^âŠ¤
$$
where
$$
W := P_Ïƒ = \begin{bmatrix} && 1 \\ & â‹° \\ 1 \end{bmatrix}
$$
is the permutation that reverses the entries, that is, $Ïƒ$ has Cauchy notation
$$
\begin{pmatrix}
1 & 2 & â‹¯ & n \\
n & n-1 & â‹¯ & 1
\end{pmatrix}.
$$


âˆ

We will not discuss in this module computation of singular value decompositions or eigenvalues:
they involve iterative algorithms (actually built on a sequence of QR decompositions).

One of the main usages for SVDs is low-rank approximation:

**Theorem (best low rank approximation)** The  matrix
$$
A_k := \begin{bmatrix} ğ®_1 | â‹¯ | ğ®_k \end{bmatrix} \begin{bmatrix}
Ïƒ_1 \\
& â‹± \\
&& Ïƒ_k\end{bmatrix} \begin{bmatrix} ğ¯_1 | â‹¯ | ğ¯_k \end{bmatrix}^âŠ¤
$$ 
is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have 
$$\|A - A_k\|_2 â‰¤ \|A -B \|_2.$$


**Proof**
We have

$$
\|A - A_k\|_2 = \|U \begin{bmatrix} 0  \cr &\ddots \cr && 0 \cr &&& Ïƒ_{k+1} \cr &&&& \ddots \cr &&&&& Ïƒ_n \cr &&&&&\vdots \cr &&&&&0\end{bmatrix} = Ïƒ_{r+1}
$$


Suppose a rank-$k$ matrix $B$ has 
$$
\|A-B\|_2  < \|A-A_k\|_2 = Ïƒ_{k+1}.
$$
For all $ğ° \in \ker(B)$ we have 
$$
\|A ğ°\|_2 = \|(A-B) ğ°\|_2 â‰¤ \|A-B\|\|ğ°\|_2  < Ïƒ_{k+1} \|ğ°\|_2
$$

But for all $ğ® \in {\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$, that is, $ğ® = V[:,1:r+1]ğœ$ for some $ğœ \in â„^{k+1}$  we have 
$$
\|A ğ®\|_2^2 = \|U Î£_k ğœ\|_2^2 = \|Î£_k ğœ\|_2^2 =
\sum_{j=1}^{k+1} (Ïƒ_j c_j)^2 â‰¥ Ïƒ_{k+1}^2 \|c\|^2,
$$
i.e., $\|A ğ®\|_2 â‰¥ Ïƒ_{k+1} \|c\|$.  Thus $ğ°$ cannot be in this span.


The dimension of the span of $\ker(B)$ is at least $n-k$, but the dimension of ${\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$ is at least $k+1$.
Since these two spaces cannot intersect we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  âˆ


In the problem sheet we explore the usage of low rank approximation to smooth functions.



## 4. Condition numbers

We have seen that floating point arithmetic induces errors in computations, and that we can typically
bound the absolute errors to be proportional to $C Ïµ_{\rm m}$. We want a way to bound the
effect of more complicated calculations like computing $A ğ±$ or $A^{-1} ğ²$ without having to deal with
the exact nature of floating point arithmetic. Here we consider only matrix-multiplication but will make a remark
about matrix inversion.

To justify what follows, we first observe that errors in implementing matrix-vector multiplication
can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A x`
(implemented with floating point) is precisely $A + Î´A$ where $Î´A$ has small norm, relative to $A$.
This is known as _backward error analysis_.



To discuss floating point errors we need to be precise which order the operations happened.
We will use the definition `mul(A,x)`, which denote ${\rm mul}(A, ğ±)$. (Note that `mul_rows` actually
does the _exact_ same operations, just in a different order.) Note that each entry of the result is in fact a dot-product
of the corresponding rows so we first consider the error in the dot product  `dot(ğ±,ğ²)` as implemented in floating-point, 
which we denote ${\rm dot}(A,x)$.

We first need a helper proposition:

**Proposition** If $|Ïµ_i| â‰¤ Ïµ$ and $n Ïµ < 1$, then
$$
\prod_{k=1}^n (1+Ïµ_i) = 1+Î¸_n
$$
for some constant $Î¸_n$ satisfying $|Î¸_n| â‰¤ {n Ïµ \over 1-nÏµ}$.

The proof is left as an exercise (Hint: use induction).

**Lemma (dot product backward error)**
For any norm,
$$
{\rm dot}(ğ±, ğ²) = (ğ± + Î´ğ±)^âŠ¤ ğ²
$$
where
$$
\|Î´ğ±\| â‰¤Â  {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}} \|ğ± \|
$$


**Proof**

Note that 
$$
{\rm dot}(ğ±, ğ²) = \{ [(x_1 âŠ— y_1) âŠ• (x_2 âŠ— y_2)] âŠ•(x_3âŠ— y_3)] âŠ•â‹¯\}âŠ•(x_n âŠ— y_n) \\
  & = \{ [(x_1 y_1)(1+Î´_1) + (x_2 y_2)(1+Î´_2)](1+Î³_1) +x_3 y_3(1+Î´_3)](1+Î³_2) + â‹¯ \}(1+Î³_{n-1})+x_n y_n(1+Î´_n) \\
  & = x_1 y_1 (1+Î´_1) \prod_{k=1}^{n-1}(1+Î³_k) + x_2 y_2 (1+Î´_2) \prod_{k=1}^{n-1}(1+Î³_k) + x_3 y_3 (1+Î´_3) \prod_{k=2}^{n-1}(1+Î³_k)  + â‹¯  +x_{n-1} y_{n-1}(1+Î´_{n-1})(1+Î³_{n-1}) + x_n y_n(1+Î´_n) \\
  &= x_1y_1(1+Î¸_n^1) + x_2y_2(1+Î¸_n^2)+ x_3y_3(1+Î¸_{n-1}) + \cdots + x_n y_n (1+Î¸_1)
\end{align*}
$$
where we denote the errors from multiplication as $Î´_k$ and those from addition by $Î³_k$ and the previous proposition tells us
$$
|Î¸_n^1|, |Î¸_n^2| â‰¤Â {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}}
$$
and
$$
|Î¸_k| â‰¤ {k Ïµ_{\rm m} \over 1-kÏµ_{\rm m}} â‰¤Â {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}}.
$$
Thus
$$
Î´ğ± =  \begin{pmatrix} x_1 Î¸_n^1 \cr x_2 Î¸_n^2 \cr x_3 Î¸_{n-1} \cr \vdots \cr x_n Î¸_1\end{pmatrix}
$$
and the theorem follows:
$$
\| Î´ğ± \| â‰¤Â {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}} \| ğ± \|
$$

âˆ

**Theorem (matrix-vector backward error)**
For any norm,
$$
{\rm mul}(A, ğ±) = (A + Î´A)^âŠ¤ ğ±
$$
where
$$
\|Î´A\| â‰¤Â  {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}} \|A \|
$$

**Proof**

Follows from applying the previous lemma to each row.

âˆ


So now we get to a mathematical question independent of floating point: can we bound the _relative error_ in approximating
$$
A ğ± â‰ˆ (A + Î´A) ğ±
$$
if we know a bound on $\|Î´A\|$?
It turns out we can in turns of the _condition number_ of the matrix:

**Definition (condition number)**
For a square matrix $A$, the _condition number_ is
$$
Îº(A) := \| A \| \| A^{-1} \| = {Ïƒ_1 \over Ïƒ_n}
$$


**Theorem (relative-error for matrix-vector)**
The _worst-case_ relative error in $A ğ± â‰ˆ (A + Î´A) ğ±$ is
$$
{\| Î´A ğ± \| \over \| A ğ± \| } â‰¤ Îº(A) {\|Î´A\| \over \|A \|}
$$

**Proof**
We can assume $A$ is invertible (as otherwise $Îº(A) = âˆ$). Denote $ğ² = A ğ±$ and we have
$$
{\|ğ± \| \over \| A ğ± \|} = {\|A^{-1} ğ² \| \over \|ğ² \|} â‰¤Â \| A^{-1}\|
$$
Thus we have:
$$
{\| Î´A ğ± \| \over \| A ğ± \| } â‰¤ \| Î´A\| \|A^{-1}\| â‰¤Â Îº(A) {\|Î´A\| \over \|A \|}
$$

âˆ


Thus for floating point arithmetic we know the error is bounded by $Îº(A) {n Ïµ_{\rm m} \over 1-nÏµ_{\rm m}}$.

If one uses QR to solve $A ğ± = ğ²$ the condition number also gives a meaningful bound on the error. 
As we have already noted, there are some matrices where PLU decompositions introduce large errors, so
in that case well-conditioning is not a guarantee (but it still usually works).