# Differential Equations via Finite Differences

We now see our first application: solving differential equations.
We will focus on the following differential equations:

1. Indefinite integration for $a â‰¤Â x â‰¤Â b$:
$$
\begin{align*}
u(a) = c, u' = f(x)
\end{align*}
$$
2. Linear time-evolution problems for $0 â‰¤Â t â‰¤Â T$:
$$
\begin{align*}
u(0) = c, u' - a(t) u = f(t)
\end{align*}
$$
4. Vector time-evolution problems for $0 â‰¤Â t â‰¤Â T$:
$$
\begin{align*}
ğ®(0) = ğœ, ğ®' - A(t) ğ® = ğŸ(t)
\end{align*}
$$
4. Nonlinear time-evolution problems for $0 â‰¤Â t â‰¤Â T$:
$$
\begin{align*}
ğ®(0) = ğœ, ğ®' = f(t, ğ®(t))
\end{align*}
$$
5. Poisson equation with Dirichlet conditions for $a â‰¤Â x â‰¤Â b$:
$$
\begin{align*}
u(a) &= c_0, u(b) = c_1, \\
u'' &= f(x)
\end{align*}
$$


Our approach to solving these is to
1. Approximate the solution on $[a,b]$ evaluated on a $n$-point grid
$x_1,â€¦,x_n$ (labelled $t_k$ for time-evolution problems) with _step size_
$$
x_{k+1}-x_k = h = {b-a \over n-1}
$$
for a vector $ğ® âˆˆ â„^n$
that will be determined by solving a linear system:
$$
\begin{bmatrix}
u(x_1) \\
â‹® \\
u(x_n)
\end{bmatrix} â‰ˆ \underbrace{\begin{bmatrix}
u_1 \\
â‹® \\
u_n
\end{bmatrix}}_ğ®
$$
2. Replace the derivatives with the finite-difference approximations (here $m_k = (x_{k+1} - x_k)/2$ is the mid-point
of the grid):
$$
\begin{align*}
u'(x_k) &â‰ˆ {u(x_{k+1}) - u(x_k) \over h} â‰ˆ {u_{k+1} - u_k \over h} \qquad\hbox{(Forward-difference)} \\
u'(m_k) &â‰ˆ {u(x_{k+1}) - u(x_k) \over h} â‰ˆ {u_{k+1} - u_k \over h} \qquad\hbox{(Central-difference)} \\
u'(x_k) &â‰ˆ {u(x_k) - u(x_{k-1}) \over h} â‰ˆ {u_k - u_{k-1} \over h} \qquad\hbox{(Backward-difference)} \\
u''(x_k) &â‰ˆ {u(x_{k+1}) - 2u(x_k) + u_{k-1} \over h^2} â‰ˆ {u_{k+1} - 2u_k + u_{k-1} \over h^2}
\end{align*}
$$
3. Recast the differential equation as a linear system whose solution
is $ğ®$, which we solve using numerical linear algebra.
Add the initial/boundary conditions as extra rows to make sure the
system is square.


**Remark (advanced)** One should normally not need to implement these methods oneselves as there
are packages available, e.g. [DifferentialEquations.jl](https://github.com/SciML/DifferentialEquations.jl). Moreover Forward and Backward
Euler are only the first baby steps to a wide range of time-steppers, with Rungeâ€“Kutta being
one of the most successful.
For example we can solve
a simple differential equation like a pendulum $u'' = -\sin u$ can be solved
as follows (writing at a system $u' = v, v' = -\sin u$):
```julia
using DifferentialEquations, LinearAlgebra, Plots

u = solve(ODEProblem((u,_,x) -> [u[2], -sin(u[1])], [1,0], (0,10)))
plot(u)
```
However, even in these automated packages one has a choice of different methods with
different behaviour, so it is important to understand what is happening.

# 1.  Time-evolution problems

In this section we consider the forward and backward Euler methods, which are based on forward and backward difference
approximations to the derivative. In the problem sheet we will investigate a rule that that takes the average of the two
(with significant benefits). We first discuss the simplest case of indefinite integration, where central differences is also
applicable, then introduce forward and backward
Euler for linear scalar, linear vector, and nonlinear differential equations.


## Indefinite integration

We begin with the simplest differential equation on an interval $[a,b]$:
$$
\begin{align*}
u(a) &= c \\
u'(x) &= f(x)
\end{align*}
$$
Using the forward-difference (which is the standard finite-difference) approximation we 
choose $u_k â‰ˆ u(x_k)$ so that, for $k = 1, â€¦, n-1$:
$$
f(x_k) = u'(x_k) â‰ˆ {u_{k+1} - u_k \over h} = f(x_k)
$$
That is, where $u$ satisfies the differential equation exactly, 
$u_k$ satisfies the _difference equation_ exactly.
We do not include $k = n$ to avoid going outside our grid.

This condition can be recast as a linear system:
$$
\underbrace{{1 \over h} \begin{bmatrix}
-1 & 1\\
& \ddots & \ddots \\
&& -1 & 1
\end{bmatrix}}_{D_h} ğ®^{\rm f} = \underbrace{\begin{bmatrix} f(x_1) \\ \vdots \\ f(x_{n-1})
\end{bmatrix}}_{ğŸ^{\rm f}}
$$
where the super-script ${\rm f}$ denotes that we are using 
forward differences (and is leading towards the
forward Euler method).
Here $D_h âˆˆ â„^{n-1,n}$ so this system is not-invertible. Thus we need to
add an extra row, coming from the initial condition: $ğ_1^âŠ¤ ğ®^{\rm f} = c$,
that is:
$$
\begin{bmatrix}
ğ_1^âŠ¤ \\
D_h
\end{bmatrix} ğ®^{\rm f} = \underbrace{ \begin{bmatrix}
1 \\
-1/h & 1/h\\
& \ddots & \ddots \\
&& -1/h & 1/h
\end{bmatrix}}_{L_h} ğ®^{\rm f} = \begin{bmatrix} c \\ ğŸ^{\rm f} \end{bmatrix}
$$
This is a lower-triangular bidiagonal system, so can be solved using
forward substitution in $O(n)$ operations. 

We can also consider discretisation at the mid-point $m_k = {x_{k+1} - x_k \over 2}$,
which is the analogue of using central-differences:
$$
u'(m_k) â‰ˆ {u_{k+1} - u_k \over h} = f(m_k)
$$
That is, we have the exact same system with a different right-hand side:
$$
\underbrace{{1 \over h} \begin{bmatrix}
-1 & 1\\
& \ddots & \ddots \\
&& -1 & 1
\end{bmatrix}}_{D_h} ğ®^{\rm m} = 
\underbrace{\begin{bmatrix} f(m_1) \\ \vdots \\ f(m_{n-1})
\end{bmatrix}}_{ğŸ^{\rm m}}
$$

And of course there is $ğ®^{\rm B}$ coming from the backwards-difference
formula:
$$
u'(x_k) â‰ˆ {u_k - u_{k-1} \over h} = f(x_k)
$$
which we leave as an exercise.


**Example**

Let's do an example of integrating $\cos x$, and see if our method matches
the true answer of $\sin x$. First we construct the system
as a lower-triangular, `Bidiagonal` matrix:
```julia
using LinearAlgebra, Plots

function indefint(x)
    h = step(x) # x[k+1]-x[k]
    n = length(x)
    L = Bidiagonal([1; fill(1/h, n-1)], fill(-1/h, n-1), :L)
end

n = 10
x = range(0, 1; length=n)
L = indefint(x)
```
We can now solve for our particular problem using both the left and 
mid-point rules:
```julia
c = 0 # u(0) = 0
f = x -> cos(x)


m = (x[1:end-1] + x[2:end])/2 # midpoints


ğŸá¶  = f.(x[1:end-1]) # evaluate f at all but last points
ğŸáµ = f.(m)          # evaluate f at mid-points
ğ®á¶  = L \ [c; ğŸá¶ ] # integrate using forward-differences
ğ®áµ = L \ [c; ğŸáµ] # integrate using central-differences

plot(x, sin.(x); label="sin(x)", legend=:bottomright)
scatter!(x, ğ®á¶ ; label="forward")
scatter!(x, ğ®áµ; label="mid")
```
They both are close though the mid-point version is significantly
more accurate.
 We can estimate how fast it converges:
```julia
# Error from indefinite integration with c and f
function forward_err(u, c, f, n)
    x = range(0, 1; length = n)
    uá¶  = indefint(x) \ [c; f.(x[1:end-1])]
    norm(uá¶  - u.(x), Inf)
end

function mid_err(u, c, f, n)
    x = range(0, 1; length = n)
    m = (x[1:end-1] + x[2:end]) / 2 # midpoints
    uáµ = indefint(x) \ [c; f.(m)]
    norm(uáµ - u.(x), Inf)
end

ns = 10 .^ (1:8) # solve up to n = 10 million
scatter(ns, forward_err.(sin, 0, f, ns); xscale=:log10, yscale=:log10, label="forward")
scatter!(ns, mid_err.(sin, 0, f, ns); label="mid")
plot!(ns, ns .^ (-1); label="1/n")
plot!(ns, ns .^ (-2); label="1/n^2")
```
This is a log-log plot:we scale both $x$ and $y$ axes logarithmically so that
$n^Î±$ becomes a straight line where the slope is dictated by $Î±$.
We seem experimentally that the error for forward-difference is $O(n^{-1})$
while for mid-point/central-differences we get faster $O(n^{-2})$ convergence. 
Both methods appear to be stable.
 

## Forward Euler

Now consider a scalar linear time-evolution problem for $0 â‰¤Â t â‰¤Â T$:
$$
\begin{align*}
u(0 ) &= c \\
u'(t) - a(t) u(t) &= f(t)
\end{align*}
$$
Label the $n$-point grid as $t_k = (k-1)h$ for $h = T/(n-1)$.

**Definition (Restriction matrices)**
Define the $n-1 Ã— n$ _restriction matrices_ as
$$
\begin{align*}
    I_n^{\rm f} := \begin{bmatrix} 1 \\ &â‹± \\ &&1 & 0 \end{bmatrix}\\
I_n^{\rm b} := \begin{bmatrix} 0 & 1 \\ &&â‹± \\ &&&1  \end{bmatrix} \\
\end{align*}
$$


Again we can replace the discretisation using finite-differences, giving us
$$
{u_{k+1} - u_k \over h} - a(t_k) u_k = f(u_k)
$$
for $k = 1,â€¦,n-1$. We need to add the term $a(t_k) u_k$ to our differential equation,
that is. We do this using the $n-1 Ã— n$ _(left) restriction matrix_ that takes a vector
evaluated at $x_1,â€¦,x_n$ and restricts it to $x_1,â€¦,x_{n-1}$,
as well as the $n Ã— n$ _multiplication matrix_
$$
A_n = \begin{bmatrix} a(t_1) \\ &â‹±\\&& a(t_n) \end{bmatrix}
$$
Putting everything together we have the system:
$$
\begin{bmatrix}
ğ_1^âŠ¤ \\
D_h -  I_n^{\rm f}  A_n
\end{bmatrix} ğ®^{\rm f} = \underbrace{ \begin{bmatrix}
1 \\
-a(t_1)-1/h & 1/h\\
& \ddots & \ddots \\
&& -a(t_{n-1})-1/h & 1/h
\end{bmatrix}}_L ğ®^{\rm f} = \begin{bmatrix} c \\ I_n^{\rm f} ğŸ \end{bmatrix}
$$
where $ğŸ = \begin{bmatrix} f(t_1) \\ â‹® \\ f(t_n) \end{bmatrix}$.

Here is a simple example for solving:
$$
u'(0) = 1, u' + t u = {\rm e}^t
$$
which has an exact solution in terms of a special error function
(which we determined using Mathematica).

```julia
using SpecialFunctions
c = 1
a = t -> t
n = 2000
t = range(0, 1; length=n)
# exact solution, found in Mathematica
u = t -> -(1/2)*exp(-(1+t^2)/2)*(-2sqrt(â„¯) + sqrt(2Ï€)erfi(1/sqrt(2)) - sqrt(2Ï€)erfi((1 + t)/sqrt(2)))

h = step(t)
L = Bidiagonal([1; fill(1/h, n-1)], a.(t[1:end-1]) .- 1/h, :L)

norm(L \ [c; exp.(t[1:end-1])] - u.(t),Inf)
```
We see that it is converging to the true result.


Note that this is a simple forward-substitution of a bidiagonal system,
so we can also just construct it directly:
$$
\begin{align*}
u_1 &= c \\
u_{k+1} &= (1 + h a(t_k)) u_k + h f(t_k)
\end{align*}
$$


**Remark (advanced)** Note this can alternatively be reduced to an integral
$$
u(t) = c \hbox{e}^{a t} + \hbox{e}^{a t} \int_0^t f(Ï„) \hbox{e}^{-a Ï„} \hbox d Ï„
$$
and solved as above but this approach is harder to generalise.


## Backward Euler


In Backward Euler we replace the forward-difference with a backward-difference,
that is
$$
{u_k - u_{k-1} \over h} - a(t_k) u_k = f(u_k)
$$
This leads to the system:
$$
\begin{bmatrix}
ğ_1^âŠ¤ \\
D_h -  I_n^{\rm b}  A_n
\end{bmatrix} ğ®^{\rm b} = \underbrace{ \begin{bmatrix}
1 \\
-a(t_1)-1/h & 1/h\\
& \ddots & \ddots \\
&& -a(t_{n-1})-1/h & 1/h
\end{bmatrix}}_L ğ®^{\rm b} = \begin{bmatrix} c \\ I_n^{\rm b} ğŸ \end{bmatrix}
$$
Again this is a bidiagonal forward-substitution:
$$
\begin{align*}
u_1 &= c \\
(1 - h a(t_{k+1})) u_{k+1} &= u_k + h f(t_{k+1})
\end{align*}
$$
That is,
$$
u_{k+1} = (1 - h a(t_{k+1}))^{-1}(u_k + h f(t_{k+1}))
$$



## Systems of equations

We can also solve systems, that is, equations of the form:
$$
\begin{align*}
ğ®(0) &= ğœ \\
ğ®'(t) - A(t) ğ®(t) &= ğŸ(t)
\end{align*}
$$
where $ğ®, ğŸ : [0,T] â†’ â„^d$ and $A : [0,T] â†’ â„^{d Ã— d}$.
We again discretise at the grid $t_k$
by approximating $ğ®(t_k) â‰ˆ ğ®_k âˆˆ â„^d$.
This can be reduced to a block-bidiagonal system as in
the scalar case which is solved via forward-substitution. Though
it's easier to think of it directly. 

Forward Euler gives us:
$$
\begin{align*}
ğ®_1 &= c \\
ğ®_{k+1} &= ğ®_k + h A(t_k) ğ®_k + h ğŸ(t_k)
\end{align*}
$$
That is, each _time-step_ consists of matrix-vector multiplication.
On the other hand Backward Euler requires inverting a matrix
at each time-step:
$$
\begin{align*}
ğ®_1 &= c \\
ğ®_{k+1} &= (I- h A(t_{k+1})^{-1} (ğ®_k  + h ğŸ(t_{k+1}))
\end{align*}
$$


**Example (Airy equation)**
Consider the (negative-time) Airy equation:
$$
\begin{align*}
u(0) &= 1 \\
u'(0) &= 0 \\
u''(t) + t u &= 0
\end{align*}
$$
We can recast it as a system by defining
$$
ğ®(x) = \begin{bmatrix} u(x) \\ u'(x) \end{bmatrix}
$$
which satisfies
$$
\begin{align*}
ğ®(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
ğ® - \begin{bmatrix} 0 & 1 \\ -t & 0 \end{bmatrix} ğ® = ğŸ.
\end{align*}
$$
It is natural to represent the _time-slices_ $ğ®_k$ as
columns of a matrix $U = [ğ®_1 | â‹¯ | ğ®_n] âˆˆ â„^{2 Ã— n}$. Thus we get:
```julia
n = 100_000
t = range(0, 50; length=n)
A = t -> [0 1; -t 0]
h = step(t)

U = zeros(2, n) # each column is a time-slice
U[:,1] = [1.0,0.0] # initial condition
for k = 1:n-1
    U[:,k+1] = (I + h*A(t[k]))*U[:,k]
end

plot(t, U')
```

We leave implementation of backward Euler as a simple exercise.

**Example (Heat on a graph)**
Those who took Introduction to Applied Mathematics will recall
heat equation on a graph. Consider a simple graph of $m$ nodes labelled
$1, â€¦, m$ where node $k$ is connected to neighbouring nodes ${k-1}$ and ${k+1}$,
whereas node $1$ is only connected to node $2$ and node $m$ only connected to
${m-1}$. The graph Laplacian corresponding to this system is the matrix:
$$
Î” := \begin{bmatrix} -1 & 1 \\ 
            1 & -2 & â‹± \\ 
            & 1 & â‹± & 1 \\
            && â‹± & -2 & 1 \\
                &&& 1 & -1
                \end{bmatrix}
$$
If we denote the heat at time $t$ at node $k$ as $u_k(t)$,
which we turn into a vector
$$
ğ®(t) = \begin{bmatrix} u_1(t) \\ â‹® \\ u_m(t) \end{bmatrix}
$$
We consider the case of a periodic forcing at the middle node $n = âŒŠm/2âŒ‹$.

Heat equation on this lattice is defined as follows:
$$
ğ®' = Î”ğ® + ğ_{âŒŠm/2âŒ‹} \cos Ï‰t
$$
We can employ forward and backward Euler:
```julia
n = 1_000 # number of time-steps
t = range(0, 100; length=n)
h = step(t)

m = 50 # number of nodes


Î” = SymTridiagonal([-1; fill(-2.0, m-2); -1], ones(m-1))
Ï‰ = 1
f = t -> cos(Ï‰*t) # periodic forcing with period 1

Uá¶  = zeros(m, n) # each column is a time-slice for forward Euler
Uáµ‡ = zeros(m, n) # each column is a time-slice for backwar Euler

Uá¶ [:,1] = Uáµ‡[:,1] = zeros(m) # initial condition



for k = 1:n-1
    Uá¶ [:,k+1] = (I + h*Î”)*Uá¶ [:,k]
    Uá¶ [mÃ·2,k+1] += h*f(t[k]) # add forcing at ğ_1
end

ğ = zeros(m); ğ[mÃ·2] = 1;

for k = 1:n-1
    Uáµ‡[:,k+1] = (I - h*Î”)\(Uáµ‡[:,k] + h*f(t[k+1])ğ)
end

scatter(Uá¶ [:,end]; label="forward")
scatter!(Uáµ‡[:,end]; label="backward")
```
Both match! 

**Remark** If you change the number of time-steps to be too small, for example `n = 100`, forward
Euler blows up while backward Euler does not. This will be discussed in the problem
sheet.

**Remark (advanced)** Memory allocations are very expensive so
in practice one should preallocate and use memory. 


## Nonlinear problems

Forward-Euler extends naturally to nonlinear equations, 
including the
vector case:
$$
ğ®' = f(t, ğ®(t))
$$
becomes:
$$
ğ®_{k+1} = ğ®_k + h f(x_k, ğ®_k)
$$
Here we show a simple solution to a nonlinear Pendulum:
$$
u'' = \sin u
$$
by writing $ğ® = [u,u']$ we have:
$$

```julia
n = 1000
ğ® = fill(zeros(2), n)
x = range(0, 20; length=n)
h = step(x) # same as x[k+1]-x[k]

ğ®[1] = [1,0]
for k = 1:n-1
    ğ®[k+1] = ğ®[k] + h * [ğ®[k][2],-sin(ğ®[k][1])]
end

plot(x, first.(ğ®))
```
As we see it correctly predicts the oscillatory behaviour of
a pendulum, and matches the simulation using DifferentialEquations.jl
above.



# 2. Two-point boundary value problems

Here we will only consider one discretisation as it is symmetric:
$$
u''(x_k) â‰ˆ {u_{k-1} - 2u_k + u_{k+1} \over h^2}
$$
That is we use the $n-1 Ã— n+1$ matrix:
$$
D_h^2 := {1 \over h^2} \begin{bmatrix}
1 & -2 & 1 \\ & â‹± & â‹± & â‹± \\
&& 1 & -2 & 1 
\end{bmatrix}
$$


**Example (Poisson)** Consider the Poisson equation with Dirichlet conditions:
$$
\begin{align*}
u(0) &= c_0 \\
u'' &= f(x) \\
u(1) &= c_1
\end{align*}
$$
which we discretise as
$$
\begin{align*}
u_0 &= c_0 \\
 {u_{k-1} - 2u_k + u_{k+1} \over h^2} &= f(x_k) \\
u_1 &= c_1
\end{align*}
$$
As a linear system this equation becomes:
$$
\begin{bmatrix}
ğ_1^âŠ¤ \\
D_h^2 \\
ğ_{n+1}^âŠ¤
\end{bmatrix} ğ® = \begin{bmatrix} c_0 \\ f(x_2) \\ â‹® \\ f(x_{n-1}) \\ c_1 \end{bmatrix}
$$

Thus we solve:
```julia
x = range(0, 1; length = n)
h = step(x)
T = Tridiagonal([fill(1/h^2, n-2); 0], [1; fill(-2/h^2, n-2); 1], [0; fill(1/h^2, n-2)])
u = T \ [1; exp.(x[2:end-1]); 2]
scatter(x, u)
```

We can test convergence on $u(x) = \cos x^2$ which satisfies
$$
\begin{align*}
u(0) = 1 \\
u(1) = \cos 1 \\
u''(x) = -4x^2*cos(x^2) - 2sin(x^2)
\end{align*}
$$
We observe uniform ($âˆ$-norm) convergence:
```julia
function poisson_err(u, c_0, c_1, f, n)
    x = range(0, 1; length = n)
    h = step(x)
    T = Tridiagonal([fill(1/h^2, n-2); 0], [1; fill(-2/h^2, n-2); 1], [0; fill(1/h^2, n-2)])
    uá¶  = T \ [c_0; f.(x[2:end-1]); c_1]
    norm(uá¶  - u.(x), Inf)
end

u = x -> cos(x^2)
f = x -> -4x^2*cos(x^2) - 2sin(x^2)

ns = 10 .^ (1:8) # solve up to n = 10 million
scatter(ns, poisson_err.(u, 1, cos(1), f, ns); xscale=:log10, yscale=:log10, label="error")
plot!(ns, ns .^ (-2); label="1/n^2")
```



## 3. Convergence

We now study convergence of the approaches for the constant-coefficient case.
We will use _Toeplitz matrices_ as a tool to simplify the explanation.

**Definition (Toeplitz)** A _Toeplitz matrix_ has constant diagonals: $T[k,j] = a_{k-j}$.


**Proposition (Bidiagonal Toeplitz inverse)**
The inverse of a $n Ã— n$ bidiagonal Toeplitz matrix is:
$$
\begin{bmatrix}
1 \\
-â„“ & 1 \\
&-â„“ & 1 \\
&& â‹± & â‹± \\
&&& -â„“ & 1\end{bmatrix} = 
 \begin{bmatrix} 1 \\
                            â„“ & 1 \\
                            â„“^2 & â„“ & 1 \\
                            â‹® & â‹± & â‹± & â‹± \\
                            â„“^{n-1} & â‹¯ & â„“^2 & â„“ & 1
\end{bmatrix}              
$$

**Theorem (Forward/Backward Euler convergence)**
Consider the equation
$$
u(0) = c, u'(t) + a u(t) = f(t)
$$
Denote
$$
ğ® := \begin{bmatrix} u(t_1) \\ â‹® \\ u(t_n) \end{bmatrix}
$$
Assume that $u$ is twice-differentiable with uniformly bounded
second derivative.
Then the error for forward/backward Euler is
$$
\|ğ®á¶  - ğ®\|_âˆ, \|ğ®áµ‡ - ğ®\|_âˆ = O(n^{-1})
$$

**Proof** 
We prove the error bound for forward Euler as backward Euler is
similar. This proof consists of two stages: (1) consistency and
(2) stability. 

_Consistency_ means our discretisation approximates the
true equation, that is:
$$
Lğ® = \begin{bmatrix} c \\
        {u(t_2) - u(t_1) \over h} + a u(t_1) \\
        â‹® \\
        {u(t_n) - u(t_{n-1}) \over h} + a u(t_{n-1}) 
\end{bmatrix} = \begin{bmatrix} c \\
        u'(t_1) + a u(t_1) + u''(Ï„_1) h \\
        â‹® \\
        u'(t_{n-1}) + a u(t_{n-1}) + u''(Ï„_{n-1}) h
\end{bmatrix} = \begin{bmatrix} c \\
        f(t_1) + u''(Ï„_1) h \\
        â‹® \\
        f(t_{n-1}) + u''(Ï„_{n-1}) h
\end{bmatrix} = \begin{bmatrix} c \\ ğŸá¶  \end{bmatrix} + \begin{bmatrix} 0 \\ Î´ \end{bmatrix}
$$
where $t_k â‰¤Â Ï„_k â‰¤Â t_{k+1}$, and uniform boundedness
implies that $\|Î´\|_âˆ = O(h)$, or in other words $\|Î´\|_1 = O(1)$. 

_Stability_ means the inverse does not
blow up the error. We need to be a bit careful and
first write, for $â„“ = 1 + h a$,
$$
L = \underbrace{\begin{bmatrix} 1 \\ & h^{-1} \\ && â‹± \\ &&& h^{-1} \end{bmatrix}}_D
 \underbrace{\begin{bmatrix} 1 \\ 
                        -â„“ & 1 \\ & â‹± & â‹± \\
                        && -â„“ &1 \end{bmatrix}}_{LÌƒ}
$$
Stability in this case is the statement that
$$
\|LÌƒ^{-1}\|_{1 â†’ âˆ} â‰¤ (1 +  |a|/n)^{n-1} = O(1)
$$
using the fact (which likely you have seen in first year) that
$$
\lim_{n â†’ âˆ}  (1 +  |a|/n)^{n-1} = \exp|a|.
$$


We now combine stability and consistency. have
$$
\|ğ®á¶  - ğ®\|_âˆ  = \|L^{-1} (Lğ®á¶  - Lğ®)\|_âˆ  = \|LÌƒ^{-1} D^{-1} \begin{bmatrix} 0 \\ Î´ \end{bmatrix} \|_âˆ 
â‰¤ h \|LÌƒ^{-1}\|_{1 â†’ âˆ} \|Î´\|_1 = O(h).
$$

âˆ


### Poisson

For 2D problems we consider Poisson. The first stage is to
row-reduce to get a symmetric tridiagonal (pos. def.) matrix:
$$
\begin{bmatrix} 
1 \\
-1/h^2 & 1 \\
    && 1 \\ 
    &&& â‹± \\
    &&&& 1 & -1/h^2 \\
    &&&&& 1\end{bmatrix} \begin{bmatrix}
    1 \\
    1/h^2 & -2/h^2 & 1/h^2 \\
        & â‹± & â‹± & â‹± \\
        && 1/h^2 & -2/h^2 & 1/h^2 \\
        &&&& 1 \end{bmatrix} = 
\begin{bmatrix}
    1 \\
    0 & -2/h^2 & 1/h^2 \\
        & â‹± & â‹± & â‹± \\
        && 1/h^2 & -2/h^2 & 0 \\
        &&&& 1 \end{bmatrix}.
$$
Considering the right-hand side and dropping
the first and last rows our equation becomes:
$$
{1 \over h^2} \underbrace{\begin{bmatrix}
 -2 & 1 \\
        1 & -2 & â‹± \\
        & â‹± &  â‹± & 1 \\
        && 1 & -2  
        \end{bmatrix}}_Î” \begin{bmatrix}
                u_2 \\ â‹® \\ u_{n-1} \end{bmatrix} = \underbrace{\begin{bmatrix} f(x_2) - c_0/h^2 \\ f(x_3) \\ â‹® \\ f(x_{n-2}) \\ f(x_{n-1}) - c_1/h^2 \end{bmatrix}}_{ğŸáµ–}
$$


**Remark (advanced)** You may recognise $Î”$ as a discrete Laplacian corresponding to a graph with
Dirichlet conditions,
as discussed in first year applied mathematics.
Thus one can interpret finite-differences as approximating a continuous differential equation by
a graph. This view-point extends naturally to higher-dimensional equations. In the problem sheet we also
discuss Neumann series.


**Theorem (Poisson convergence)** Suppose that $u$ is four-times differentiable with uniformly
bounded fourth derivative. Then the finite difference approximation to Poisson converges like $O(n^2)$.


**Proof**


For consistency we need the Taylor series error of second-order finite differences. We have
$$
\begin{align*}
u(x+h) = u(x) + h u'(x) + h^2 {u''(x) \over 2} + h^3 {u'''(x) \over 6} +  h^4 {u^{(4)}(t_+) \over 24}  \\
u(x-h) = u(x) - h u'(x) + h^2 {u''(x) \over 2} - h^3 {u'''(x) \over 6}  +  h^4 {u^{(4)}(t_-)  \over 24}
\end{align*}
$$
where $t_+ âˆˆ [x,x+h]$ and $t_- âˆˆ [x-h,x]$. 
Thus
$$
{u(x+h) - 2u(x) + u(x-h) \over h^2} = u''(x) + h^2 {u^{(4)}(t_+) + u^{(4)}(t_-) \over 24} = u''(x) + O(h^2)
$$
(Note this is a slightly stronger result than used in the solution of PS2.)
Thus we have _consistency_:
$$
{Î” \over h^2} \begin{bmatrix} u_2 \\ â‹® \\ u_{n-1} \end{bmatrix} = ğŸáµ– + {\bf\delta}
$$
where $\|{\bf\delta}\|_âˆ = O(h^2)$.


Following PS5 we deduce that it has the Cholesky-like decomposition
$$
Î” = -\underbrace{\begin{bmatrix} 1 & -1 \\  & â‹± & â‹± \\ && 1 & -1 \\ &&& 1 \end{bmatrix}}_U \underbrace{\begin{bmatrix} 1 \\ -1 & 1 \\ & â‹± & â‹± \\ && -1 & 1 \end{bmatrix} }_{U^âŠ¤}
$$
We can invert:
$$
U^{-1} = \begin{bmatrix} 1 & 1 & â‹¯ & 1 \\ & â‹± & â‹± & â‹® \\
                                        && 1 & 1 \\ &&& 1 \end{bmatrix}
$$
Thus we have _stability_: $\|h^2 Î”^{-1}\|_âˆ â‰¤ h^2 \|U^{-1}\|_{âˆ} \|U^{-âŠ¤}\|_{âˆ} = 1$.

Putting everything together we have, for $ğ® = [u(x_1), â€¦, u(x_n)]^âŠ¤$,
$$
\|ğ®á¶  - ğ®\|_âˆ  = \|Î”^{-1} (Î”ğ®á¶  - Î”ğ®)\|_âˆ  â‰¤ \|h^2  Î”^{-1}\|_âˆ  \| {\bf\delta} \|_âˆ = O(n^{-2})
$$

âˆ

What about the observed instability? The condition number of the matrix provides an intuition
(though not a proof: condition numbers are only upper bounds!).  Here we have
$$
Îº_âˆ(Î”/h^2) = Îº_âˆ(Î”) = \|Î”\|_âˆ \|Î”^{-1}\|_âˆ â‰¤ 2 n^2
$$
One can show by looking at $Î”^{-1}$ directly that the condition number is indeed growing like $n^2$.
Thus we _expect_ floating-point errors to magnified propotional to $n^2$ in the linear solve.
