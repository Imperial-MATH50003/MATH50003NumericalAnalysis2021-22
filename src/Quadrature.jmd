# Interpolation and Gaussian quadrature


Consider integration
$$
\int_a^b f(x) w(x) {\rm d}x.
$$
For periodic integration we approximated (using the Trapezium rule) an integral by a sum.
We can think of it as a weighted sum:
$$
{1 \over 2Ï€} \int_0^{2Ï€} f(Î¸) {\rm d} Î¸ â‰ˆ  âˆ‘_{j=0}^{n-1} w_j f(Î¸_j)
$$
where $w_j = 1/n$. Replacing an integral by a weighted sum is a known as a _quadrature_ rule.
This quadrature rule had several important properties:
1. It was _exact_ for integrating trigonometric polynomials with 2n-1 coefficients
$$
p(Î¸) = \sum_{k=1-n}^{n-1} pÌ‚_k \exp({\rm i}k Î¸)
$$
as seen by the formula
$$
âˆ‘_{j=0}^{n-1} w_j f(Î¸_j) = pÌ‚_0^n = â€¦ + pÌ‚_{n-1} + pÌ‚_0 + pÌ‚_n + â‹¯ = pÌ‚_0 = {1 \over 2Ï€} \int_0^{2Ï€} p(Î¸) {\rm d} Î¸
$$
2. It exactly recovered the coefficients ($pÌ‚_k^n = pÌ‚_k$) for expansions of trigonometric polynomials with $n$ coeffiicents:
$$
p(Î¸) = \sum_{k=-âŒˆ(n-1)/2âŒ‰}^{âŒŠ(n-1)/2âŒ‹} pÌ‚_k \exp({\rm i}k Î¸)
$$
3. It converged fast for smooth, periodic functions $f$.

In this section we consider other quadrature rules
$$
\int_a^b f(x) w(x) {\rm d}x â‰ˆ \sum_{j=1}^n w_j f(x_j)
$$
We want to choose $w_j$ and $x_j$ so that the following properties are satisfied:
1. It is _exact_ for integrating polynomials up to degree $2n-1$:
$$
p(Î¸) = \sum_{k=0}^{2n-1} c_k q_k(x)
$$
2. It exactly recovers the coefficients for expansions:
$$
p(Î¸) = \sum_{k=0}^{n-1} c_k q_k(x)
$$
3. It converges fast for smooth functions $f$.
We will focus on properties (1) and (2) as property (3) is more involved.



1. Polynomial Interpolation: we describe how to interpolate a function by a polynomial.
2. Interpolatory quadrature rule: polynomial interpolation leads naturally to ways to integrate
functions, but onely realisable in the simplest cases.
3. Truncated Jacobi matrices: we see that truncated Jacobi matrices are diagonalisable
in terms of orthogonal polynomials and their zeros. 
3. Gaussian quadrature: Using roots of orthogonal polynomials and truncated Jacobi matrices 
leads naturally to an efficiently
computable interpolatory quadrature rule. The _miracle_ is its exact for twice as many polynomials as
expected.



## 1. Polynomial Interpolation

We already saw a special case of polynomial interpolation, where we saw that the polynomial
$$
f(z) â‰ˆ âˆ‘_{k=0}^{n-1} fÌ‚_k^n z^k
$$
equaled $f$ at evenly spaced points on the unit circle: ${\rm e}^{{\rm i} 2Ï€ j/n}$. 
But here we consider the following:

**Definition (interpolatory polynomial)** Given $n$ distinct points $x_1,â€¦,x_n âˆˆ â„$ 
and $n$ _samples_ $f_1,â€¦,f_n âˆˆ â„$, a degree $n-1$
_interpolatory polynomial_ $p(x)$ satisfies
$$
p(x_j) = f_j
$$

The easiest way to solve this problem is to invert the Vandermonde system:

**Definition (Vandermonde)** The _Vandermonde matrix_ associated with $n$ distinct points $x_1,â€¦,x_n âˆˆ â„$
is the matrix
$$
V := \begin{bmatrix} 1 & x_1 & â‹¯ & x_1^{n-1} \\
                    â‹® & â‹® & â‹± & â‹® \\
                    1 & x_n & â‹¯ & x_n^{n-1}
                    \end{bmatrix}
$$

**Proposition (interpolatory polynomial uniqueness)** 
The interpolatory polynomial is unique, and the Vandermonde matrix is invertible.

**Proof**
Suppose $p$ and $pÌƒ$ are both interpolatory polynomials. Then $p(x) - pÌƒ(x)$ vanishes at $n$ distinct points $x_j$. By the fundamental theorem of
algebra it must be zero, i.e., $p = pÌƒ$.

For the second part, if $V ğœ = 0$ for $ğœ âˆˆ â„$ then for $q(x) = c_1 + â‹¯ + c_n x^{n-1}$ we have
$$
q(x_j) = ğ_j^âŠ¤ V ğœ = 0
$$
hence $q$ vanishes at $n$ distinct points and is therefore 0, i.e., $ğœ = 0$.

âˆ

Thus a quick-and-dirty way to to do interpolation is to invert the Vandermonde matrix
(which we saw in the least squares setting with more samples then coefficients):
```julia
using Plots, LinearAlgebra
f = x -> cos(3x)
n = 10

x = range(0, 1; length=n-1)# evenly spaced points (BAD for interpolation)
V = x .^ (0:n-1)' # Vandermonde matrix
c = V \ f.(x) # coefficients of interpolatory polynomial
p = x -> dot(c, x .^ (0:n-1))

g = range(0,1; length=1000) # plotting grid
plot(g, f.(g); label="function")
plot!(g, p.(g); label="interpolation")
```

But it turns out we can also construct the interpolatory polynomial directly.
We will use the following with equal $1$ at one grid point
and are zero at the others:

**Definition (Lagrange basis polynomial)** The _Lagrange basis polynomial_ is defined as
$$
â„“_k(x) := âˆ_{j â‰  k} {x-x_j \over x_k - x_j} =  {(x-x_1) â‹¯(x-x_{k-1})(x-x_{k+1}) â‹¯ (x-x_n) \over (x_k - x_1) â‹¯ (x_k - x_{k-1}) (x_k - x_{k+1}) â‹¯ (x_k - x_n)}
$$

Plugging in the grid points verifies the following:

**Proposition (delta interpolation)**
$$
â„“_k(x_j) = Î´_{kj}
$$

We can use these to construct the interpolatory polynomial:

**Theorem (Lagrange interpolation)**
The unique  polynomial of degree at most $n-1$ that interpolates $f$ at $x_j$ is:
$$
p(x) = f(x_1) â„“_1(x) + â‹¯ + f(x_n) â„“_n(x)
$$

**Proof**
Note that
$$
p(x_j) = âˆ‘_{k=1}^n f(x_k) â„“_k(x_j) = f(x_k)
$$
so we just need to show it is unique. Suppose $pÌƒ(x)$ is a  polynomial
of degree at most $n-1$
that also interpolates $f$.


âˆ

**Example** We can interpolate $\exp(x)$ at the points $0,1,2$:
$$
p(x) = â„“_1(x) + {\rm e} â„“_2(x) + {\rm e}^2 â„“_3(x) = 
{(x - 1) (x-2) \over (-1)(-2)} + {\rm e} {x (x-2) \over (-1)} + 
{\rm e}^2 {x (x-1) \over 2} = (1/2 - {\rm e} +{\rm e}^2/2)x^2  
-  (-3/2 + 2 {\rm e}  - {\rm e}^2 /2)  x + 1
$$


**Remark** Interpolating at evenly spaced points is a really **bad** idea:
interpolation is inheritely ill-conditioned. 
The problem sheet asks you to explore
this experimentally.

## 2. Interpolatory quadrature rules

**Definition (interpolatory quadrature rule)** Given a set of points $x_1,â€¦,x_n$
the interpolatory quadrature rule is:
$$
Î£_n^{\rm i}[f] := âˆ‘_{k=1}^n w_k f(x_k)
$$
where
$$
w_k := âˆ«_a^b â„“_k(x) w(x) {\rm d} x
$$


**Proposition (interpolatory quadrature is exact for polynomials)** 
Interpolatory quadrature is exact for all degree $n-1$ polynomials $p$:
$$
âˆ«_a^b p(x) w(x) {\rm d}x = Î£_n^{\rm i}[f]
$$

**Proof**
The result follows since, by uniqueness of interpolatory polynomial:
$$
p(x) = âˆ‘_{k=1}^n p(x_k) â„“_k(x)
$$

âˆ


## 3. Roots of orthogonal polynomials and truncated Jacobi matrices

The key to property (1) is to use _roots (zeros) of $q_n(x)$_.

**Lemma** $q_n(x)$ has exactly $n$ distinct roots.

**Proof**

Suppose $x_1, â€¦,x_j$ are the roots where $q_n(x)$ changes sign, that is,
$$
q_n(x) = c_j (x-x_j) + O((x-x_j)^2)
$$
for $c_j â‰  0$. Then
$$
q_n(x) (x-x_1) â‹¯(x-x_j)
$$
does not change sign.
In other words:
$$
âŸ¨q_n,(x-x_1) â‹¯(x-x_j) âŸ© = \int_a^b q_n(x) (x-x_1) â‹¯(x-x_j) {\rm d} x â‰  0.
$$
This is only possible if $j = n$.

âˆ

**Definition (truncated Jacobi matrix)** Given a symmetric Jacobi matrix $X$,
(or the weight $w(x)$ whose orthonormal polynomials are associated with $X$)
 the _truncated Jacobi matrix_ is
$$
X_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & â‹± & â‹± \\
                         & â‹± & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} âˆˆ â„^{n Ã— n}
$$



**Lemma (zeros)** The zeros $x_1, â€¦,x_n$ of $q_n(x)$ are the eigenvalues of the truncated Jacobi matrix $X_n$.
More precisely,
$$
X_n Q_n = Q_n \begin{bmatrix} x_1 \\ & â‹± \\ && x_n \end{bmatrix}
$$
for the orthogonal matrix
$$
Q_n = \begin{bmatrix}
q_0(x_1) & â‹¯ & q_0(x_n) \\
â‹®  & â‹¯ & â‹®  \\
q_{n-1}(x_1) & â‹¯ & q_{n-1}(x_n)
\end{bmatrix} \begin{bmatrix} Î±_1^{-1} \\ & â‹± \\ && Î±_n^{-1} \end{bmatrix}
$$
where $Î±_k = \sqrt{q_0(x_k)^2 + â‹¯ + q_{n-1}(x_k)^2}$.

**Proof**

We construct the eigenvector (noting $b_{n-1} p_n(x_j) = 0$):
$$
X_n \begin{bmatrix} p_0(x_j) \\ â‹® \\ p_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 p_0(x_j) + b_0 p_1(x_j) \\
 b_0 p_0(x_j) + a_1 p_1(x_j) + b_1 p_2(x_j) \\
â‹® \\
b_{n-3} p_{n-3}(x_j) + a_{n-2} p_{n-2}(x_j) + b_{n-2} p_{n-1}(x_j) \\
b_{n-2} p_{n-2}(x_j) + a_{n-1} p_{n-1}(x_j) + b_{n-1} p_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} p_0(x_j) \\
 p_1(x_j) \\
â‹® \\
p_n(x_j)
\end{bmatrix}
$$
The result follows from normalising the eigenvectors. Since $X_n$ is symmetric
the eigenvector matrix is orthogonal.

âˆ



## 4. Gaussian quadrature

Gaussian quadrature is the interpolatory quadrature rule corresponding
to the grid $x_k$ defined as the roots of the orthogonal polynomial $q_n(x)$.
We shall see that it is exact for polynomials up to degree $2n-1$, i.e., double
the degree of other interpolatory quadrature rules from other grids.



**Definition (Gauss quadrature)** Given a weight $w(x)$, the Gauss quadrature rule is:
$$
âˆ«_a^b f(x)w(x) {\rm d}x â‰ˆ \underbrace{âˆ‘_{k=1}^n w_k f(x_k)}_{Î£_n^w[f]}
$$
where $x_1,â€¦,x_n$ are the eigenvalues of $X_n$ and
$$
w_k = Q_n[k,1]^2 = {1 \over Î±_k^2}
$$

In analogy to how Fourier series are orthogonal with respect to Trapezium rule,
Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

**Lemma (Discrete orthogonality)**
For $0 â‰¤Â â„“,m â‰¤Â n-1$,
$$
Î£_n^w[q_â„“ q_m] = Î´_{â„“m}
$$

**Proof**
$$
Î£_n^w[q_â„“ q_m] = âˆ‘_{k=1}^n {q_â„“(x_k) q_m(x_k) \over Î±_k^2}
= \left[q_â„“(x_1)/ Î±_1 | â‹¯ | {q_â„“(x_n)/ Î±_n}\right] 
\begin{bmatrix}
q_m(x_1)/Î±_1 \\
â‹® \\
q_m(x_n)/Î±_n \end{bmatrix} = ğ_â„“ Q_n Q_n^âŠ¤ ğ_m = Î´_{â„“m}
$$

âˆ

Just as approximating Fourier coefficients using Trapezium rule gives a way of
interpolating at the grid, so does Gaussian quadrature:

**Theorem (interpolation via quadrature)**
$$
f_n(x) = âˆ‘_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := Î£_n^w[f q_k]
$$
interpolates $f(x)$ at the Gaussian quadrature points $x_1,â€¦,x_n$.

**Proof**

Note that we can write:
$$
\begin{bmatrix}
c_0^n \\
â‹® \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix}
f_1 \\
â‹® \\
f_n \end{bmatrix}.
$$

Consider the Vandermonde-like matrix:
$$
VÌƒ = \begin{bmatrix} q_0(x_1) & â‹¯ & q_{n-1}(x_1) \\
                â‹® & â‹± & â‹® \\
                q_0(x_n) & â‹¯ & q_{n-1}(x_n) \end{bmatrix}
$$
Note that if $p(x) = [q_0(x) | â‹¯ | q_{n-1}(x)] ğœ$ then
$$
\begin{bmatrix}
p(x_1) \\
â‹® \\
p(x_n)
\end{bmatrix} = VÌƒ ğœ
$$
But we see that (similar to the Fourier case)
$$
Q_n^w VÌƒ = \begin{bmatrix} Î£_n^w[q_0 q_0] & â‹¯ & Î£_n^w[q_0 q_{n-1}]\\
                â‹® & â‹± & â‹® \\
                Î£_n^w[q_{n-1} q_0] & â‹¯ & Î£_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I_n
$$

âˆ

**Corollary** Gaussian quadrature is an interpolatory quadrature rule.



A consequence of being an interpolatory quadrature rule is that it is exact for all
polynomials of degree $n-1$. The _miracle_ of Gaussian quadrature is it is exact for twice
as many!



**Theorem (Exactness of Gauss quadrature)** If $p(x)$ is a degree $2n-1$ polynomial then
Gauss quadrature is exact:
$$
âˆ«_a^b p(x)w(x) {\rm d}x = Î£_n^w[p].
$$

**Proof**
Using polynomial division algorithm (e.g. by matching terms) we can write
$$
p(x) = q_n(x) s(x) + r(x)
$$
where $s$ and $r$ are degree $n-1$. Then we have:
$$
Î£_n^w[p] = \undebrace{Î£_n^w[q_n s]}_{$0$ since evaluating $q_n$ at zeros} + Î£_n^w[r] = âˆ«_a^b r(x) w(x) {\rm d}x
= \underbrace{âˆ«_a^b q_n(x)s(x) w(x) {\rm d}x}_{$0$ since $s$ is degree $<n$}  + âˆ«_a^b r(x) w(x) {\rm d}x = âˆ«_a^b p(x)w(x) {\rm d}x.
$$
âˆ

