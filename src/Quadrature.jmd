# Interpolation and Gaussian quadrature



_Polynomial interpolation_ is the process of finding a polynomial that equals data at a precise set of points.
_Quadrature_ is the act of approximating an integral by a weighted sum:
$$
\int_a^b f(x) w(x) {\rm d}x â‰ˆ \sum_{j=1}^n w_j f(x_j).
$$
In these notes we see that the two concepts are intrinsically linked:  interpolation leads naturally
to quadrature rules. Moreover, by using a set of points $x_j$ linked to orthogonal polynomials we get
significantly more accurate rules, and in fact can use quadrature to compute expansions in orthogonal polynomials that
interpolate. That is, we can mirror the link between the Trapezium rule, Fourier series, and interpolation but
now for polynomials.


1. Polynomial Interpolation: we describe how to interpolate a function by a polynomial.
3. Truncated Jacobi matrices: we see that truncated Jacobi matrices are diagonalisable
in terms of orthogonal polynomials and their zeros. 
2. Interpolatory quadrature rule: polynomial interpolation leads naturally to ways to integrate
functions, but onely realisable in the simplest cases.
3. Gaussian quadrature: Using roots of orthogonal polynomials and truncated Jacobi matrices 
leads naturally to an efficiently
computable interpolatory quadrature rule. The _miracle_ is its exact for twice as many polynomials as
expected.



## 1. Polynomial Interpolation

We already saw a special case of polynomial interpolation, where we saw that the polynomial
$$
f(z) â‰ˆ âˆ‘_{k=0}^{n-1} fÌ‚_k^n z^k
$$
equaled $f$ at evenly spaced points on the unit circle: ${\rm e}^{{\rm i} 2Ï€ j/n}$. 
But here we consider the following:

**Definition (interpolatory polynomial)** Given $n$ distinct points $x_1,â€¦,x_n âˆˆ â„$ 
and $n$ _samples_ $f_1,â€¦,f_n âˆˆ â„$, a degree $n-1$
_interpolatory polynomial_ $p(x)$ satisfies
$$
p(x_j) = f_j
$$

The easiest way to solve this problem is to invert the Vandermonde system:

**Definition (Vandermonde)** The _Vandermonde matrix_ associated with $n$ distinct points $x_1,â€¦,x_n âˆˆ â„$
is the matrix
$$
V := \begin{bmatrix} 1 & x_1 & â‹¯ & x_1^{n-1} \\
                    â‹® & â‹® & â‹± & â‹® \\
                    1 & x_n & â‹¯ & x_n^{n-1}
                    \end{bmatrix}
$$

**Proposition (interpolatory polynomial uniqueness)** 
The interpolatory polynomial is unique, and the Vandermonde matrix is invertible.

**Proof**
Suppose $p$ and $pÌƒ$ are both interpolatory polynomials. Then $p(x) - pÌƒ(x)$ vanishes at $n$ distinct points $x_j$. By the fundamental theorem of
algebra it must be zero, i.e., $p = pÌƒ$.

For the second part, if $V ğœ = 0$ for $ğœ âˆˆ â„$ then for $q(x) = c_1 + â‹¯ + c_n x^{n-1}$ we have
$$
q(x_j) = ğ_j^âŠ¤ V ğœ = 0
$$
hence $q$ vanishes at $n$ distinct points and is therefore 0, i.e., $ğœ = 0$.

âˆ

Thus a quick-and-dirty way to to do interpolation is to invert the Vandermonde matrix
(which we saw in the least squares setting with more samples then coefficients):
```julia
using Plots, LinearAlgebra
f = x -> cos(10x)
n = 5

x = range(0, 1; length=n)# evenly spaced points (BAD for interpolation)
V = x .^ (0:n-1)' # Vandermonde matrix
c = V \ f.(x) # coefficients of interpolatory polynomial
p = x -> dot(c, x .^ (0:n-1))

g = range(0,1; length=1000) # plotting grid
plot(g, f.(g); label="function")
plot!(g, p.(g); label="interpolation")
scatter!(x, f.(x); label="samples")
```

But it turns out we can also construct the interpolatory polynomial directly.
We will use the following with equal $1$ at one grid point
and are zero at the others:

**Definition (Lagrange basis polynomial)** The _Lagrange basis polynomial_ is defined as
$$
â„“_k(x) := âˆ_{j â‰  k} {x-x_j \over x_k - x_j} =  {(x-x_1) â‹¯(x-x_{k-1})(x-x_{k+1}) â‹¯ (x-x_n) \over (x_k - x_1) â‹¯ (x_k - x_{k-1}) (x_k - x_{k+1}) â‹¯ (x_k - x_n)}
$$

Plugging in the grid points verifies the following:

**Proposition (delta interpolation)**
$$
â„“_k(x_j) = Î´_{kj}
$$

We can use these to construct the interpolatory polynomial:

**Theorem (Lagrange interpolation)**
The unique  polynomial of degree at most $n-1$ that interpolates $f$ at $x_j$ is:
$$
p(x) = f(x_1) â„“_1(x) + â‹¯ + f(x_n) â„“_n(x)
$$

**Proof**
Note that
$$
p(x_j) = âˆ‘_{k=1}^n f(x_k) â„“_k(x_j) = f(x_j)
$$
so we just need to show it is unique. Suppose $pÌƒ(x)$ is a  polynomial
of degree at most $n-1$
that also interpolates $f$.


âˆ

**Example** We can interpolate $\exp(x)$ at the points $0,1,2$:
$$
\begin{align*}
p(x) &= â„“_1(x) + {\rm e} â„“_2(x) + {\rm e}^2 â„“_3(x) = 
{(x - 1) (x-2) \over (-1)(-2)} + {\rm e} {x (x-2) \over (-1)} + 
{\rm e}^2 {x (x-1) \over 2} \\
&= (1/2 - {\rm e} +{\rm e}^2/2)x^2  
+  (-3/2 + 2 {\rm e}  - {\rm e}^2 /2)  x + 1
\end{align*}
$$


**Remark** Interpolating at evenly spaced points is a really **bad** idea:
interpolation is inheritely ill-conditioned. 
The problem sheet asks you to explore
this experimentally.

## 2. Roots of orthogonal polynomials and truncated Jacobi matrices

We now consider roots (zeros) of orthogonal polynomials $q_n(x)$. This is important as we shall
see they are useful for interpolation and quadrature. For interpolation to be well-defined we
first need to guarantee that the roots are distinct.

**Lemma** $q_n(x)$ has exactly $n$ distinct roots.

**Proof**

Suppose $x_1, â€¦,x_j$ are the roots where $q_n(x)$ changes sign, that is,
$$
q_n(x) = c_j (x-x_j) + O((x-x_j)^2)
$$
for $c_j â‰  0$. Then
$$
q_n(x) (x-x_1) â‹¯(x-x_j)
$$
does not change sign.
In other words:
$$
âŸ¨q_n,(x-x_1) â‹¯(x-x_j) âŸ© = \int_a^b q_n(x) (x-x_1) â‹¯(x-x_j) w(x) {\rm d} x â‰  0.
$$
This is only possible if $j = n$ as $q_n(x)$ is orthogonal w.r.t. all lower degree
polynomials.

âˆ

**Definition (truncated Jacobi matrix)** Given a symmetric Jacobi matrix $X$,
(or the weight $w(x)$ whose orthonormal polynomials are associated with $X$)
 the _truncated Jacobi matrix_ is
$$
X_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & â‹± & â‹± \\
                         & â‹± & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} âˆˆ â„^{n Ã— n}
$$



**Lemma (zeros)** The zeros $x_1, â€¦,x_n$ of $q_n(x)$ are the eigenvalues of the truncated Jacobi matrix $X_n$.
More precisely,
$$
X_n Q_n = Q_n \begin{bmatrix} x_1 \\ & â‹± \\ && x_n \end{bmatrix}
$$
for the orthogonal matrix
$$
Q_n = \begin{bmatrix}
q_0(x_1) & â‹¯ & q_0(x_n) \\
â‹®  & â‹¯ & â‹®  \\
q_{n-1}(x_1) & â‹¯ & q_{n-1}(x_n)
\end{bmatrix} \begin{bmatrix} Î±_1^{-1} \\ & â‹± \\ && Î±_n^{-1} \end{bmatrix}
$$
where $Î±_k = \sqrt{q_0(x_k)^2 + â‹¯ + q_{n-1}(x_k)^2}$.

**Proof**

We construct the eigenvector (noting $b_{n-1} q_n(x_j) = 0$):
$$
X_n \begin{bmatrix} q_0(x_j) \\ â‹® \\ q_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 q_0(x_j) + b_0 q_1(x_j) \\
 b_0 q_0(x_j) + a_1 q_1(x_j) + b_1 q_2(x_j) \\
â‹® \\
b_{n-3} q_{n-3}(x_j) + a_{n-2} q_{n-2}(x_j) + b_{n-2} q_{n-1}(x_j) \\
b_{n-2} q_{n-2}(x_j) + a_{n-1} q_{n-1}(x_j) + b_{n-1} q_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} q_0(x_j) \\
 q_1(x_j) \\
â‹® \\
q_{n-1}(x_j)
\end{bmatrix}
$$
The result follows from normalising the eigenvectors. Since $X_n$ is symmetric
the eigenvector matrix is orthogonal.

âˆ

**Example (Chebyshev roots)** Consider $T_n(x) = \cos n {\rm acos}\, x$. The roots 
are $x_j = \cos Î¸_j$ where $Î¸_j = (j-1/2)Ï€/n$ for $j = 1,â€¦,n$ are the roots of $\cos n Î¸$
that are inside $[0,Ï€]$. 

Consider the $n = 3$ case where we have
$$
x_1,x_2,x_3 = \cos(Ï€/6),\cos(Ï€/2),\cos(5Ï€/6) = \sqrt{3}/2,0,-\sqrt{3/2}
$$
We also have from the 3-term recurrence:
$$
\begin{align*}
T_0(x) = 1 \\
T_1(x) = x \\
T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1 \\
T_3(x) = 2x T_2(x) - T_1(x) = 4x^2-3x
\end{align*}
$$
We orthonormalise by rescaling
$$
\begin{align*}
q_0(x) &= 1/\sqrt{Ï€} \\
q_k(x) &= T_k(x) \sqrt{2}/\sqrt{Ï€}
\end{align*}
$$
so that the Jacobi matrix is symmetric:
$$
x [q_0(x)|q_1(x)|â€¦] = [q_0(x)|q_1(x)|â€¦] \underbrace{\begin{bmatrix} 0 & 1/\sqrt{2} \\
                            1/\sqrt{2} & 0 & 1/2 \\
                            1/2 & 0 & 1/2 \\
                                & 1/2 & 0 & â‹± \\
                                && â‹± & â‹±
\end{bmatrix}}_X
$$
We can then confirm that we have constructed an eigenvector/eigenvalue of the $3 Ã— 3$ truncation of the Jacobi matrix,
e.g. at $x_2 = 0$:
$$
\begin{bmatrix} 
0 & 1/\sqrt{2}
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} q_0(0) \\ q_1(0) \\ q_2(0) 
    \end{bmatrix} = {1 \over \sqrt Ï€} \begin{bmatrix} 
0 & 1/\sqrt{2}
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ -{1 \over \sqrt{2}}
    \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 0
    \end{bmatrix}
$$



## 3. Interpolatory quadrature rules

**Definition (interpolatory quadrature rule)** Given a set of points $ğ± = [x_1,â€¦,x_n]$
the interpolatory quadrature rule is:
$$
Î£_n^{w,ğ±}[f] := âˆ‘_{k=1}^n w_k f(x_k)
$$
where
$$
w_k := âˆ«_a^b â„“_k(x) w(x) {\rm d} x
$$


**Proposition (interpolatory quadrature is exact for polynomials)** 
Interpolatory quadrature is exact for all degree $n-1$ polynomials $p$:
$$
âˆ«_a^b p(x) w(x) {\rm d}x = Î£_n^{w,ğ±}[f]
$$

**Proof**
The result follows since, by uniqueness of interpolatory polynomial:
$$
p(x) = âˆ‘_{k=1}^n p(x_k) â„“_k(x)
$$

âˆ

**Example (arbitrary points)** Find the interpolatory quadrature rule for $w(x) = 1$ on $[0,1]$ with  points $[x_1,x_2,x_3] = [0,1/4,1]$?
We have:
$$
\begin{align*}
w_1 = \int_0^1 w(x) â„“_1(x) {\rm d}x  = \int_0^1 {(x-1/4)(x-1) \over (-1/4)(-1)}{\rm d}x = -1/6 \\
w_2 = \int_0^1 w(x) â„“_2(x) {\rm d}x  = \int_0^1 {x(x-1) \over (1/4)(-1)}{\rm d}x = 8/9 \\
w_3 = \int_0^1 w(x) â„“_3(x) {\rm d}x  = \int_0^1 {x(x-1/4) \over 1/4}{\rm d}x = 5/18
\end{align*}
$$
That is we have
$$
Î£_n^{w,ğ±}[f]  = -{f(0) \over 6} + {8f(1/4) \over 9} + {5 f(1) \over 18}
$$
This is indeed exact for polynomials up to degree $2$ (and no more):
$$
Î£_n^{w,ğ±}[1] = 1, Î£_n^{w,ğ±}[x] = 1/2, Î£_n^{w,ğ±}[x^2] = 1/3, Î£_n^{w,ğ±}[x^2] = 7/24 â‰  1/4.
$$

**Example (Chebyshev roots)** Find the interpolatory quadrature rule for $w(x) = 1/\sqrt{1-x^2}$ on $[-1,1]$ with points equal to the
roots of $T_3(x)$. This is a special case of Gaussian quadrature which we will approach in another way below. We use:
$$
\int_{-1}^1 w(x) {\rm d}x = Ï€, \int_{-1}^1 xw(x) {\rm d}x = 0, \int_{-1}^1 x^2 w(x) {\rm d}x = {Ï€/2}
$$
Recall from before that $x_1,x_2,x_3 = \sqrt{3}/2,0,-\sqrt{3}/2$. Thus we have:
$$
\begin{align*}
w_1 = \int_{-1}^1 w(x) â„“_1(x) {\rm d}x = \int_{-1}^1 {x(x+\sqrt{3}/2) \over (\sqrt{3}/2) \sqrt{3} \sqrt{1-x^2}}{\rm d}x = {Ï€ \over 3} \\
w_2 = \int_{-1}^1 w(x) â„“_2(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2)(x+\sqrt{3}/2) \over (-3/4)\sqrt{1-x^2}}{\rm d}x = {Ï€ \over 3} \\
w_3 = \int_{-1}^1 w(x) â„“_3(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2) x \over (-\sqrt{3})(-\sqrt{3}/2) \sqrt{1-x^2}}{\rm d}x = {Ï€ \over 3}
\end{align*}
$$
(It's not a coincidence that they are all the same but this will differ for roots of other OPs.) 
That is we have
$$
Î£_n^{w,ğ±}[f]  = {Ï€ \over 3}(f(\sqrt{3}/2) + f(0) + f(-\sqrt{3}/2)
$$
This is indeed exact for polynomials up to degree $n-1=2$, but it goes all the way up to $2n-1 = 5$:
$$
\begin{align*}
Î£_n^{w,ğ±}[1] &= Ï€, Î£_n^{w,ğ±}[x] = 0, Î£_n^{w,ğ±}[x^2] = {Ï€ \over 2}, \\
Î£_n^{w,ğ±}[x^3] &= 0, Î£_n^{w,ğ±}[x^4] &= {3 Ï€ \over 8}, Î£_n^{w,ğ±}[x^5] &= 0 \\
Î£_n^{w,ğ±}[x^6] &= {9 Ï€ \over 32} â‰ Â {5 Ï€ \over 16}
\end{align*}
$$
We shall explain this miracle next.





## 4. Gaussian quadrature

Gaussian quadrature is the interpolatory quadrature rule corresponding
to the grid $x_k$ defined as the roots of the orthogonal polynomial $q_n(x)$.
We shall see that it is exact for polynomials up to degree $2n-1$, i.e., double
the degree of other interpolatory quadrature rules from other grids.



**Definition (Gauss quadrature)** Given a weight $w(x)$, the Gauss quadrature rule is:
$$
âˆ«_a^b f(x)w(x) {\rm d}x â‰ˆ \underbrace{âˆ‘_{k=1}^n w_k f(x_k)}_{Î£_n^w[f]}
$$
where $x_1,â€¦,x_n$ are the eigenvalues of $X_n$ and
$$
w_k = Q_n[k,1]^2 = {1 \over Î±_k^2}
$$

In analogy to how Fourier series are orthogonal with respect to Trapezium rule,
Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

**Lemma (Discrete orthogonality)**
For $0 â‰¤Â â„“,m â‰¤Â n-1$,
$$
Î£_n^w[q_â„“ q_m] = Î´_{â„“m}
$$

**Proof**
$$
Î£_n^w[q_â„“ q_m] = âˆ‘_{k=1}^n {q_â„“(x_k) q_m(x_k) \over Î±_k^2}
= \left[q_â„“(x_1)/ Î±_1 | â‹¯ | {q_â„“(x_n)/ Î±_n}\right] 
\begin{bmatrix}
q_m(x_1)/Î±_1 \\
â‹® \\
q_m(x_n)/Î±_n \end{bmatrix} = ğ_â„“ Q_n Q_n^âŠ¤ ğ_m = Î´_{â„“m}
$$

âˆ

Just as approximating Fourier coefficients using Trapezium rule gives a way of
interpolating at the grid, so does Gaussian quadrature:

**Theorem (interpolation via quadrature)**
$$
f_n(x) = âˆ‘_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := Î£_n^w[f q_k]
$$
interpolates $f(x)$ at the Gaussian quadrature points $x_1,â€¦,x_n$.

**Proof**

Note that we can write:
$$
\begin{bmatrix}
c_0^n \\
â‹® \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix}
f_1 \\
â‹® \\
f_n \end{bmatrix}.
$$

Consider the Vandermonde-like matrix:
$$
VÌƒ = \begin{bmatrix} q_0(x_1) & â‹¯ & q_{n-1}(x_1) \\
                â‹® & â‹± & â‹® \\
                q_0(x_n) & â‹¯ & q_{n-1}(x_n) \end{bmatrix}
$$
Note that if $p(x) = [q_0(x) | â‹¯ | q_{n-1}(x)] ğœ$ then
$$
\begin{bmatrix}
p(x_1) \\
â‹® \\
p(x_n)
\end{bmatrix} = VÌƒ ğœ
$$
But we see that (similar to the Fourier case)
$$
Q_n^w VÌƒ = \begin{bmatrix} Î£_n^w[q_0 q_0] & â‹¯ & Î£_n^w[q_0 q_{n-1}]\\
                â‹® & â‹± & â‹® \\
                Î£_n^w[q_{n-1} q_0] & â‹¯ & Î£_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I_n
$$

âˆ

**Corollary** Gaussian quadrature is an interpolatory quadrature rule.



A consequence of being an interpolatory quadrature rule is that it is exact for all
polynomials of degree $n-1$. The _miracle_ of Gaussian quadrature is it is exact for twice
as many!



**Theorem (Exactness of Gauss quadrature)** If $p(x)$ is a degree $2n-1$ polynomial then
Gauss quadrature is exact:
$$
âˆ«_a^b p(x)w(x) {\rm d}x = Î£_n^w[p].
$$

**Proof**
Using polynomial division algorithm (e.g. by matching terms) we can write
$$
p(x) = q_n(x) s(x) + r(x)
$$
where $s$ and $r$ are degree $n-1$. Then we have:
$$
\begin{align*}
Î£_n^w[p] &= \underbrace{Î£_n^w[q_n s]}_{\hbox{$0$ since evaluating $q_n$ at zeros}} + Î£_n^w[r] = âˆ«_a^b r(x) w(x) {\rm d}x
= \underbrace{âˆ«_a^b q_n(x)s(x) w(x) {\rm d}x}_{\hbox{$0$ since $s$ is degree $<n$}}  + âˆ«_a^b r(x) w(x) {\rm d}x \\
&= âˆ«_a^b p(x)w(x) {\rm d}x.
\end{align*}
$$
âˆ


**Example (Chebyshev points revisited)** 
Consider the construction of Gaussian quadrature for $n = 3$. To determine the weights we need:
$$
w_k^{-1} = Î±_k^2 = q_0(x_k)^2 + q_1(x_k)^2 + q_2(x_k)^2 = 
{1 \over Ï€} + {2 \over Ï€} x_k^2 + {2 \over Ï€} (2x_k^2-1)^2
$$
We can check each case and deduce that $w_k = Ï€/3$.
Thus we recover the interpolatory quadrature rule.