{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norms, singular values, and conditioning\n",
    "\n",
    "\n",
    "In this lecture we discuss matrix and vector norms. The matrix $2$-norm involves\n",
    "_singular values_, which are a measure of how matrices \"stretch\" vectors, similar to\n",
    "eigenvalues but more robust. We also introduce condition of problems, and show that\n",
    "the singular values of a matrix give a notion of a _condition number_, which allows us\n",
    "to bound errors introduced by floating point numbers in linear algebra operations.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Vector norms\n",
    "\n",
    "Recall the definition of a (vector-)norm:\n",
    "\n",
    "**Definition (vector-norm)** A norm $\\|\\cdot\\|$ on $â„^n$ is a function that satisfies the following, for $ğ±,ğ² âˆˆ â„^n$ and\n",
    "$c âˆˆ â„$:\n",
    "1. Triangle inequality: $\\|ğ± + ğ² \\| â‰¤ \\|ğ±\\| + \\|ğ²\\|$\n",
    "2. Homogeneneity: $\\| c ğ± \\| = |c| \\| ğ± \\|$\n",
    "3. Positive-definiteness: $\\|ğ±\\| = 0$ implies that $ğ± = 0$.\n",
    "\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "**Definition (p-norm)**\n",
    "For $1 â‰¤ p < âˆ$ and $ğ± \\in â„^n$, define the $p$-norm:\n",
    "$$\n",
    "\\|ğ±\\|_p := \\left(\\sum_{k=1}^n |x_k|^p\\right)^{1/p}\n",
    "$$\n",
    "where $x_k$ is the $k$-th entry of $ğ±$. \n",
    "For $p = âˆ$ we define\n",
    "$$\n",
    "\\|ğ±\\|_âˆ := \\max_k |x_k|\n",
    "$$\n",
    "\n",
    "**Theorem (p-norm)** $\\| â‹… \\|_p$ is a norm for $1 â‰¤ p â‰¤ âˆ$.\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Homogeneity and positive-definiteness are straightforward: e.g.,\n",
    "$$\n",
    "\\|c ğ±\\|_p = (\\sum_{k=1}^n |cx_k|^p)^{1/p} = (|c|^p \\sum_{k=1}^n |x_k|^p)^{1/p} = |c| \\| ğ± \\|\n",
    "$$\n",
    "and if $\\| ğ± \\|_p = 0$ then all $|x_k|^p$ are have to be zero.\n",
    "\n",
    "For $p = 1,âˆ$ the triangle inequality is also straightforward:\n",
    "$$\n",
    "\\| ğ± + ğ² \\|_âˆ = \\max_k (|x_k + y_k|) â‰¤Â \\max_k (|x_k| + |y_k|) â‰¤ \\|ğ±\\|_âˆ + \\|ğ²\\|_âˆ\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\| ğ± + ğ² \\|_1 = \\sum_{k=1}^n |x_k + y_k| â‰¤Â  \\sum_{k=1}^n (|x_k| + |y_k|) = \\| ğ± \\|_1 + \\| ğ²\\|_1\n",
    "$$\n",
    "\n",
    "For $p = 2$ it can be proved using the Cauchyâ€“Schwartz inequality:\n",
    "$$\n",
    "|ğ±^âŠ¤ ğ²| â‰¤ \\| ğ± \\|_2 \\| ğ² \\|_2\n",
    "$$\n",
    "That is, we have\n",
    "$$\n",
    "\\| ğ± + ğ² \\|^2 = \\|ğ±\\|^2 + 2 ğ±^âŠ¤ ğ² + \\|ğ²\\|^2 â‰¤Â \\|ğ±\\|^2 + 2\\| ğ± \\| \\| ğ² \\| + \\|ğ²\\|^2 = (\\| ğ± \\| +  \\| ğ² \\|)\n",
    "$$\n",
    "\n",
    "For general $1 < p  < âˆ$, it suffices to assume $\\| ğ± + ğ² \\| = 1$.\n",
    "consider $\\| ğ± + ğ² \\|^p$...\n",
    "\n",
    "âˆ\n",
    "\n",
    "\n",
    " In Julia can use the inbuilt `norm` function to calculate norms:\n",
    " ```julia\n",
    " norm([1,-2,3]) == norm([1,-2,3],2) == sqrt(1^2+2^2+3^2);\n",
    " norm([1,-2,3],1) == sqrt(1 + 2 + 3)\n",
    " ```\n",
    "\n",
    "\n",
    "## 2. Matrix norms\n",
    " Just like vectors, matrices have norms that measure their \"length\".  The simplest example is the FrÃ¶benius norm, \n",
    " defined for an $m \\times n$ real matrix $A$ as\n",
    "$$\n",
    "\\|A\\|_F := \\sqrt{\\sum_{k=1}^m \\sum_{j=1}^n A_{kj}^2}\n",
    "$$\n",
    "While this is the simplest norm, it is not the most useful.  Instead, we will build a matrix norm from a \n",
    "vector norm:\n",
    "\n",
    "\n",
    "\n",
    "**Definition (matrix-norm)** Suppose $A âˆˆ â„^{m Ã— n}$  and consider two norms $\\| â‹… \\|_X$ on $â„^n$  and \n",
    "$\\| â‹… \\|_Y$ on $â„^n$. Define the _(induced) matrix norm_ as:\n",
    "$$\n",
    "\\|A \\|_{X â†’ Y} := \\sup_{ğ¯ : \\|ğ¯\\|_X=1} \\|A ğ¯\\|_Y\n",
    "$$\n",
    "Also define\n",
    "$$\n",
    "\\|A\\|_X \\triangleq \\|A\\|_{X \\rightarrow X}\n",
    "$$\n",
    "\n",
    "For  the induced 2, 1, and $âˆ$-norm we use\n",
    "$$\n",
    "\\|A\\|_2, \\|A\\|_1 \\qquad \\hbox{and} \\qquad \\|A\\|_âˆ.\n",
    "$$\n",
    "\n",
    "Note an equivalent definition of the induced norm:\n",
    "$$\n",
    "\\|A\\|_{X â†’ Y} = \\sup_{ğ± âˆˆ â„^n, ğ± â‰  0} {\\|A ğ±\\|_Y \\over \\| ğ±\\|_X}\n",
    "$$\n",
    "This follows since we can scale $ğ±$ by its norm so that it has unit norm, that is,\n",
    "${ğ±} \\over \\|ğ±\\|_X$ has unit norm.\n",
    "\n",
    "**Lemma (matrix norms are norms)** Induced matrix norms are norms, that is for $\\| â‹… \\| = \\| â‹… \\|_{X â†’ Y}$ we have:\n",
    "1. Triangle inequality: $\\| A + B \\| â‰¤  \\|A\\| + \\|B\\|$\n",
    "1. Homogeneneity: $\\|c A \\| = |c| \\|A\\|$\n",
    "3. Positive-definiteness: $\\|A\\| =0 \\Rightarrow A = 0$\n",
    "In addition, they satisfy the following additional propertie:\n",
    "1. $\\|A ğ± \\|_Y â‰¤ \\|A\\|_{X â†’ Y} \\|ğ± \\|_X$\n",
    "2. Multiplicative inequality: $\\| AB\\|_{X â†’ Z} â‰¤ \\|A \\|_{Y â†’ Z} \\|B\\|_{X â†’  Y}$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "First we show the _triangle inequality_:\n",
    "$$\n",
    "\\|A + B \\| â‰¤ \\sup_{ğ¯ : \\|ğ¯\\|_X=1} (\\|A ğ¯\\|_Y + \\|B ğ¯\\|_Y) â‰¤ \\| A \\| + \\|B \\|.\n",
    "$$\n",
    "Homogeneity is also immediate. Positive-definiteness follows from the fact that if\n",
    "$\\|A\\| = 0$ then $A ğ±  = 0$ for all $ğ± âˆˆ â„^n$.\n",
    "The property $\\|A ğ± \\|_Y â‰¤ \\|A\\|_{X â†’ Y} \\|ğ± \\|_X$ follows from the definition. Finally, \n",
    "Finally, the multiplicative inequality follows from\n",
    "$$\n",
    "\\|A B\\| = \\sup_{ğ¯ : \\|ğ¯\\|_X=1} \\|A B ğ¯ |_Z â‰¤Â \\sup_{ğ¯ : \\|ğ¯\\|_X=1} \\|A\\|_{Y â†’ Z} \\| B ğ¯ | = \\|A \\|_{Y â†’ Z} \\|B\\|_{X â†’  Y}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "âˆ\n",
    "\n",
    "We have some simple examples of induced norms:\n",
    "\n",
    "**Example ($1$-norm)** We claim \n",
    "$$\n",
    "\\|A \\|_1 = \\max_j \\|ğš_j\\|_1\n",
    "$$\n",
    "that is, the maximum $1$-norm of the columns. To see this use the triangle inequality to\n",
    "find for $\\|ğ±\\|_1 = 1$\n",
    "$$\n",
    "\\| A ğ± \\|_1 â‰¤Â âˆ‘_{j = 1}^n |x_j| \\| ğš_j\\|_1 â‰¤Â \\max_j \\| ğš_j\\| âˆ‘_{j = 1}^n |x_j| = \\max_j \\| ğš_j\\|_1.\n",
    "$$\n",
    "But the bound is also attained since if $j$ is the column that maximises the norms then\n",
    "$$\n",
    "\\|A ğ_j \\|_1 = \\|ğš_j\\|_1 =  \\max_j \\| ğš_j\\|_1.\n",
    "$$\n",
    "\n",
    "In the problem sheet we see that\n",
    "$$\n",
    "\\|A\\|_âˆ = \\max_k \\|A[k,:]\\|_1\n",
    "$$\n",
    "that is, the maximum $1$-norm of the rows.\n",
    "\n",
    "\n",
    "\n",
    "An example that is not simple is $\\|A \\|_2$, but we do have two simple cases:\n",
    "\n",
    "**Proposition (diagonal/orthogonal 2-norms)** If $Î›$ is diagonal with entries $Î»_k$ then\n",
    "$\\|Î›\\|_2 = \\max_k |Î»_k|.$. If $Q$ is orthogonal then $\\|Q\\| = 1$.\n",
    "\n",
    "\n",
    "## 3. Singular value decomposition\n",
    "\n",
    "To define the induced $2$-norm we need to consider the following:\n",
    "\n",
    "**Definition (singular value decomposition)** For $A âˆˆ â„^{m Ã— n}$ with rank $r > 0$, \n",
    "the _reduced singular value decomposition (SVD)_ is\n",
    "$$\n",
    "A = U Î£ V^âŠ¤\n",
    "$$\n",
    "where $U âˆˆ â„^{m Ã— r}$ and $V âˆˆ  â„^{r Ã— n}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose\n",
    "diagonal entries, which which we call _singular values_, are all non-negative and decreasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_{\\min(m,n)} â‰¥ 0$.\n",
    "The _full singular value decomposition (SVD)_ is\n",
    "$$\n",
    "A = U Î£ V^âŠ¤\n",
    "$$\n",
    "where $U âˆˆ â„^{m Ã— m}$ and $V âˆˆ  â„^{n Ã— n}$ are orthogonal matrices and $Î£ âˆˆ â„^{m Ã— n}$.\n",
    "\n",
    "For symmetric matrices, the SVD is related to the eigenvalue decomposition.\n",
    "Recall that a symmetric matrix has real eigenvalues and orthogonal eigenvectors:\n",
    "$$\n",
    "A = Q Î› Q^âŠ¤ = \\underbrace{Q}_U \\underbrace{|Î›|}_Î£ \\underbrace{(\\hbox{sign}(Î›) Q)^âŠ¤}_{V^âŠ¤}\n",
    "$$\n",
    "For non-symmetric matrices we relate it to the eigenvalues of the _Gram matrix_ $A^âŠ¤A$ and $AA^âŠ¤$ via:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^âŠ¤ A = V Î£^2 V^âŠ¤ \\\\\n",
    "A A^âŠ¤ = U Î£^2 U^âŠ¤\n",
    "\\end{align*}\n",
    "$$\n",
    "That is, $Ïƒ_k^2$ are non-zero eigenvalues of $A^âŠ¤ A$ and $A A^âŠ¤$. \n",
    "We now establish some properties of a Gram matrix:\n",
    "\n",
    "**Proposition (Gram matrix kernel)** The kernel of $A$ is the also the kernel of $A^âŠ¤ A$. \n",
    "\n",
    "**Proof**\n",
    "If $A^âŠ¤ A ğ± = 0$ then we have\n",
    "$$\n",
    "0 = ğ± A^âŠ¤ A ğ± = \\| A ğ± \\|^2\n",
    "$$\n",
    "which means $A ğ± = 0$ and $ğ± âˆˆ \\hbox{ker}(A)$.\n",
    "âˆ\n",
    "\n",
    "\n",
    "\n",
    "This connection allows us to prove existence:\n",
    "\n",
    "**Theorem (SVD existence)** Every $A âˆˆ â„^{mÃ— n}$ has an SVD.\n",
    "\n",
    "**Proof**\n",
    "\n",
    "First note that $A^âŠ¤ A = Q Î› Q^âŠ¤$ has non-negative eigenvalues $Î»_k$ as,\n",
    "for the corresponding (orthonormal) eigenvector $ğª_k$,\n",
    "$$\n",
    "Î»_k = Î»_k ğª_k^âŠ¤ ğª_k = ğª_k^âŠ¤ A^âŠ¤ A ğª_k = \\| A ğª_k \\| â‰¥ 0.\n",
    "$$\n",
    "Further, the kernel of $A^âŠ¤ A$ is the same as $A$.\n",
    "Assume the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$\n",
    "are an enumeration of the non-zero eigenvalues and\n",
    "$$\n",
    "V := \\begin{bmatrix} ğª_1 | â‹¯ | ğª_r \\end{bmatrix}\n",
    "$$\n",
    "the corresponding (orthonormal) eigenvectors, with\n",
    "$$\n",
    "K = \\begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \\end{bmatrix}\n",
    "$$\n",
    "the corresponding kernel. \n",
    "Define\n",
    "$$\n",
    "Î£ :=  \\begin{bmatrix} \\sqrt{Î»_1} \\\\ & â‹± \\\\ && \\sqrt{Î»_r} \\end{bmatrix}\n",
    "$$\n",
    "Now define\n",
    "$$\n",
    "U := AV Î£^{-1}\n",
    "$$\n",
    "which is orthogonal since $A^âŠ¤ A V = Î£^2 V$:\n",
    "$$\n",
    "U^âŠ¤ U = Î£^{-1} V^âŠ¤ A^âŠ¤ A V Î£^{-1} = I.\n",
    "$$\n",
    "Thus we have\n",
    "$$\n",
    "U Î£ V^âŠ¤ = A V V^âŠ¤ = A \\underbrace{\\begin{bmatrix} V | K \\end{bmatrix}}_Q\\underbrace{\\begin{bmatrix} V^âŠ¤ \\\\ K^âŠ¤ \\end{bmatrix}}_{Q^âŠ¤}\n",
    "$$\n",
    "where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.\n",
    "\n",
    "âˆ\n",
    "\n",
    "Singular values tell us the 2-norm:\n",
    "\n",
    "**Corollary (singular values and norm)**\n",
    "$$\n",
    "\\|A \\|_2 = Ïƒ_1\n",
    "$$\n",
    "and if $A$ is invertible, then\n",
    "$$\n",
    "\\|A^{-1} \\|_2 = Ïƒ_r^{-1}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "First we establish the upper-bound:\n",
    "$$\n",
    "\\|A \\|_2 â‰¤Â  \\|U \\|_2 \\| Î£ \\|_2 \\| V^âŠ¤\\|_2 = \\| Î£ \\|_2  = Ïƒ_1\n",
    "$$\n",
    "This is attained using the first right singular vector:\n",
    "$$\n",
    "\\|A ğ¯_1\\|_2 = \\|Î£ V^âŠ¤ ğ¯_1\\|_2 = \\|Î£  ğ_1\\|_2 = Ïƒ_1\n",
    "$$\n",
    "The inverse result follows since the inverse has SVD\n",
    "$$\n",
    "A^{-1} = V Î£^{-1} U^âŠ¤ = V (W Î£^{-1} W) U^âŠ¤\n",
    "$$\n",
    "where\n",
    "$$\n",
    "W := P_Ïƒ = \\begin{bmatrix} && 1 \\\\ & â‹° \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "is the permutation that reverses the entries, that is, $Ïƒ$ has Cauchy notation\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & â‹¯ & n \\\\\n",
    "n & n-1 & â‹¯ & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "âˆ\n",
    "\n",
    "We will not discuss in this module computation of singular value decompositions or eigenvalues:\n",
    "they involve iterative algorithms (actually built on a sequence of QR decompositions).\n",
    "\n",
    "One of the main usages for SVDs is low-rank approximation:\n",
    "\n",
    "**Theorem (best low rank approximation)** The  matrix\n",
    "$$\n",
    "A_k := \\begin{bmatrix} ğ®_1 | â‹¯ | ğ®_k \\end{bmatrix} \\begin{bmatrix}\n",
    "Ïƒ_1 \\\\\n",
    "& â‹± \\\\\n",
    "&& Ïƒ_k\\end{bmatrix} \\begin{bmatrix} ğ¯_1 | â‹¯ | ğ¯_k \\end{bmatrix}^âŠ¤\n",
    "$$ \n",
    "is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have \n",
    "$$\\|A - A_k\\|_2 â‰¤ \\|A -B \\|_2.$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\|A - A_k\\|_2 = \\|U \\begin{bmatrix} 0  \\cr &\\ddots \\cr && 0 \\cr &&& Ïƒ_{k+1} \\cr &&&& \\ddots \\cr &&&&& Ïƒ_n \\cr &&&&&\\vdots \\cr &&&&&0\\end{bmatrix} = Ïƒ_{r+1}\n",
    "$$\n",
    "\n",
    "\n",
    "Suppose a rank-$k$ matrix $B$ has \n",
    "$$\n",
    "\\|A-B\\|_2  < \\|A-A_k\\|_2 = Ïƒ_{k+1}.\n",
    "$$\n",
    "For all $ğ° \\in \\ker(B)$ we have \n",
    "$$\n",
    "\\|A ğ°\\|_2 = \\|(A-B) ğ°\\|_2 â‰¤ \\|A-B\\|\\|ğ°\\|_2  < Ïƒ_{k+1} \\|ğ°\\|_2\n",
    "$$\n",
    "\n",
    "But for all $ğ® \\in {\\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$, that is, $ğ® = V[:,1:r+1]ğœ$ for some $ğœ \\in â„^{k+1}$  we have \n",
    "$$\n",
    "\\|A ğ®\\|_2^2 = \\|U Î£_k ğœ\\|_2^2 = \\|Î£_k ğœ\\|_2^2 =\n",
    "\\sum_{j=1}^{k+1} (Ïƒ_j c_j)^2 â‰¥ Ïƒ_{k+1}^2 \\|c\\|^2,\n",
    "$$\n",
    "i.e., $\\|A ğ®\\|_2 â‰¥ Ïƒ_{k+1} \\|c\\|$.  Thus $ğ°$ cannot be in this span.\n",
    "\n",
    "\n",
    "The dimension of the span of $\\ker(B)$ is at least $n-k$, but the dimension of ${\\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$ is at least $k+1$.\n",
    "Since these two spaces cannot intersect we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  âˆ\n",
    "\n",
    "\n",
    "In the problem sheet we explore the usage of low rank approximation to smooth functions.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Condition numbers\n",
    "\n",
    "We have seen that floating point arithmetic induces errors in computations, and that we can typically\n",
    "bound the absolute errors to be proportional to $C Ïµ_{\\rm m}$. We want a way to bound the\n",
    "effect of more complicated calculations like computing $A ğ±$ or $A^{-1} ğ²$ without having to deal with\n",
    "the exact nature of floating point arithmetic. Here we consider only matrix-multiplication but will make a remark\n",
    "about matrix inversion.\n",
    "\n",
    "To justify what follows, we first observe that errors in implementing matrix-vector multiplication\n",
    "can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A x`\n",
    "(implemented with floating point) is precisely $A + Î´A$ where $Î´A$ has small norm, relative to $A$.\n",
    "This is known as _backward error analysis_.\n",
    "\n",
    "\n",
    "\n",
    "To discuss floating point errors we need to be precise which order the operations happened.\n",
    "We will use the definition `mul(A,x)`, which denote ${\\rm mul}(A, ğ±)$. (Note that `mul_rows` actually\n",
    "does the _exact_ same operations, just in a different order.) Note that each entry of the result is in fact a dot-product\n",
    "of the corresponding rows so we first consider the error in the dot product  `dot(ğ±,ğ²)` as implemented in floating-point, \n",
    "which we denote ${\\rm dot}(A,x)$.\n",
    "\n",
    "We first need a helper proposition:\n",
    "\n",
    "**Proposition** If $|Ïµ_i| â‰¤ Ïµ$ and $n Ïµ < 1$, then\n",
    "$$\n",
    "\\prod_{k=1}^n (1+Ïµ_i) = 1+Î¸_n\n",
    "$$\n",
    "for some constant $Î¸_n$ satisfying $|Î¸_n| â‰¤ {n Ïµ \\over 1-nÏµ}$.\n",
    "\n",
    "The proof is left as an exercise (Hint: use induction).\n",
    "\n",
    "**Lemma (dot product backward error)**\n",
    "For any norm,\n",
    "$$\n",
    "{\\rm dot}(ğ±, ğ²) = (ğ± + Î´ğ±)^âŠ¤ ğ²\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|Î´ğ±\\| â‰¤Â  {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}} \\|ğ± \\|\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Note that \n",
    "$$\n",
    "{\\rm dot}(ğ±, ğ²) = \\{ [(x_1 âŠ— y_1) âŠ• (x_2 âŠ— y_2)] âŠ•(x_3âŠ— y_3)] âŠ•â‹¯\\}âŠ•(x_n âŠ— y_n) \\\\\n",
    "  & = \\{ [(x_1 y_1)(1+Î´_1) + (x_2 y_2)(1+Î´_2)](1+Î³_1) +x_3 y_3(1+Î´_3)](1+Î³_2) + â‹¯ \\}(1+Î³_{n-1})+x_n y_n(1+Î´_n) \\\\\n",
    "  & = x_1 y_1 (1+Î´_1) \\prod_{k=1}^{n-1}(1+Î³_k) + x_2 y_2 (1+Î´_2) \\prod_{k=1}^{n-1}(1+Î³_k) + x_3 y_3 (1+Î´_3) \\prod_{k=2}^{n-1}(1+Î³_k)  + â‹¯  +x_{n-1} y_{n-1}(1+Î´_{n-1})(1+Î³_{n-1}) + x_n y_n(1+Î´_n) \\\\\n",
    "  &= x_1y_1(1+Î¸_n^1) + x_2y_2(1+Î¸_n^2)+ x_3y_3(1+Î¸_{n-1}) + \\cdots + x_n y_n (1+Î¸_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "where we denote the errors from multiplication as $Î´_k$ and those from addition by $Î³_k$ and the previous proposition tells us\n",
    "$$\n",
    "|Î¸_n^1|, |Î¸_n^2| â‰¤Â {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "|Î¸_k| â‰¤ {k Ïµ_{\\rm m} \\over 1-kÏµ_{\\rm m}} â‰¤Â {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}}.\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "Î´ğ± =  \\begin{pmatrix} x_1 Î¸_n^1 \\cr x_2 Î¸_n^2 \\cr x_3 Î¸_{n-1} \\cr \\vdots \\cr x_n Î¸_1\\end{pmatrix}\n",
    "$$\n",
    "and the theorem follows:\n",
    "$$\n",
    "\\| Î´ğ± \\| â‰¤Â {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}} \\| ğ± \\|\n",
    "$$\n",
    "\n",
    "âˆ\n",
    "\n",
    "**Theorem (matrix-vector backward error)**\n",
    "For any norm,\n",
    "$$\n",
    "{\\rm mul}(A, ğ±) = (A + Î´A)^âŠ¤ ğ±\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|Î´A\\| â‰¤Â  {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}} \\|A \\|\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Follows from applying the previous lemma to each row.\n",
    "\n",
    "âˆ\n",
    "\n",
    "\n",
    "So now we get to a mathematical question independent of floating point: can we bound the _relative error_ in approximating\n",
    "$$\n",
    "A ğ± â‰ˆ (A + Î´A) ğ±\n",
    "$$\n",
    "if we know a bound on $\\|Î´A\\|$?\n",
    "It turns out we can in turns of the _condition number_ of the matrix:\n",
    "\n",
    "**Definition (condition number)**\n",
    "For a square matrix $A$, the _condition number_ is\n",
    "$$\n",
    "Îº(A) := \\| A \\| \\| A^{-1} \\| = {Ïƒ_1 \\over Ïƒ_n}\n",
    "$$\n",
    "\n",
    "\n",
    "**Theorem (relative-error for matrix-vector)**\n",
    "The _worst-case_ relative error in $A ğ± â‰ˆ (A + Î´A) ğ±$ is\n",
    "$$\n",
    "{\\| Î´A ğ± \\| \\over \\| A ğ± \\| } â‰¤ Îº(A) {\\|Î´A\\| \\over \\|A \\|}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "We can assume $A$ is invertible (as otherwise $Îº(A) = âˆ$). Denote $ğ² = A ğ±$ and we have\n",
    "$$\n",
    "{\\|ğ± \\| \\over \\| A ğ± \\|} = {\\|A^{-1} ğ² \\| \\over \\|ğ² \\|} â‰¤Â \\| A^{-1}\\|\n",
    "$$\n",
    "Thus we have:\n",
    "$$\n",
    "{\\| Î´A ğ± \\| \\over \\| A ğ± \\| } â‰¤ \\| Î´A\\| \\|A^{-1}\\| â‰¤Â Îº(A) {\\|Î´A\\| \\over \\|A \\|}\n",
    "$$\n",
    "\n",
    "âˆ\n",
    "\n",
    "\n",
    "Thus for floating point arithmetic we know the error is bounded by $Îº(A) {n Ïµ_{\\rm m} \\over 1-nÏµ_{\\rm m}}$.\n",
    "\n",
    "If one uses QR to solve $A ğ± = ğ²$ the condition number also gives a meaningful bound on the error. \n",
    "As we have already noted, there are some matrices where PLU decompositions introduce large errors, so\n",
    "in that case well-conditioning is not a guarantee (but it still usually works)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
