{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompositions and least squares\n",
    "\n",
    "\n",
    "In this lecture, we look at several factorizations of a matrix. For a square or rectangular matrix $A âˆˆ â„^{m Ã— n}$ with $m â‰¥ n$, we consider:\n",
    "1. The _QR decomposition_\n",
    "$$\n",
    "A = \\underbrace{Q}_{m Ã— m} \\underbrace{R}_{m Ã— n} = \\begin{bmatrix} ğª_1 | \\cdots | ğª_m \\end{bmatrix} \\begin{bmatrix} Ã— & \\cdots & Ã— \\\\ & \\ddots & \\vdots \\\\ && Ã— \\\\ &&0 \\\\ &&\\vdots \\\\ && 0 \\end{bmatrix} \n",
    "$$\n",
    "where $Q$ is orthogonal ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) and $R$ is _right triangular_.\n",
    "\n",
    "2. The _reduced QR decomposition_\n",
    "$$\n",
    "A = \\underbrace{QÌ‚}_{m Ã— n} \\underbrace{RÌ‚}_{m Ã— m} = \\begin{bmatrix} ğª_1 | \\cdots | ğª_n \\end{bmatrix} \\begin{bmatrix} Ã— & \\cdots & Ã— \\\\ & \\ddots & \\vdots \\\\ && Ã—  \\end{bmatrix} \n",
    "$$\n",
    "where $Q$ has orthogonal columns ($Q^âŠ¤Q = I$, $ğª_j âˆˆ â„^m$) \n",
    "\n",
    "For a square matrix we consider the _PLU decomposition_:\n",
    "$$\n",
    "A = P^âŠ¤ LU\n",
    "$$\n",
    "where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular.\n",
    "\n",
    "Finally, for a square, _symmetric positive definite_ ($ğ±^âŠ¤ A ğ± > 0$ for all $ğ± âˆˆ â„^n$) \n",
    "matrix we consider the _Cholesky decomposition_:\n",
    "$$\n",
    "A = L L^âŠ¤\n",
    "$$\n",
    "\n",
    "The importance of these decomposition for square matrices is that their component pieces are easy to invert on a computer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A = P^âŠ¤ LU &\\Rightarrow\\qquad A^{-1}ğ› = U^{-1} L^{-1} P ğ› \\\\\n",
    "A = QR &\\Rightarrow\\qquad A^{-1}ğ› = R^{-1} Q^\\top ğ› \\\\\n",
    "A = L L^âŠ¤ &\\Rightarrow\\qquad A^{-1}ğ› = L^{-âŠ¤} L^{-1} ğ›\n",
    "\\end{align*}\n",
    "$$\n",
    "and we saw last lecture that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$.\n",
    "For rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find\n",
    "$ğ±$ that minimizes the 2-norm\n",
    "$$\n",
    "\\| A ğ± - ğ› \\|.\n",
    "$$\n",
    "\n",
    "In this lecture we discuss the followng:\n",
    "\n",
    "1. QR,  Reduced QR, and least squares: We discuss the QR decomposition and its usage in solving least squares problems.\n",
    "We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt, and the Full QR decomposition using \n",
    "Householder reflections.\n",
    "2. PLU decomposition: we discuss how the LU decomposition can be computed using Gaussian elimination, and the computation of\n",
    "the PLU decomposition via Gaussian elimination with pivoting.\n",
    "3. Cholesky decomposition: we introduce symmetric positive definite matrices and show that their LU decomposition can be re-interpreted\n",
    "as a Cholesky decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:23.015311Z",
     "iopub.status.busy": "2022-01-24T16:13:22.613374Z",
     "iopub.status.idle": "2022-01-24T16:13:27.658972Z",
     "shell.execute_reply": "2022-01-24T16:13:27.658345Z"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. QR, Reduced QR, and least squares\n",
    "\n",
    "Here we consider rectangular matrices with more rows than columns.\n",
    "A QR decomposition decomposes a matrix into an orthogonal matrix $Q$ times a right triangular matrix $R$. \n",
    "Note the QR decomposition contains within it the reduced QR decomposition:\n",
    "$$\n",
    "A = QR = \\begin{bmatrix} QÌ‚ | ğª_{n+1} | â‹¯ | ğª_m \\end{bmatrix} \\begin{bmatrix} RÌ‚ \\\\  ğŸ_{m-n Ã— n} \\end{bmatrix} = QÌ‚ RÌ‚.\n",
    "$$\n",
    "\n",
    "### Relationship with least squares\n",
    "\n",
    "We can use it to solve a least squares problem using the norm-preserving property (see PS3) of orthogonal matrices:\n",
    "$$\n",
    "\\| A ğ± - ğ› \\| = \\| Q R ğ± - ğ› \\| = \\| R ğ± - Q^âŠ¤ ğ› \\| = \\left \\| \n",
    "\\begin{bmatrix} RÌ‚ \\\\ ğŸ_{m-n Ã— n} \\end{bmatrix} ğ± - \\begin{bmatrix} QÌ‚^âŠ¤ \\\\ ğª_{n+1}^âŠ¤ \\\\ \\vdots \\\\ ğª_m^âŠ¤ \\end{bmatrix}     ğ› \\right \\|\n",
    "$$\n",
    "Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to\n",
    "drop them and minimise:\n",
    "$$\n",
    "\\| RÌ‚ ğ± - QÌ‚^âŠ¤ ğ› \\|\n",
    "$$\n",
    "This norm is minimisable if it is attained. Provided the column rank of $A$ is full, $RÌ‚$ will be invertible (Exercise: why is this?).\n",
    "Thus we have the solution\n",
    "$$\n",
    "ğ± = RÌ‚^{-1} QÌ‚^âŠ¤ ğ›\n",
    "$$\n",
    "\n",
    "**Example (quadratic fit)** Suppose we want to find noisy data by a quadratic\n",
    "$$\n",
    "p(x) = a + bx + cx^2\n",
    "$$\n",
    "That is, we want to choose $a,b,c$ at data samples $x_1, \\ldots, x_m$ so that the following is true:\n",
    "$$\n",
    "a + b x_k + c x_k^2 â‰ˆ f_k\n",
    "$$\n",
    "where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm\n",
    "$$\n",
    "\\left\\| \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_m & x_m^2 \\end{bmatrix}\n",
    "\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} - \\begin{bmatrix} f_1 \\\\ \\vdots \\\\ f_m \\end{bmatrix} \\right \\|\n",
    "$$\n",
    "We can solve this using the QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:28.602852Z",
     "iopub.status.busy": "2022-01-24T16:13:27.660419Z",
     "iopub.status.idle": "2022-01-24T16:13:29.972878Z",
     "shell.execute_reply": "2022-01-24T16:13:29.972407Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: number of rows of each array must match (got (3, 100, 100))",
     "output_type": "error",
     "traceback": [
      "ArgumentError: number of rows of each array must match (got (3, 100, 100))",
      "",
      "Stacktrace:",
      " [1] _typed_hcat(#unused#::Type{Float64}, A::Tuple{Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Vector{Float64}})",
      "   @ Base ./abstractarray.jl:1570",
      " [2] typed_hcat",
      "   @ ./abstractarray.jl:1557 [inlined]",
      " [3] hcat(::Vector{Float64}, ::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, ::Vector{Float64})",
      "   @ Base ./abstractarray.jl:1560",
      " [4] top-level scope",
      "   @ In[2]:6",
      " [5] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "m,n = 100,3\n",
    "\n",
    "x = range(0,1; length=m) # 100 points\n",
    "f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic\n",
    "\n",
    "A = [ones(n) x x.^2] # 100 x 3 matrix\n",
    "Q,RÌ‚ = qr(A)\n",
    "QÌ‚ = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns\n",
    "\n",
    "a,b,c = RÌ‚ \\ QÌ‚'f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:29.974899Z",
     "iopub.status.busy": "2022-01-24T16:13:29.974395Z",
     "iopub.status.idle": "2022-01-24T16:13:32.744645Z",
     "shell.execute_reply": "2022-01-24T16:13:32.744220Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: b not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: b not defined",
      "",
      "Stacktrace:",
      " [1] (::var\"#1#2\")(x::Float64)",
      "   @ Main ./In[3]:1",
      " [2] _broadcast_getindex_evalf",
      "   @ ./broadcast.jl:670 [inlined]",
      " [3] _broadcast_getindex",
      "   @ ./broadcast.jl:643 [inlined]",
      " [4] getindex",
      "   @ ./broadcast.jl:597 [inlined]",
      " [5] copy",
      "   @ ./broadcast.jl:899 [inlined]",
      " [6] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, var\"#1#2\", Tuple{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}})",
      "   @ Base.Broadcast ./broadcast.jl:860",
      " [7] top-level scope",
      "   @ In[3]:4",
      " [8] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "p = x -> a + b*x + c*x^2\n",
    "\n",
    "scatter(x, f; label=\"samples\", legend=:bottomright)\n",
    "plot!(x, p.(x); label=\"quadratic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `\\` with a rectangular system does the least squares by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:32.746619Z",
     "iopub.status.busy": "2022-01-24T16:13:32.746132Z",
     "iopub.status.idle": "2022-01-24T16:13:32.748712Z",
     "shell.execute_reply": "2022-01-24T16:13:32.748242Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: A not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: A not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "A \\ f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramâ€“Schmidt and reduced QR\n",
    "\n",
    "\n",
    "How do we compute the QR decomposition? We begin with a method\n",
    "you may have seen before in another guise. Write\n",
    "$$\n",
    "A = \\begin{bmatrix} ğš_1 | \\dots | ğš_n \\end{bmatrix}\n",
    "$$\n",
    "where $ğš_k \\in  â„^m$.\n",
    "Note that the column span of the first $j$ columns of $A$\n",
    "will be the same as the first $j$ columns of $QÌ‚$:\n",
    "$$\n",
    "\\span{ğš_1,\\ldots,ğš_j} = \\span{ğª_1,\\ldots,ğª_j}\n",
    "$$\n",
    "In other words: the columns of $QÌ‚$ are an orthogonal basis\n",
    "of the column span of $A$.\n",
    "To see this: note that since `RÌ‚` is triangular we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:32.750592Z",
     "iopub.status.busy": "2022-01-24T16:13:32.749995Z",
     "iopub.status.idle": "2022-01-24T16:13:32.752834Z",
     "shell.execute_reply": "2022-01-24T16:13:32.752368Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: j not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: j not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "A[:,1:j] = QÌ‚[:,1:j]*RÌ‚[1:j,1:j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $j$, and the multiplication is taking linear combinations of\n",
    "`QÌ‚[:,1:j]`. \n",
    " \n",
    "It is possible to find an orthogonal basis using the _Gramâ€“Schmidt algorithm_,\n",
    "We construct it via induction:\n",
    "assume that\n",
    "$$\n",
    "\\span{ğš_1,\\ldots,ğš_{j-1}} = \\span{ğª_1,\\ldots,ğª_{j-1}}\n",
    "$$\n",
    "where $ğª_1,\\ldots,ğª_{j-1}$ are orthogonal:\n",
    "$$\n",
    "ğª_k^\\top ğª_â„“ = Î´_{kâ„“} = \\begin{cases} 1 & k == â„“ \\\\\n",
    "                                            0 & \\hbox{otherwise} \\end{cases}.\n",
    "$$\n",
    "for $k,â„“ < 1$.\n",
    "Define\n",
    "$$\n",
    "ğ¯_j := ğš_j - \\sum_{k=1}^{j-1} \\underbrace{ğª_k^\\top ğš_j}_{v_{kj}} ğª_k\n",
    "$$\n",
    "so that for $k < j$\n",
    "$$\n",
    "ğª_k^\\top ğ¯_j = ğª_k^\\top ğš_j - \\sum_{k=1}^{j-1} \\underbrace{ğª_k^\\top ğš_j}_{v_{kj}} ğª_k^\\top ğª_k = 0.\n",
    "$$\n",
    "Then we define\n",
    "$$\n",
    "ğª_j := {ğ¯_j \\over \\|ğ¯_j\\|}.\n",
    "$$\n",
    "which sastisfies the same properties as the assumption.\n",
    "\n",
    "We now reinterpret this construction as a reduced QR decomposition.\n",
    "Define\n",
    "$$\n",
    "r_{jj} := {1 \\over \\|ğ¯_j\\|}, \\qquad r_{kj} := {v_{kj} \\over \\|ğ¯_j\\|}\n",
    "$$\n",
    "Then rearrange the definition we have\n",
    "$$\n",
    "ğš_j = \\begin{bmatrix} ğª_1|\\cdots|ğª_j \\end{bmatrix} \\begin{bmatrix} r_{1j} \\\\ \\vdots \\\\ r_{jj} \\end{bmatrix}\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "\\begin{bmatrix} ğš_1|\\cdots|ğš_j \\end{bmatrix} r_{11} & \\cdots & r_{1j} \\\\ & \\ddots & \\vdots \\\\ && r_{jj} \\end{bmatrix}\n",
    "$$\n",
    "That is, we are computing the reduced QR decomposition column-by-column. Running this algorithm to $j = n$ completes the decomposition.\n",
    "\n",
    "Unfortunately, the Gramâ€“Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point\n",
    "accumulate in a disasterous way. This will be explored in the problem sheet.\n",
    "\n",
    "\n",
    "### Householder reflections and QR\n",
    "\n",
    "As an alternative, we will consider using Householder reflections to introduce zeros.\n",
    "Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices\n",
    "to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ \n",
    "(using orthogonal matrices to triangularise).\n",
    "\n",
    "Consider multiplication by the Householder reflection corresponding to the first column,\n",
    "that is, for\n",
    "$$\n",
    "Q_1 := Q_{ğš_1}^{Â±,\\rm H},\n",
    "$$\n",
    "consider\n",
    "$$\n",
    "Q_1 A = \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ \n",
    "& ğš_2^1 & \\cdots & ğš_n^1   \\end{bmatrix}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "r_{1j} :=  (Q_1 ğš_j)[1] \\qquad \\hbox{and} \\qquad ğš_j^1 := (Q_1 ğš_j)[2:m],\n",
    "$$\n",
    "noting $r_{11} = Â±\\|ğš_1\\|$ and all entries of $ğš_1^1$ are zero (thus not included).\n",
    "That is, we have made the first column triangular.\n",
    "\n",
    "But now consider\n",
    "$$\n",
    "Q_2 := \\begin{bmatrix} 1  \\\\ & Q_{ğš_2^1}^{Â±,\\rm H} \\end{bmatrix}\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "Q_2 Q_1A  \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ \n",
    "    & r_{22} & r_{23} \\cdots & r_{2n} \\\\\n",
    "&& ğš_3^2 & \\cdots & ğš_n^2   \\end{bmatrix}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "r_{2j} :=  (Q_2 ğš_j^1)[1] \\qquad \\hbox{and} \\qquad ğš_j^2 := (Q_2 ğš_j^1)[2:m-1]\n",
    "$$\n",
    "Thus the first two columns are triangular. \n",
    "\n",
    "The inductive construction is thus clear. If we define $ğš_j^0 := ğš_j$ we\n",
    "have the construction\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_j &:= \\begin{bmatrix} I_{j-1 \\times j-1}  \\\\ & Q_{ğš_j^j}^{Â±,\\rm H} \\end{bmatrix} \\\\\n",
    "ğš_j^k &:= (Q_k ğš_j^{k-1})[2:m-k+1] \\\\\n",
    "r_{kj} &:= (Q_k ğš_j^{k-1})[1]\n",
    "\\end{align*}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "Q_n \\cdots Q_1 A = \\underbrace{\\begin{bmatrix} \n",
    "r_{11} & \\cdots & r_{1n} \\\\ & \\ddots & \\vdots\\\\\n",
    "                                        && r_{nn} \\\\&& 0 \\\\ && \\vdots \\\\ && 0 \\end{bmatrix}}_R\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "A = \\underbrace{Q_n \\cdots Q_1}_Q R.\n",
    "$$\n",
    "\n",
    "The implementation is cleaner. We do a naive implementation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T16:13:32.754610Z",
     "iopub.status.busy": "2022-01-24T16:13:32.754109Z",
     "iopub.status.idle": "2022-01-24T16:13:34.533413Z",
     "shell.execute_reply": "2022-01-24T16:13:34.533024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function householderreflection(x)\n",
    "    y = copy(x)\n",
    "    y[1] += sign(x[1])norm(x)\n",
    "    w = y/norm(y)\n",
    "    I - 2*w*w'\n",
    "end\n",
    "\n",
    "m,n = 7,5\n",
    "A = randn(m, n)\n",
    "R = copy(A)\n",
    "Q = Matrix(1.0I, m, m)\n",
    "for j = 1:n\n",
    "    Qâ±¼ = householderreflection(R[j:end,j])\n",
    "    R[j:end,:] = Qâ±¼*R[j:end,:]\n",
    "    Q[:,j:end] = Q[:,j:end]*Qâ±¼\n",
    "end\n",
    "Q*R â‰ˆ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note because we are forming a full matrix representation of each Householder\n",
    "reflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet\n",
    "will consider a better implementation that takes $O(n^3)$ operations.\n",
    "\n",
    "\n",
    "**Example** We will now do an example by hand. Consider the $4 \\times 3$ matrix\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "1 & 2 & 2 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1.\n",
    "$$\n",
    "For the first column we have\n",
    "$$\n",
    "Q_1 = I - {1 \\over 6} \\begin{bmatrix}3 \\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix} \\begin{bmatrix}3 & 1& 1& 1 \\end{bmatrix} = {1 \\over 6} \\begin{bmatrix} -3 & -3 & -3 & -3\\\\ -3 & 5 &-1 & -1 \\\\ -3 & -1 & 5 & -1 &-3 &-1 & -1 5 \\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Q_1 A = {1 \\over 6} \\begin{bmatrix} -12 & -15 &  -18 \\\\  & -3 & 2 \\\\ &-3 &-4\\\\ -3 & -4 \\end{bmatrix}\n",
    "$$\n",
    "Then \n",
    "$$\n",
    "Q_2 = \\begin{bmatrix} 1 \\\\ & I - 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "## 2. PLU Decomposition\n",
    "\n",
    "Just as Gramâ€“Schmidt can be reinterpreted as a reduced QR decomposition,\n",
    "Gaussian elimination with pivoting can be interpreted as a PLU decomposition.\n",
    "\n",
    "\n",
    "### LU Decomposition\n",
    "\n",
    "Before discussing pivoting, consider standard Gaussian elimation where one row-reduces\n",
    "to introduce zeros column-by-column. We will mimick the computation of the QR decomposition\n",
    "to view this as a _triangular triangularisation_.\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "L_ğ¯ = \\begin{bmatrix} 1 \\\\ -{v_2 \\over v_1} & 1 \\\\ \\vdots &&\\ddots \\\\\n",
    "                -{v_n \\over v_1}  &&& 1\n",
    "                \\end{bmatrix} = I -\\left[{ğ¯ \\over v_1}  - ğ_1 \\right] ğ_1^\\top\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "L_ğ¯ ğ¯ = ğ¯ - (ğ¯ - v_1 ğ_1) = v_1 ğ_1.\n",
    "$$\n",
    "Note that this is immediately invertible:\n",
    "$$\n",
    "L_ğ¯^{-1} = \\begin{bmatrix} 1 \\\\ {v_2 \\over v_1} & 1 \\\\ \\vdots &&\\ddots \\\\\n",
    "                {v_n \\over v_1}  &&& 1\n",
    "                \\end{bmatrix} = I + \\left[{ğ¯ \\over v_1}  - ğ_1 \\right] ğ_1^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "For $L_1 := L_{ğš_1}$ we have \n",
    "$$\n",
    "L_1 A =  \\begin{bmatrix} u_{11} & u_{12} & \\cdots & u_{1n} \\\\ \n",
    "& ğš_2^1 & \\cdots & ğš_n^1   \\end{bmatrix}\n",
    "$$\n",
    "where $ğš_j^1 := (L_1 ğš_j)[2:m]$ and $u_{1j} = a_{1j}$. But now consider\n",
    "$$\n",
    "L_2 := \\begin{bmatrix} 1  \\\\ & L_{ğš_2^1} \\end{bmatrix}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "L_2 L_1 A = \\begin{bmatrix} u_{11} & u_{12} & \\cdots & u_{1n} \\\\ \n",
    "    & u_{22} & u_{23} \\cdots & u_{2n} \\\\\n",
    "&& ğš_3^2 & \\cdots & ğš_n^2   \\end{bmatrix}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "r_{2j} :=  (ğš_j^1)[1] \\qquad \\hbox{and} \\qquad ğš_j^2 := (L_2 ğš_j^1)[2:m-1]\n",
    "$$\n",
    "Thus the first two columns are triangular. \n",
    "\n",
    "The inductive construction is again clear. If we define $ğš_j^0 := ğš_j$ we\n",
    "have the construction\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_j &:= \\begin{bmatrix} I_{j-1 \\times j-1}  \\\\ & L_{ğš_j^j} \\end{bmatrix} \\\\\n",
    "ğš_j^k &:= (L_k ğš_j^{k-1})[2:m-k+1]\n",
    " \\\\\n",
    "u_{kj} &:= (L_k ğš_j^{k-1})[1]\n",
    "\\end{align*}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "L_n \\cdots L_1 A = \\underbrace{\\begin{bmatrix} \n",
    "u_{11} & \\cdots & u_{1n} \\\\ & \\ddots & \\vdots\\\\\n",
    "                                        && u_{nn}\\end{bmatrix}}_U\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "A = \\underbrace{L_n^{-1} \\cdots L_1^{-1}}_L R.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### PLU Decomposition\n",
    "\n",
    "\n",
    "## 3. Cholesky Decomposition\n",
    "\n",
    "Cholesky Decomposition is a form of Gaussian elimination (without pivoting)\n",
    "that exploits symmetry in the problem. It is only relevant for _symmetric positive definite_\n",
    "matrices.\n",
    "\n",
    "**Definition (positive definite)** A square matrix $A \\in \\bbR^{n \\times n}$ is _positive definite_ if\n",
    "for all $ğ± \\in \\bbR^n$ we have\n",
    "$$\n",
    "ğ±^\\top A ğ± > 0\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
